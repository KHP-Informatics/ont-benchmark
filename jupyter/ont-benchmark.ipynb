{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from statistics import mean\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pysam\n",
    "from scipy import stats\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import metrics\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Seaborn settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "# Logging settings\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\", force=True)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford Nanopore Sequencing Benchmark\n",
    "\n",
    "This report presents a benchmark of SNVs, indels, and SVs, and a characterisation of the ONT dataset used for this benchmark.\n",
    "\n",
    "## Methods\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "- **Basecalling**: [wf-basecalling v1.1.7](https://github.com/epi2me-labs/wf-basecalling/tree/v1.1.7)\n",
    "\n",
    "- **Alignment and Variant Calling**: [wf-human-variation v2.1.0](https://github.com/epi2me-labs/wf-human-variation/tree/v2.1.0)\n",
    "\n",
    "### Quality Control Tools\n",
    "\n",
    "- **NanoPlot**: [1.42.0](https://quay.io/biocontainers/nanoplot:1.42.0--pyhdfd78af_0)\n",
    "\n",
    "  - Generates summary statistics for each sample and creates visualizations of QC metrics for sequencing summaries and aligned BAM files.\n",
    "\n",
    "- **NanoComp**: [1.23.1](https://quay.io/biocontainers/nanocomp:1.23.1--pyhdfd78af_0)\n",
    "\n",
    "  - Compares multiple sequencing runs and generates comparative plots.\n",
    "\n",
    "- **mosdepth**: [0.3.3](https://github.com/brentp/mosdepth/tree/v0.3.3)\n",
    "\n",
    "  - Calculates sequencing depth across the human genome for each sample.\n",
    "\n",
    "- **rtg-tools**: [3.12.1](https://github.com/RealTimeGenomics/rtg-tools/tree/3.12.1)\n",
    "\n",
    "  - Performs performs variant comparison against a truth dataset.\n",
    "\n",
    "- **SURVIVOR**: [1.0.7](https://github.com/fritzsedlazeck/SURVIVOR)\n",
    "\n",
    "  - Performs merging of vcf files to compare SVs within a sample and among populations/samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequencing Quality Control\n",
    "\n",
    "Aggregate table of the QC metrics from NanoStats for both singleplexed and multiplexed samples, from the aligned `.cram` files produced by `wf-human-variation`.\n",
    "\n",
    "Unless otherwise specified, subsequent plots and statistics include only samples basecalled with the `sup` algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "NanoStats Aligned BAMs directory not found: /scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/aligned_bams",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 219\u001b[0m\n\u001b[1;32m    214\u001b[0m np_aligned_bams_dir \u001b[38;5;241m=\u001b[39m Path(\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/aligned_bams/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    216\u001b[0m )\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Collect metrics\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m np_metrics_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_nanoplot_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43maligned_bams_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp_aligned_bams_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_summaries_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp_seq_summaries_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbasecall_suffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(np_metrics_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 162\u001b[0m, in \u001b[0;36mcollect_nanoplot_data\u001b[0;34m(aligned_bams_dir, seq_summaries_dir, basecall_suffixes)\u001b[0m\n\u001b[1;32m    159\u001b[0m seq_summaries_path \u001b[38;5;241m=\u001b[39m Path(seq_summaries_dir)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m aligned_bams_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNanoStats Aligned BAMs directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maligned_bams_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m seq_summaries_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNanoStats Sequencing summaries directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_summaries_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: NanoStats Aligned BAMs directory not found: /scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/aligned_bams"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class NanoplotMetrics:\n",
    "    \"\"\"Data class to store Nanoplot metrics data.\"\"\"\n",
    "\n",
    "    sample: str\n",
    "    basecall: str\n",
    "    multiplexing: str\n",
    "    metrics: Dict[str, Any]\n",
    "\n",
    "\n",
    "def parse_nanostats(nanostats_file: Path) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Parse NanoStats.txt file and extract metrics.\n",
    "\n",
    "    Args:\n",
    "        nanostats_file: Path to the NanoStats.txt file.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing parsed metrics from the NanoStats file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the NanoStats file doesn't exist.\n",
    "        ValueError: If the file format is invalid or missing required metrics.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "\n",
    "    try:\n",
    "        with open(nanostats_file, \"r\") as f:\n",
    "            # Skip header line\n",
    "            next(f)\n",
    "\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                # Split on tab\n",
    "                parts = line.split(\"\\t\") if \"\\t\" in line else line.split()\n",
    "\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                key = parts[0]\n",
    "                value = parts[1]\n",
    "\n",
    "                # Clean up key names\n",
    "                key = key.rstrip(\":_()\")\n",
    "                key = key.replace(\" \", \"_\")\n",
    "\n",
    "                # Handle special cases\n",
    "                if \"longest_read\" in key or \"highest_Q_read\" in key:\n",
    "                    # Extract the number and quality/length\n",
    "                    value = value.strip(\"()\")\n",
    "                    if \"(\" in value:\n",
    "                        main_val, extra_val = value.split(\"(\")\n",
    "                        main_val = float(main_val.strip())\n",
    "                        extra_val = float(extra_val.rstrip(\")\"))\n",
    "                        metrics[f\"{key}_length\"] = main_val\n",
    "                        metrics[f\"{key}_quality\"] = extra_val\n",
    "                    continue\n",
    "\n",
    "                # Handle percentage cases (e.g., \"Reads >Q5: 8151702 (99.8%) 46705.8Mb\")\n",
    "                if \"(\" in value and \")\" in value:\n",
    "                    count = float(value.split()[0])\n",
    "                    percentage = float(value.split(\"(\")[1].split(\"%\")[0])\n",
    "                    size = float(value.split()[-1].rstrip(\"Mb\"))\n",
    "                    metrics[f\"{key}_count\"] = count\n",
    "                    metrics[f\"{key}_percentage\"] = percentage\n",
    "                    metrics[f\"{key}_size_mb\"] = size\n",
    "                    continue\n",
    "\n",
    "                # Convert numeric values\n",
    "                try:\n",
    "                    if \".\" in value:\n",
    "                        value = float(value)\n",
    "                    else:\n",
    "                        value = int(value)\n",
    "                except ValueError:\n",
    "                    # Keep as string if not numeric\n",
    "                    pass\n",
    "\n",
    "                metrics[key] = value\n",
    "\n",
    "        if not metrics:\n",
    "            raise ValueError(f\"No metrics found in {nanostats_file}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"NanoStats file not found: {nanostats_file}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error parsing NanoStats file {nanostats_file}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def determine_multiplexing(sample_name: str, seq_summaries_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Determine multiplexing status by checking if sequencing summary folder names\n",
    "    contain two samples separated by double underscores.\n",
    "\n",
    "    Args:\n",
    "        sample_name: Name of the sample to check.\n",
    "        seq_summaries_path: Path to the directory containing sequencing summaries.\n",
    "\n",
    "    Returns:\n",
    "        String indicating multiplexing status ('multiplexed' or 'singleplexed').\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the sequencing summaries directory doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check each directory in seq_summaries_path\n",
    "        for dir_path in seq_summaries_path.iterdir():\n",
    "            if not dir_path.is_dir():\n",
    "                continue\n",
    "\n",
    "            # Check if directory name contains the sample name\n",
    "            if sample_name in dir_path.name:\n",
    "                # Check if the directory name contains two samples separated by '__'\n",
    "                if \"__\" in dir_path.name:\n",
    "                    return \"multiplexed\"\n",
    "                return \"singleplexed\"\n",
    "\n",
    "        logger.warning(f\"No matching directory found for sample: {sample_name}\")\n",
    "        return \"singleplexed\"\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Sequencing summaries directory not found: {seq_summaries_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error determining multiplexing for sample {sample_name}: {str(e)}\"\n",
    "        )\n",
    "        raise\n",
    "\n",
    "\n",
    "def collect_nanoplot_data(\n",
    "    aligned_bams_dir: Path | str,\n",
    "    seq_summaries_dir: Path | str,\n",
    "    basecall_suffixes: List[str] = [\"sup\"],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collects and processes Nanoplot data from specified directories.\n",
    "\n",
    "    Args:\n",
    "        aligned_bams_dir: Directory containing NanoStats files generated from aligned bam files.\n",
    "        seq_summaries_dir: Directory containing NanoStats files generated from sequencing summaries.\n",
    "        basecall_suffixes: List of basecalling suffixes.\n",
    "\n",
    "    Returns:\n",
    "        List of dictionaries containing processed Nanoplot metrics.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If required directories don't exist.\n",
    "        ValueError: If no valid data found in the directories.\n",
    "    \"\"\"\n",
    "    aligned_bams_path = Path(aligned_bams_dir)\n",
    "    seq_summaries_path = Path(seq_summaries_dir)\n",
    "\n",
    "    if not aligned_bams_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"NanoStats Aligned BAMs directory not found: {aligned_bams_path}\"\n",
    "        )\n",
    "    if not seq_summaries_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"NanoStats Sequencing summaries directory not found: {seq_summaries_path}\"\n",
    "        )\n",
    "\n",
    "    nanoplot_data = []\n",
    "\n",
    "    try:\n",
    "        for subdir in aligned_bams_path.iterdir():\n",
    "            if not subdir.is_dir():\n",
    "                continue\n",
    "\n",
    "            for basecall_suffix in basecall_suffixes:\n",
    "                file_suffix = f\"_{basecall_suffix}\"\n",
    "                if subdir.name.endswith(file_suffix):\n",
    "                    sample_name = subdir.name.split(file_suffix)[0]\n",
    "                    nanostats_file = subdir / \"NanoStats.txt\"\n",
    "\n",
    "                    if nanostats_file.is_file():\n",
    "                        logger.info(f\"Processing NanoStats for sample: {sample_name}\")\n",
    "                        metrics = parse_nanostats(nanostats_file)\n",
    "                        metrics[\"sample\"] = sample_name\n",
    "                        metrics[\"basecall\"] = basecall_suffix\n",
    "                        metrics[\"multiplexing\"] = determine_multiplexing(\n",
    "                            sample_name, seq_summaries_path\n",
    "                        )\n",
    "                        nanoplot_data.append(metrics)\n",
    "                    else:\n",
    "                        logger.warning(\n",
    "                            f\"NanoStats.txt file not found for sample: {sample_name}\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "        if not nanoplot_data:\n",
    "            raise ValueError(\n",
    "                \"No valid Nanoplot data found in the specified directories\"\n",
    "            )\n",
    "\n",
    "        return nanoplot_data\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing Nanoplot data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Define paths\n",
    "np_seq_summaries_dir = Path(\n",
    "    \"/scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/seq_summaries/\"\n",
    ")\n",
    "np_aligned_bams_dir = Path(\n",
    "    \"/scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/aligned_bams/\"\n",
    ")\n",
    "\n",
    "# Collect metrics\n",
    "np_metrics_data = collect_nanoplot_data(\n",
    "    aligned_bams_dir=np_aligned_bams_dir,\n",
    "    seq_summaries_dir=np_seq_summaries_dir,\n",
    "    basecall_suffixes=[\"sup\"],\n",
    ")\n",
    "\n",
    "logger.info(f\"Successfully processed {len(np_metrics_data)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_nanostats(file_path):\n",
    "    metrics = {}\n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line and not line.startswith(\"Metrics dataset\"):\n",
    "                if line.startswith(\"Reads >Q\"):\n",
    "                    parts = line.split(\":\")\n",
    "                    key = parts[0].strip()\n",
    "                    values = parts[1].strip().split()\n",
    "                    metrics[f\"{key}_count\"] = int(values[0].replace(\",\", \"\"))\n",
    "                    metrics[f\"{key}_percentage\"] = float(values[1].strip(\"()%\"))\n",
    "                    metrics[f\"{key}_bases\"] = float(values[2].strip(\"Mb\")) * 1e6\n",
    "                else:\n",
    "                    key, value = line.split(maxsplit=1)\n",
    "                    metrics[key.strip()] = value.strip()\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def determine_multiplexing(sample_name, seq_summaries_dir):\n",
    "    for subdir in os.listdir(seq_summaries_dir):\n",
    "        if sample_name in subdir:\n",
    "            return \"multiplex\" if \"__\" in subdir else \"singleplex\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def collect_nanoplot_data():\n",
    "    np_data = []\n",
    "    for subdir in os.listdir(NP_ALIGNED_BAMS_DIR):\n",
    "        for suffix in NP_BASECALL_SUFFIXES:\n",
    "            if subdir.endswith(suffix):\n",
    "                sample_name = subdir.split(suffix)[0]\n",
    "                file_path = os.path.join(NP_ALIGNED_BAMS_DIR, subdir, \"NanoStats.txt\")\n",
    "                if os.path.isfile(file_path):\n",
    "                    metrics = parse_nanostats(file_path)\n",
    "                    metrics[\"sample\"] = sample_name\n",
    "                    metrics[\"basecall\"] = suffix.lstrip(\"_\")\n",
    "                    metrics[\"multiplexing\"] = determine_multiplexing(\n",
    "                        sample_name, NP_SEQ_SUMMARIES_DIR\n",
    "                    )\n",
    "                    np_data.append(metrics)\n",
    "                break\n",
    "    return np_data\n",
    "\n",
    "\n",
    "def process_nanoplot_dataframe(df):\n",
    "    df.drop(columns=[\"Metrics\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    sorted_samples = sorted(df[\"sample\"].unique())\n",
    "    sample_to_anon = {\n",
    "        sample: f\"Sample {i+1}\" for i, sample in enumerate(sorted_samples)\n",
    "    }\n",
    "\n",
    "    df[\"anonymised_sample\"] = df[\"sample\"].map(sample_to_anon)\n",
    "\n",
    "    column_order = [\"sample\", \"anonymised_sample\", \"basecall\", \"multiplexing\"] + [\n",
    "        col\n",
    "        for col in df.columns\n",
    "        if col not in [\"sample\", \"anonymised_sample\", \"basecall\", \"multiplexing\"]\n",
    "    ]\n",
    "\n",
    "    column_types = {\n",
    "        \"multiplexing\": \"category\",\n",
    "        \"basecall\": \"category\",\n",
    "        \"anonymised_sample\": \"category\",\n",
    "        \"number_of_reads\": \"numeric\",\n",
    "        \"number_of_bases\": \"numeric\",\n",
    "        \"number_of_bases_aligned\": \"numeric\",\n",
    "        \"fraction_bases_aligned\": \"numeric\",\n",
    "        \"mean_read_length\": \"numeric\",\n",
    "        \"median_read_length\": \"numeric\",\n",
    "        \"read_length_stdev\": \"numeric\",\n",
    "        \"n50\": \"numeric\",\n",
    "        \"average_identity\": \"numeric\",\n",
    "        \"Reads >Q5_count\": \"numeric\",\n",
    "        \"Reads >Q5_percentage\": \"numeric\",\n",
    "        \"Reads >Q5_bases\": \"numeric\",\n",
    "        \"Reads >Q7_count\": \"numeric\",\n",
    "        \"Reads >Q7_percentage\": \"numeric\",\n",
    "        \"Reads >Q7_bases\": \"numeric\",\n",
    "        \"Reads >Q10_count\": \"numeric\",\n",
    "        \"Reads >Q10_percentage\": \"numeric\",\n",
    "        \"Reads >Q10_bases\": \"numeric\",\n",
    "        \"Reads >Q12_count\": \"numeric\",\n",
    "        \"Reads >Q12_percentage\": \"numeric\",\n",
    "        \"Reads >Q12_bases\": \"numeric\",\n",
    "        \"Reads >Q15_count\": \"numeric\",\n",
    "        \"Reads >Q15_percentage\": \"numeric\",\n",
    "        \"Reads >Q15_bases\": \"numeric\",\n",
    "    }\n",
    "\n",
    "    for column, dtype in column_types.items():\n",
    "        if column in df.columns:\n",
    "            if dtype == \"category\":\n",
    "                df[column] = df[column].astype(dtype)\n",
    "            elif dtype == \"numeric\":\n",
    "                df[column] = pd.to_numeric(df[column], errors=\"coerce\")\n",
    "\n",
    "    df = df[column_order]\n",
    "    return df.sort_values(by=[\"multiplexing\", \"sample\"])\n",
    "\n",
    "\n",
    "NP_SEQ_SUMMARIES_DIR = (\n",
    "    \"/scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/seq_summaries/\"\n",
    ")\n",
    "NP_ALIGNED_BAMS_DIR = (\n",
    "    \"/scratch/prj/ppn_als_longread/ont-benchmark/qc/nanoplot/aligned_bams/\"\n",
    ")\n",
    "NP_BASECALL_SUFFIXES = [\"_sup\", \"_hac\"]\n",
    "\n",
    "np_metrics_data = collect_nanoplot_data()\n",
    "np_metrics_df = pd.DataFrame(np_metrics_data)\n",
    "np_metrics_df = process_nanoplot_dataframe(np_metrics_df)\n",
    "\n",
    "np_metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Yield\n",
    "\n",
    "### 1. Raw Yields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yield_plot(ax, data, x, y, hue, title, xlabel, ylabel):\n",
    "    data = data.copy()\n",
    "\n",
    "    data[\"sample_num\"] = data[x].str.extract(r\"(\\d+)\").astype(int)\n",
    "    data = data.sort_values(\"sample_num\")\n",
    "\n",
    "    sns.barplot(x=x, y=y, hue=hue, data=data, ax=ax, order=data[x])\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_ha(\"right\")\n",
    "\n",
    "    locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "    ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "\n",
    "    ax.legend(title=hue)\n",
    "\n",
    "\n",
    "def plot_sample_yields(metrics_df, basecall_type=\"sup\", figsize=(16, 6), dpi=300):\n",
    "    yields_df = metrics_df[metrics_df[\"basecall\"] == basecall_type]\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "\n",
    "    create_yield_plot(\n",
    "        ax1,\n",
    "        yields_df,\n",
    "        \"anonymised_sample\",\n",
    "        \"number_of_reads\",\n",
    "        \"multiplexing\",\n",
    "        f\"Read Yield per Sample\",\n",
    "        \"Sample\",\n",
    "        \"Number of Reads\",\n",
    "    )\n",
    "\n",
    "    create_yield_plot(\n",
    "        ax2,\n",
    "        yields_df,\n",
    "        \"anonymised_sample\",\n",
    "        \"number_of_bases\",\n",
    "        \"multiplexing\",\n",
    "        f\"Base Yield per Sample\",\n",
    "        \"Sample\",\n",
    "        \"Number of Bases\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "yield_plot = plot_sample_yields(np_metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    return f\"{num:,}\"\n",
    "\n",
    "\n",
    "def calculate_yield_stats(df):\n",
    "    return {\n",
    "        \"reads\": {\n",
    "            \"max\": df[\"number_of_reads\"].max(),\n",
    "            \"min\": df[\"number_of_reads\"].min(),\n",
    "            \"mean\": df[\"number_of_reads\"].mean(),\n",
    "            \"std\": df[\"number_of_reads\"].std(),\n",
    "            \"median\": df[\"number_of_reads\"].median(),\n",
    "        },\n",
    "        \"bases\": {\n",
    "            \"max\": df[\"number_of_bases\"].max(),\n",
    "            \"min\": df[\"number_of_bases\"].min(),\n",
    "            \"mean\": df[\"number_of_bases\"].mean(),\n",
    "            \"std\": df[\"number_of_bases\"].std(),\n",
    "            \"median\": df[\"number_of_bases\"].median(),\n",
    "        },\n",
    "    }\n",
    "\n",
    "\n",
    "def print_yield_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Samples Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    for metric in [\"reads\", \"bases\"]:\n",
    "        print(f\"\\n{metric.capitalize()}:\")\n",
    "        for stat, value in stats[metric].items():\n",
    "            formatted_value = format_number(round(value))\n",
    "            print(f\"  {stat.capitalize():4s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "singleplex_yields = np_metrics_df[\n",
    "    (np_metrics_df[\"multiplexing\"] == \"singleplex\")\n",
    "    & (np_metrics_df[\"basecall\"] == \"sup\")\n",
    "]\n",
    "multiplex_yields = np_metrics_df[\n",
    "    (np_metrics_df[\"multiplexing\"] == \"multiplex\")\n",
    "    & (np_metrics_df[\"basecall\"] == \"sup\")\n",
    "]\n",
    "\n",
    "singleplex_stats = calculate_yield_stats(singleplex_yields)\n",
    "multiplex_stats = calculate_yield_stats(multiplex_yields)\n",
    "\n",
    "print_yield_stats(singleplex_stats, \"Singleplexed\")\n",
    "print_yield_stats(multiplex_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for metric in [\"reads\", \"bases\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_stats[metric][\"mean\"], multiplex_stats[metric][\"mean\"]\n",
    "    )\n",
    "    print(f\"Mean Number of {metric.capitalize():5s}: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read Lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_read_length_data(metrics_df):\n",
    "    read_length_df = pd.melt(\n",
    "        metrics_df[metrics_df[\"basecall\"] == \"sup\"],\n",
    "        id_vars=[\"sample\", \"anonymised_sample\", \"multiplexing\"],\n",
    "        value_vars=[\"mean_read_length\", \"median_read_length\"],\n",
    "        var_name=\"read_length_type\",\n",
    "        value_name=\"read_length\",\n",
    "    )\n",
    "\n",
    "    read_length_df[\"read_length_type\"] = read_length_df[\"read_length_type\"].replace(\n",
    "        {\n",
    "            \"mean_read_length\": \"Mean Read Length\",\n",
    "            \"median_read_length\": \"Median Read Length\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    read_length_df[\"sample_num\"] = (\n",
    "        read_length_df[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    read_length_df = read_length_df.sort_values(\"sample_num\")\n",
    "\n",
    "    return read_length_df\n",
    "\n",
    "\n",
    "def create_length_subplot(data, ax, title, hue):\n",
    "    sns.barplot(\n",
    "        x=\"anonymised_sample\",\n",
    "        y=\"read_length\",\n",
    "        hue=hue,\n",
    "        data=data,\n",
    "        errorbar=None,\n",
    "        ax=ax,\n",
    "        order=data[\"anonymised_sample\"].unique(),\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Sample\")\n",
    "    ax.set_ylabel(\"Read Length (bp)\")\n",
    "\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_ha(\"right\")\n",
    "\n",
    "    locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "    ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "\n",
    "    ax.legend(title=hue)\n",
    "\n",
    "\n",
    "def plot_read_lengths(read_length_df, figsize=(16, 6), dpi=300):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=figsize, sharey=True, dpi=dpi)\n",
    "\n",
    "    for ax, read_length_type in zip(axes, [\"Mean Read Length\", \"Median Read Length\"]):\n",
    "        data = read_length_df[read_length_df[\"read_length_type\"] == read_length_type]\n",
    "        create_length_subplot(data, ax, read_length_type, hue=\"multiplexing\")\n",
    "\n",
    "        if ax != axes[0]:  # For the right subplot\n",
    "            ax.yaxis.set_tick_params(labelleft=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "read_length_df = prepare_read_length_data(np_metrics_df)\n",
    "plot_read_lengths(read_length_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nanoplot_data(base_dir, metrics_df):\n",
    "    all_nanoplot_data = pd.DataFrame()\n",
    "\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        anonymised_sample = row[\"anonymised_sample\"]\n",
    "        sample = row[\"sample\"]\n",
    "        basecall = row[\"basecall\"]\n",
    "        sample_dir = f\"{sample}_{basecall}\"\n",
    "        pickle_path = os.path.join(base_dir, sample_dir, \"NanoPlot-data.pickle\")\n",
    "\n",
    "        if os.path.isfile(pickle_path):\n",
    "            with open(pickle_path, \"rb\") as file:\n",
    "                nanoplot_data = pickle.load(file)\n",
    "            sample_data = pd.DataFrame(nanoplot_data)\n",
    "            sample_data = sample_data[[\"readIDs\", \"quals\", \"lengths\", \"mapQ\"]].copy()\n",
    "            sample_data[\"anonymised_sample\"] = anonymised_sample\n",
    "            sample_data[\"basecall\"] = basecall\n",
    "            all_nanoplot_data = pd.concat(\n",
    "                [all_nanoplot_data, sample_data], ignore_index=True\n",
    "            )\n",
    "\n",
    "    return all_nanoplot_data\n",
    "\n",
    "\n",
    "def process_nanoplot_data(nanoplot_data, metrics_df):\n",
    "    processed_data = nanoplot_data.merge(\n",
    "        metrics_df[\n",
    "            [\"anonymised_sample\", \"multiplexing\", \"basecall\", \"number_of_reads\"]\n",
    "        ],\n",
    "        on=[\"anonymised_sample\", \"basecall\"],\n",
    "    )\n",
    "\n",
    "    max_length = processed_data[\"lengths\"].max()\n",
    "    bin_edges = np.logspace(np.log10(10), np.log10(max_length), num=100)\n",
    "    processed_data[\"length_bin\"] = pd.cut(processed_data[\"lengths\"], bins=bin_edges)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def calculate_length_distribution(processed_data, basecall_type=\"sup\"):\n",
    "    length_dist = (\n",
    "        processed_data[processed_data[\"basecall\"] == basecall_type]\n",
    "        .groupby([\"anonymised_sample\", \"length_bin\", \"multiplexing\"], observed=False)\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    length_dist = length_dist.merge(\n",
    "        processed_data[processed_data[\"basecall\"] == basecall_type][\n",
    "            [\"anonymised_sample\", \"number_of_reads\"]\n",
    "        ].drop_duplicates(),\n",
    "        on=\"anonymised_sample\",\n",
    "    )\n",
    "\n",
    "    length_dist[\"percentage\"] = (\n",
    "        length_dist[\"count\"] / length_dist[\"number_of_reads\"] * 100\n",
    "    )\n",
    "\n",
    "    max_length = processed_data[\"lengths\"].max()\n",
    "    bin_edges = np.logspace(np.log10(10), np.log10(max_length), num=100)\n",
    "    bin_centers = np.sqrt(bin_edges[:-1] * bin_edges[1:])\n",
    "    bin_centers = pd.Series(bin_centers, index=length_dist[\"length_bin\"].cat.categories)\n",
    "    length_dist[\"bin_center\"] = length_dist[\"length_bin\"].map(bin_centers)\n",
    "\n",
    "    return length_dist\n",
    "\n",
    "\n",
    "def plot_read_length_distribution(length_dist, max_length):\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "    non_zero_samples = length_dist.groupby(\"anonymised_sample\")[\"percentage\"].sum() > 0\n",
    "    non_zero_samples = non_zero_samples[non_zero_samples].index\n",
    "\n",
    "    filtered_length_dist = length_dist[\n",
    "        length_dist[\"anonymised_sample\"].isin(non_zero_samples)\n",
    "    ]\n",
    "\n",
    "    filtered_length_dist[\"sample_num\"] = (\n",
    "        filtered_length_dist[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    filtered_length_dist = filtered_length_dist.sort_values(\"sample_num\")\n",
    "\n",
    "    filtered_length_dist = filtered_length_dist.rename(\n",
    "        columns={\"anonymised_sample\": \"sample ID\"}\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=filtered_length_dist,\n",
    "        x=\"bin_center\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        hue_order=filtered_length_dist[\"sample ID\"].unique(),\n",
    "    )\n",
    "\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Read Length\")\n",
    "    plt.ylabel(\"Proportion of Reads (%)\")\n",
    "    plt.title(\"Distribution of Read Lengths\")\n",
    "\n",
    "    tick_positions = np.logspace(np.log10(10), np.log10(max_length), num=20)\n",
    "    plt.xticks(\n",
    "        ticks=tick_positions, labels=[f\"{int(tick):,}\" for tick in tick_positions]\n",
    "    )\n",
    "\n",
    "    plt.xlim(left=10, right=max_length)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "NANOPLOT_BASE_DIR = \"/scratch/prj/ppn_als_longread/qc/nanoplot/aligned_bams/\"\n",
    "\n",
    "nanoplot_data = load_nanoplot_data(NANOPLOT_BASE_DIR, np_metrics_df)\n",
    "processed_nanoplot_data = process_nanoplot_data(nanoplot_data, np_metrics_df)\n",
    "length_distribution = calculate_length_distribution(processed_nanoplot_data)\n",
    "max_read_length = processed_nanoplot_data[\"lengths\"].max()\n",
    "\n",
    "plot_read_length_distribution(length_distribution, max_read_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(num):\n",
    "    return f\"{num:,.2f}\"\n",
    "\n",
    "\n",
    "def calculate_length_stats(df):\n",
    "    return {\n",
    "        \"max\": df[\"lengths\"].max(),\n",
    "        \"min\": df[\"lengths\"].min(),\n",
    "        \"mean\": df[\"lengths\"].mean(),\n",
    "        \"std\": df[\"lengths\"].std(),\n",
    "        \"median\": df[\"lengths\"].median(),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_length_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Samples Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nRead Lengths:\")\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = format_number(value)\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "singleplexed_reads = processed_nanoplot_data[\n",
    "    (processed_nanoplot_data[\"multiplexing\"] == \"singleplex\")\n",
    "    & (processed_nanoplot_data[\"basecall\"] == \"sup\")\n",
    "]\n",
    "multiplexed_reads = processed_nanoplot_data[\n",
    "    (processed_nanoplot_data[\"multiplexing\"] == \"multiplex\")\n",
    "    & (processed_nanoplot_data[\"basecall\"] == \"sup\")\n",
    "]\n",
    "\n",
    "singleplex_stats = calculate_length_stats(singleplexed_reads)\n",
    "multiplex_stats = calculate_length_stats(multiplexed_reads)\n",
    "\n",
    "print_length_stats(singleplex_stats, \"Singleplexed\")\n",
    "print_length_stats(multiplex_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_stats[stat], multiplex_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Read Length: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_yield_plot(metrics_df, read_length_df, length_distribution):\n",
    "    fig = plt.figure(figsize=(12, 12), dpi=300)\n",
    "    gs = fig.add_gridspec(3, 2, height_ratios=[1, 1, 1.2])\n",
    "\n",
    "    def adjust_tick_labels(ax):\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "            tick.set_ha(\"right\")\n",
    "        locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "        ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "\n",
    "    sample_order = sorted(\n",
    "        metrics_df[\"anonymised_sample\"].unique(), key=lambda x: int(x.split()[-1])\n",
    "    )\n",
    "\n",
    "    # A: Reads per Sample\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    create_yield_plot(\n",
    "        ax_a,\n",
    "        metrics_df[metrics_df[\"basecall\"] == \"sup\"],\n",
    "        \"anonymised_sample\",\n",
    "        \"number_of_reads\",\n",
    "        \"multiplexing\",\n",
    "        \"Reads per Sample\",\n",
    "        \"Sample\",\n",
    "        \"Number of Reads\",\n",
    "    )\n",
    "    ax_a.text(\n",
    "        -0.10,\n",
    "        1.07,\n",
    "        \"A\",\n",
    "        transform=ax_a.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    adjust_tick_labels(ax_a)\n",
    "\n",
    "    # B: Bases per Sample\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    create_yield_plot(\n",
    "        ax_b,\n",
    "        metrics_df[metrics_df[\"basecall\"] == \"sup\"],\n",
    "        \"anonymised_sample\",\n",
    "        \"number_of_bases\",\n",
    "        \"multiplexing\",\n",
    "        \"Bases per Sample\",\n",
    "        \"Sample\",\n",
    "        \"Number of Bases\",\n",
    "    )\n",
    "    ax_b.text(\n",
    "        -0.10,\n",
    "        1.07,\n",
    "        \"B\",\n",
    "        transform=ax_b.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    adjust_tick_labels(ax_b)\n",
    "    ax_b.get_legend().remove()\n",
    "\n",
    "    # C: Mean Read Length\n",
    "    ax_c = fig.add_subplot(gs[1, 0])\n",
    "    mean_length_data = read_length_df[\n",
    "        read_length_df[\"read_length_type\"] == \"Mean Read Length\"\n",
    "    ]\n",
    "    create_length_subplot(mean_length_data, ax_c, \"Mean Read Length\", \"multiplexing\")\n",
    "    ax_c.text(\n",
    "        -0.10,\n",
    "        1.07,\n",
    "        \"C\",\n",
    "        transform=ax_c.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    adjust_tick_labels(ax_c)\n",
    "    ax_c.get_legend().remove()\n",
    "\n",
    "    # D: Median Read Length\n",
    "    ax_d = fig.add_subplot(gs[1, 1])\n",
    "    median_length_data = read_length_df[\n",
    "        read_length_df[\"read_length_type\"] == \"Median Read Length\"\n",
    "    ]\n",
    "    create_length_subplot(\n",
    "        median_length_data, ax_d, \"Median Read Length\", \"multiplexing\"\n",
    "    )\n",
    "    ax_d.text(\n",
    "        -0.10,\n",
    "        1.07,\n",
    "        \"D\",\n",
    "        transform=ax_d.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    adjust_tick_labels(ax_d)\n",
    "    ax_d.get_legend().remove()\n",
    "\n",
    "    # E: Distribution of Read Lengths\n",
    "    ax_e = fig.add_subplot(gs[2, :])\n",
    "\n",
    "    non_zero_samples = (\n",
    "        length_distribution.groupby(\"anonymised_sample\")[\"percentage\"].sum() > 0\n",
    "    )\n",
    "    non_zero_samples = non_zero_samples[non_zero_samples].index\n",
    "\n",
    "    filtered_length_dist = length_distribution[\n",
    "        length_distribution[\"anonymised_sample\"].isin(non_zero_samples)\n",
    "    ]\n",
    "\n",
    "    filtered_length_dist[\"sample_num\"] = (\n",
    "        filtered_length_dist[\"anonymised_sample\"].str.split().str[-1].astype(int)\n",
    "    )\n",
    "    filtered_length_dist = filtered_length_dist.sort_values(\"sample_num\")\n",
    "\n",
    "    filtered_length_dist = filtered_length_dist.rename(\n",
    "        columns={\"anonymised_sample\": \"Sample ID\"}\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=filtered_length_dist,\n",
    "        x=\"bin_center\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        ax=ax_e,\n",
    "        hue_order=filtered_length_dist[\"Sample ID\"].unique(),\n",
    "    )\n",
    "\n",
    "    ax_e.set_xscale(\"log\")\n",
    "    ax_e.set_xlabel(\"Read Length\")\n",
    "    ax_e.set_ylabel(\"Proportion of Reads (%)\")\n",
    "    ax_e.set_title(\"Distribution of Read Lengths\")\n",
    "    ax_e.text(\n",
    "        -0.05,\n",
    "        1.07,\n",
    "        \"E\",\n",
    "        transform=ax_e.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    max_length = filtered_length_dist[\"bin_center\"].max()\n",
    "\n",
    "    tick_positions = np.logspace(np.log10(10), np.log10(max_length), num=19)\n",
    "    ax_e.set_xticks(tick_positions)\n",
    "    ax_e.set_xticklabels([f\"{int(tick):,}\" for tick in tick_positions])\n",
    "    ax_e.set_xlim(left=10, right=max_length)\n",
    "    ax_e.set_ylim(bottom=0)\n",
    "\n",
    "    plt.setp(ax_e.get_xticklabels(), ha=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_combined_yield_plot(np_metrics_df, read_length_df, length_distribution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Quality\n",
    "\n",
    "### 1. Basecalling Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_quality_distribution(processed_data, basecall_type=\"sup\"):\n",
    "    qual_data = processed_data[processed_data[\"basecall\"] == basecall_type].copy()\n",
    "\n",
    "    qual_data[\"quals_bin\"] = pd.cut(\n",
    "        qual_data[\"quals\"],\n",
    "        bins=pd.interval_range(\n",
    "            start=qual_data[\"quals\"].min(), end=qual_data[\"quals\"].max(), freq=0.5\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    qual_data[\"sample_num\"] = (\n",
    "        qual_data[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    qual_data = qual_data.sort_values(\"sample_num\")\n",
    "\n",
    "    quality_dist = (\n",
    "        qual_data.groupby(\n",
    "            [\"anonymised_sample\", \"quals_bin\", \"multiplexing\"], observed=True\n",
    "        )\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    total_reads = quality_dist.groupby(\n",
    "        [\"anonymised_sample\", \"multiplexing\"], observed=True\n",
    "    )[\"count\"].transform(\"sum\")\n",
    "\n",
    "    quality_dist[\"percentage\"] = (quality_dist[\"count\"] / total_reads) * 100\n",
    "\n",
    "    quality_dist[\"quals_bin_mid\"] = quality_dist[\"quals_bin\"].apply(lambda x: x.mid)\n",
    "\n",
    "    quality_dist[\"sample_num\"] = (\n",
    "        quality_dist[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    quality_dist = quality_dist.sort_values(\"sample_num\")\n",
    "\n",
    "    quality_dist = quality_dist.rename(columns={\"anonymised_sample\": \"Sample ID\"})\n",
    "\n",
    "    return quality_dist\n",
    "\n",
    "\n",
    "def plot_base_quality_distribution(quality_dist):\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "    sample_order = quality_dist[\"Sample ID\"].unique()\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=quality_dist,\n",
    "        x=\"quals_bin_mid\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        hue_order=sample_order,\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Quality Score\")\n",
    "    plt.ylabel(\"Percentage of Total Reads\")\n",
    "    plt.title(\"Distribution of Read Quality Scores\")\n",
    "\n",
    "    max_qual = int(quality_dist[\"quals_bin_mid\"].max())\n",
    "    tick_positions = np.arange(0, max_qual + 1, 5)\n",
    "    plt.xticks(ticks=tick_positions, labels=tick_positions)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "quality_distribution = calculate_base_quality_distribution(processed_nanoplot_data)\n",
    "plot_base_quality_distribution(quality_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_qscore_data(metrics_df):\n",
    "    qscore_df = pd.melt(\n",
    "        metrics_df[metrics_df[\"basecall\"] == \"sup\"],\n",
    "        id_vars=[\"anonymised_sample\", \"multiplexing\"],\n",
    "        value_vars=[\n",
    "            \"Reads >Q5_percentage\",\n",
    "            \"Reads >Q7_percentage\",\n",
    "            \"Reads >Q10_percentage\",\n",
    "            \"Reads >Q12_percentage\",\n",
    "            \"Reads >Q15_percentage\",\n",
    "        ],\n",
    "        var_name=\"Quality_Score\",\n",
    "        value_name=\"Percentage\",\n",
    "    )\n",
    "\n",
    "    qscore_df[\"Quality_Score\"] = (\n",
    "        qscore_df[\"Quality_Score\"]\n",
    "        .str.replace(\"Reads >\", \"\")\n",
    "        .str.replace(\"_percentage\", \"\")\n",
    "    )\n",
    "\n",
    "    qscore_df = qscore_df.rename(columns={\"anonymised_sample\": \"Sample ID\"})\n",
    "\n",
    "    return qscore_df\n",
    "\n",
    "\n",
    "def plot_qscore_distribution(qscore_df, figsize=(20, 6), dpi=300):\n",
    "    qscore_df[\"sample_num\"] = qscore_df[\"Sample ID\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    qscore_df = qscore_df.sort_values(\"sample_num\")\n",
    "\n",
    "    quality_score_order = [\"Q5\", \"Q7\", \"Q10\", \"Q12\", \"Q15\"]\n",
    "    qscore_df[\"Quality_Score\"] = pd.Categorical(\n",
    "        qscore_df[\"Quality_Score\"], categories=quality_score_order, ordered=True\n",
    "    )\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "\n",
    "    for ax, multiplex_type in zip([ax1, ax2], [\"multiplex\", \"singleplex\"]):\n",
    "        data = qscore_df[qscore_df[\"multiplexing\"] == multiplex_type]\n",
    "\n",
    "        if not data.empty:\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x=\"Sample ID\",\n",
    "                y=\"Percentage\",\n",
    "                hue=\"Quality_Score\",\n",
    "                errorbar=None,\n",
    "                ax=ax,\n",
    "                order=data[\"Sample ID\"].unique(),\n",
    "                hue_order=quality_score_order,\n",
    "            )\n",
    "\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(\"Percentage of Reads\")\n",
    "            ax.set_title(\n",
    "                f\"Percentage of Reads Above Quality Scores\\n{multiplex_type.capitalize()} Samples\"\n",
    "            )\n",
    "            ax.legend(title=\"Quality Score\", loc=\"lower right\")\n",
    "\n",
    "            for tick in ax.get_xticklabels():\n",
    "                tick.set_rotation(45)\n",
    "                tick.set_ha(\"right\")\n",
    "\n",
    "            locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "            ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "        else:\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No {multiplex_type} samples\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "qscore_df = prepare_qscore_data(np_metrics_df)\n",
    "plot_qscore_distribution(qscore_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_base_quality_stats(df):\n",
    "    return {\n",
    "        \"max\": df[\"quals\"].max(),\n",
    "        \"min\": df[\"quals\"].min(),\n",
    "        \"mean\": df[\"quals\"].mean(),\n",
    "        \"std\": df[\"quals\"].std(),\n",
    "        \"median\": df[\"quals\"].median(),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_base_quality_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Samples Quality Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nBase Qualities:\")\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = format_number(value)\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "singleplexed_quals = processed_nanoplot_data[\n",
    "    (processed_nanoplot_data[\"multiplexing\"] == \"singleplex\")\n",
    "    & (processed_nanoplot_data[\"basecall\"] == \"sup\")\n",
    "]\n",
    "multiplexed_quals = processed_nanoplot_data[\n",
    "    (processed_nanoplot_data[\"multiplexing\"] == \"multiplex\")\n",
    "    & (processed_nanoplot_data[\"basecall\"] == \"sup\")\n",
    "]\n",
    "\n",
    "singleplex_quality_stats = calculate_base_quality_stats(singleplexed_quals)\n",
    "multiplex_quality_stats = calculate_base_quality_stats(multiplexed_quals)\n",
    "\n",
    "print_base_quality_stats(singleplex_quality_stats, \"Singleplexed\")\n",
    "print_base_quality_stats(multiplex_quality_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_quality_stats[stat], multiplex_quality_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Base Quality: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mapping Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mapping_quality_distribution(processed_data, basecall_type=\"sup\"):\n",
    "    mapQ_data = processed_data[processed_data[\"basecall\"] == basecall_type].copy()\n",
    "\n",
    "    mapQ_data[\"mapQ_bin\"] = pd.cut(\n",
    "        mapQ_data[\"mapQ\"],\n",
    "        bins=pd.interval_range(\n",
    "            start=mapQ_data[\"mapQ\"].min(), end=mapQ_data[\"mapQ\"].max(), freq=0.5\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    mapQ_data[\"sample_num\"] = (\n",
    "        mapQ_data[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    mapQ_data = mapQ_data.sort_values(\"sample_num\")\n",
    "\n",
    "    mapping_quality_dist = (\n",
    "        mapQ_data.groupby(\n",
    "            [\"anonymised_sample\", \"mapQ_bin\", \"multiplexing\"], observed=True\n",
    "        )\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    total_counts = mapping_quality_dist.groupby(\n",
    "        [\"anonymised_sample\", \"multiplexing\"], observed=True\n",
    "    )[\"count\"].transform(\"sum\")\n",
    "\n",
    "    mapping_quality_dist[\"percentage\"] = (\n",
    "        mapping_quality_dist[\"count\"] / total_counts * 100\n",
    "    )\n",
    "\n",
    "    mapping_quality_dist[\"mapQ_bin_mid\"] = mapping_quality_dist[\"mapQ_bin\"].apply(\n",
    "        lambda x: x.mid\n",
    "    )\n",
    "\n",
    "    mapping_quality_dist[\"sample_num\"] = (\n",
    "        mapping_quality_dist[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    mapping_quality_dist = mapping_quality_dist.sort_values(\"sample_num\")\n",
    "\n",
    "    mapping_quality_dist = mapping_quality_dist.rename(\n",
    "        columns={\"anonymised_sample\": \"Sample ID\"}\n",
    "    )\n",
    "\n",
    "    return mapping_quality_dist\n",
    "\n",
    "\n",
    "def plot_mapping_quality_distribution(mapping_quality_dist):\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "    sample_order = mapping_quality_dist[\"Sample ID\"].unique()\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=mapping_quality_dist,\n",
    "        x=\"mapQ_bin_mid\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        hue_order=sample_order,\n",
    "        legend=\"full\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Mapping Quality Score\")\n",
    "    plt.ylabel(\"Percentage of Total Reads\")\n",
    "    plt.title(\"Distribution of Mapping Quality Scores\")\n",
    "\n",
    "    max_mapQ = int(mapping_quality_dist[\"mapQ_bin_mid\"].max())\n",
    "    tick_positions = np.arange(0, max_mapQ + 1, 5)\n",
    "    plt.xticks(ticks=tick_positions, labels=tick_positions)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "mapping_quality_distribution = calculate_mapping_quality_distribution(\n",
    "    processed_nanoplot_data\n",
    ")\n",
    "\n",
    "plot_mapping_quality_distribution(mapping_quality_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mapQ_stats(df):\n",
    "    return {\n",
    "        \"max\": df[\"mapQ\"].max(),\n",
    "        \"min\": df[\"mapQ\"].min(),\n",
    "        \"mean\": df[\"mapQ\"].mean(),\n",
    "        \"std\": df[\"mapQ\"].std(),\n",
    "        \"median\": df[\"mapQ\"].median(),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_mapQ_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Samples Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nMapping Quality:\")\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = format_number(value)\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "singleplex_mapQ_stats = calculate_mapQ_stats(singleplexed_quals)\n",
    "multiplex_mapQ_stats = calculate_mapQ_stats(multiplexed_quals)\n",
    "\n",
    "print_mapQ_stats(singleplex_mapQ_stats, \"Singleplexed\")\n",
    "print_mapQ_stats(multiplex_mapQ_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_mapQ_stats[stat], multiplex_mapQ_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Mapping Quality: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequencing_metrics_plot(\n",
    "    quality_distribution, mapping_quality_distribution, qscore_df\n",
    "):\n",
    "    fig = plt.figure(figsize=(12, 8), dpi=300)\n",
    "    gs = fig.add_gridspec(2, 2)\n",
    "\n",
    "    def adjust_tick_labels(ax):\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "            tick.set_ha(\"right\")\n",
    "        locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "        ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "\n",
    "    # A: Base Quality Distribution\n",
    "    ax_a = fig.add_subplot(gs[0, 0])\n",
    "    sns.lineplot(\n",
    "        data=quality_distribution,\n",
    "        x=\"quals_bin_mid\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        ax=ax_a,\n",
    "        legend=False,\n",
    "    )\n",
    "    ax_a.set_xlabel(\"Quality Score\")\n",
    "    ax_a.set_ylabel(\"Percentage of Total Reads\")\n",
    "    ax_a.set_title(\"Distribution of Read Quality Scores\")\n",
    "    max_qual = int(quality_distribution[\"quals_bin_mid\"].max())\n",
    "    tick_positions = np.arange(0, max_qual + 1, 5)\n",
    "    ax_a.set_xticks(ticks=tick_positions, labels=tick_positions)\n",
    "    ax_a.text(\n",
    "        -0.1,\n",
    "        1.05,\n",
    "        \"A\",\n",
    "        transform=ax_a.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # B: Mapping Quality Distribution\n",
    "    ax_b = fig.add_subplot(gs[0, 1])\n",
    "    sns.lineplot(\n",
    "        data=mapping_quality_distribution,\n",
    "        x=\"mapQ_bin_mid\",\n",
    "        y=\"percentage\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    ax_b.set_xlabel(\"Mapping Quality Score\")\n",
    "    ax_b.set_ylabel(\"Percentage of Total Reads\")\n",
    "    ax_b.set_title(\"Distribution of Mapping Quality Scores\")\n",
    "    max_mapQ = int(mapping_quality_distribution[\"mapQ_bin_mid\"].max())\n",
    "    tick_positions = np.arange(0, max_mapQ + 1, 5)\n",
    "    ax_b.set_xticks(ticks=tick_positions, labels=tick_positions)\n",
    "    ax_b.text(\n",
    "        -0.1,\n",
    "        1.05,\n",
    "        \"B\",\n",
    "        transform=ax_b.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "    ax_b.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    # C and D: Q-Score Distribution\n",
    "    ax_c = fig.add_subplot(gs[1, 0])\n",
    "    ax_d = fig.add_subplot(gs[1, 1])\n",
    "    for ax, multiplex_type, letter in zip(\n",
    "        [ax_c, ax_d], [\"multiplex\", \"singleplex\"], [\"C\", \"D\"]\n",
    "    ):\n",
    "        data = qscore_df[qscore_df[\"multiplexing\"] == multiplex_type]\n",
    "        if not data.empty:\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x=\"Sample ID\",\n",
    "                y=\"Percentage\",\n",
    "                hue=\"Quality_Score\",\n",
    "                errorbar=None,\n",
    "                ax=ax,\n",
    "                order=data[\"Sample ID\"].unique(),\n",
    "            )\n",
    "            ax.set_xlabel(\"Sample ID\")\n",
    "            ax.set_ylabel(\"Percentage of Reads\")\n",
    "            ax.set_title(\n",
    "                f\"Percentage of Reads Above Quality Scores\\n{multiplex_type.capitalize()} Samples\"\n",
    "            )\n",
    "            adjust_tick_labels(ax)\n",
    "            ax.text(\n",
    "                -0.1,\n",
    "                1.05,\n",
    "                letter,\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=12,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "        else:\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                0.5,\n",
    "                f\"No {multiplex_type} samples\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    if not ax_c.get_legend() is None:\n",
    "        ax_c.get_legend().remove()\n",
    "    if not ax_d.get_legend() is None:\n",
    "        ax_d.legend(title=\"Quality Score\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_combined_sequencing_metrics_plot(\n",
    "    quality_distribution, mapping_quality_distribution, qscore_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing Depth\n",
    "\n",
    "### 1. Depth per Chromosome\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_mosdepth_file(file_path, suffix):\n",
    "    sample_name = os.path.basename(file_path).split(\".\")[0].replace(suffix, \"\")\n",
    "    df = pd.read_csv(file_path, sep=\"\\t\")\n",
    "    chromosomes = [f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"]\n",
    "    df = df[df[\"chrom\"].isin(chromosomes + [\"total\"])]\n",
    "    df = df[~df[\"chrom\"].str.endswith(\"_region\")]\n",
    "    df = df[[\"chrom\", \"mean\"]]\n",
    "    df[\"sample\"] = sample_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_per_base_file(file_path, suffix):\n",
    "    sample_name = os.path.basename(file_path).split(\".\")[0].replace(suffix, \"\")\n",
    "    df = pd.read_csv(\n",
    "        file_path, sep=\"\\t\", header=None, names=[\"chrom\", \"start\", \"end\", \"depth\"]\n",
    "    )\n",
    "    df[\"chrom\"] = df[\"chrom\"].astype(str)\n",
    "    chromosomes = [f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"]\n",
    "    df = df[df[\"chrom\"].isin(chromosomes)]\n",
    "    df_grouped = df.groupby(\"chrom\")[\"depth\"]\n",
    "    df = df_grouped.agg(\n",
    "        mean=\"mean\", sem=lambda x: np.std(x, ddof=1) / np.sqrt(len(x))\n",
    "    ).reset_index()\n",
    "    df[\"sample\"] = sample_name\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyze_mosdepth_data(np_metrics_df, suffix=\"_sup\"):\n",
    "    path_to_mosdepth_summary = f\"/scratch/prj/ppn_als_longread/qc/mosdepth/*{suffix}/*{suffix}.mosdepth.summary.txt\"\n",
    "    mosdepth_summary_files = glob.glob(path_to_mosdepth_summary)\n",
    "\n",
    "    path_to_per_base = (\n",
    "        f\"/scratch/prj/ppn_als_longread/qc/mosdepth/*{suffix}/*{suffix}.per-base.bed.gz\"\n",
    "    )\n",
    "    per_base_files = glob.glob(path_to_per_base)\n",
    "\n",
    "    all_dfs = [process_mosdepth_file(file, suffix) for file in mosdepth_summary_files]\n",
    "    depth_df = pd.concat(all_dfs)\n",
    "\n",
    "    all_per_base_dfs = [process_per_base_file(file, suffix) for file in per_base_files]\n",
    "    per_base_df = pd.concat(all_per_base_dfs)\n",
    "\n",
    "    depth_df = depth_df.merge(\n",
    "        per_base_df.rename(columns={\"mean\": \"per_base_mean\", \"sem\": \"per_base_sem\"}),\n",
    "        on=[\"chrom\", \"sample\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    total_depth_df = (\n",
    "        depth_df[depth_df[\"chrom\"] == \"total\"].copy().drop_duplicates(subset=\"sample\")\n",
    "    )\n",
    "    total_depth_df = total_depth_df[[\"sample\", \"mean\"]]\n",
    "\n",
    "    total_depth_df = total_depth_df.rename(columns={\"mean\": \"mean_depth\"}).merge(\n",
    "        np_metrics_df[[\"sample\", \"multiplexing\", \"anonymised_sample\"]], on=\"sample\"\n",
    "    )\n",
    "\n",
    "    total_depth_df = total_depth_df.sort_values(by=[\"multiplexing\", \"sample\"])\n",
    "\n",
    "    depth_df = depth_df[depth_df[\"chrom\"] != \"total\"]\n",
    "\n",
    "    chromosome_order = [f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"]\n",
    "    depth_df[\"chrom\"] = pd.Categorical(\n",
    "        depth_df[\"chrom\"], categories=chromosome_order, ordered=True\n",
    "    )\n",
    "\n",
    "    depth_df = depth_df.merge(\n",
    "        np_metrics_df[[\"sample\", \"multiplexing\", \"anonymised_sample\"]],\n",
    "        on=\"sample\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    depth_df[\"sample_num\"] = (\n",
    "        depth_df[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    depth_df = depth_df.sort_values(by=\"sample_num\")\n",
    "    depth_df = depth_df.rename(columns={\"anonymised_sample\": \"Sample ID\"})\n",
    "\n",
    "    depth_df = depth_df.drop_duplicates(subset=[\"chrom\", \"Sample ID\"]).sort_values(\n",
    "        by=[\"Sample ID\", \"multiplexing\"]\n",
    "    )\n",
    "\n",
    "    wg_depth_df = depth_df[depth_df[\"chrom\"].isin(chromosome_order)]\n",
    "\n",
    "    return wg_depth_df, total_depth_df\n",
    "\n",
    "\n",
    "def plot_mean_depth_per_chromosome_with_sem(wg_depth_df):\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "\n",
    "    wg_depth_df[\"sample_num\"] = (\n",
    "        wg_depth_df[\"Sample ID\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    wg_depth_df = wg_depth_df.sort_values([\"sample_num\", \"chrom\"])\n",
    "\n",
    "    unique_samples = wg_depth_df[\"Sample ID\"].unique()\n",
    "    color_palette = sns.color_palette(\"husl\", n_colors=len(unique_samples))\n",
    "    color_dict = dict(zip(unique_samples, color_palette))\n",
    "\n",
    "    line_plot = sns.lineplot(\n",
    "        data=wg_depth_df,\n",
    "        x=\"chrom\",\n",
    "        y=\"mean\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        palette=color_dict,\n",
    "        legend=\"full\",\n",
    "        hue_order=unique_samples,\n",
    "    )\n",
    "\n",
    "    for sample_id in unique_samples:\n",
    "        sample_df = wg_depth_df[wg_depth_df[\"Sample ID\"] == sample_id].sort_values(\n",
    "            \"chrom\"\n",
    "        )\n",
    "        color = color_dict[sample_id]\n",
    "        plt.fill_between(\n",
    "            sample_df[\"chrom\"],\n",
    "            sample_df[\"mean\"] - sample_df[\"per_base_sem\"],\n",
    "            sample_df[\"mean\"] + sample_df[\"per_base_sem\"],\n",
    "            alpha=0.2,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "    plt.title(\"Mean Depth per Chromosome (with SEM)\")\n",
    "    plt.xlabel(\"Chromosome\")\n",
    "    plt.ylabel(\"Mean Depth\")\n",
    "\n",
    "    locs, labels = plt.xticks()\n",
    "    new_locs = [loc + 0.01 for loc in locs]\n",
    "\n",
    "    plt.xticks(new_locs, labels, rotation=45, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "wg_depth_df, total_depth_df = analyze_mosdepth_data(np_metrics_df)\n",
    "\n",
    "plot_mean_depth_per_chromosome_with_sem(wg_depth_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Whole Genome Depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_whole_genome_depth(total_depth_df):\n",
    "    total_depth_df[\"sample_num\"] = (\n",
    "        total_depth_df[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "\n",
    "    total_depth_df = total_depth_df.sort_values(\"sample_num\")\n",
    "\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "    sns.barplot(\n",
    "        data=total_depth_df,\n",
    "        x=\"anonymised_sample\",\n",
    "        y=\"mean_depth\",\n",
    "        hue=\"multiplexing\",\n",
    "        dodge=False,\n",
    "        order=total_depth_df[\"anonymised_sample\"],\n",
    "    )\n",
    "\n",
    "    plt.title(\"Mean Whole Genome Depth per Sample\")\n",
    "    plt.xlabel(\"Sample ID\")\n",
    "    plt.ylabel(\"Depth\")\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.xticks([loc - 0.2 for loc in locs], labels, rotation=45, ha=\"right\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_mean_whole_genome_depth(total_depth_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_depth_stats(df, column_name):\n",
    "    return {\n",
    "        \"max\": df[column_name].max(),\n",
    "        \"min\": df[column_name].min(),\n",
    "        \"mean\": df[column_name].mean(),\n",
    "        \"std\": df[column_name].std(),\n",
    "        \"median\": df[column_name].median(),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_depth_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Samples Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nDepth:\")\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = f\"{value:.2f}\"\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "# Calculate stats for per-chromosome depth\n",
    "singleplexed_depth = wg_depth_df[wg_depth_df[\"multiplexing\"] == \"singleplex\"]\n",
    "multiplexed_depth = wg_depth_df[wg_depth_df[\"multiplexing\"] == \"multiplex\"]\n",
    "\n",
    "singleplex_depth_stats = calculate_depth_stats(singleplexed_depth, \"mean\")\n",
    "multiplex_depth_stats = calculate_depth_stats(multiplexed_depth, \"mean\")\n",
    "\n",
    "print(\"\\nPer-Chromosome Depth Statistics:\")\n",
    "print_depth_stats(singleplex_depth_stats, \"Singleplexed\")\n",
    "print_depth_stats(multiplex_depth_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_depth_stats[stat], multiplex_depth_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Depth: {increase:6.2f}%\")\n",
    "\n",
    "# Calculate stats for whole genome depth\n",
    "singleplexed_wg = total_depth_df[total_depth_df[\"multiplexing\"] == \"singleplex\"]\n",
    "multiplexed_wg = total_depth_df[total_depth_df[\"multiplexing\"] == \"multiplex\"]\n",
    "\n",
    "singleplex_wg_stats = calculate_depth_stats(singleplexed_wg, \"mean_depth\")\n",
    "multiplex_wg_stats = calculate_depth_stats(multiplexed_wg, \"mean_depth\")\n",
    "\n",
    "print(\"\\nWhole Genome Depth Statistics:\")\n",
    "print_depth_stats(singleplex_wg_stats, \"Singleplexed\")\n",
    "print_depth_stats(multiplex_wg_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_wg_stats[stat], multiplex_wg_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Depth: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Flowcell Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_seq_stats(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df[\"multiplexing\"] = df[\"flowcell_id\"].apply(\n",
    "        lambda x: \"multiplex\" if \"__\" in str(x) else \"singleplex\"\n",
    "    )\n",
    "    df[\"flowcell_id\"] = df[\"flowcell_id\"].astype(str)\n",
    "\n",
    "    df = df.sort_values([\"multiplexing\", \"flowcell_id\"])\n",
    "\n",
    "    multiplex_count = 1\n",
    "    singleplex_count = 1\n",
    "    new_names = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"multiplexing\"] == \"multiplex\":\n",
    "            new_names.append(f\"Multiplex Flowcell {multiplex_count}\")\n",
    "            multiplex_count += 1\n",
    "        else:\n",
    "            new_names.append(f\"Singleplex Flowcell {singleplex_count}\")\n",
    "            singleplex_count += 1\n",
    "\n",
    "    df[\"new_flowcell_name\"] = new_names\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_pores_at_start(df):\n",
    "    df[\"multiplexing\"] = df[\"multiplexing\"].astype(\"category\")\n",
    "\n",
    "    plt.figure(figsize=(14, 6), dpi=300)\n",
    "    sns.lineplot(\n",
    "        data=df,\n",
    "        x=\"new_flowcell_name\",\n",
    "        y=\"number_pores_start\",\n",
    "        hue=\"multiplexing\",\n",
    "        marker=\"o\",\n",
    "        style=\"multiplexing\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Number of Pores Available at the Start of Sequencing\")\n",
    "    plt.xlabel(\"Flowcell ID\")\n",
    "    plt.ylabel(\"Number of Pores\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.legend(title=\"Multiplexing\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "SEQ_STATS_PATH = \"/scratch/prj/ppn_als_longread/seq_stats.csv\"\n",
    "seq_stats_df = read_seq_stats(SEQ_STATS_PATH)\n",
    "\n",
    "plot_pores_at_start(seq_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pore_stats(df, multiplexing_type):\n",
    "    subset = df[df[\"multiplexing\"] == multiplexing_type]\n",
    "    return {\n",
    "        \"max\": subset[\"number_pores_start\"].max(),\n",
    "        \"min\": subset[\"number_pores_start\"].min(),\n",
    "        \"mean\": subset[\"number_pores_start\"].mean(),\n",
    "        \"std\": subset[\"number_pores_start\"].std(),\n",
    "        \"median\": subset[\"number_pores_start\"].median(),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_pore_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} Flowcells Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(\"\\nNumber of Pores at Start:\")\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = f\"{value:.2f}\" if isinstance(value, float) else f\"{value}\"\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "singleplex_pore_stats = calculate_pore_stats(seq_stats_df, \"singleplex\")\n",
    "multiplex_pore_stats = calculate_pore_stats(seq_stats_df, \"multiplex\")\n",
    "\n",
    "print_pore_stats(singleplex_pore_stats, \"Singleplexed\")\n",
    "print_pore_stats(multiplex_pore_stats, \"Multiplexed\")\n",
    "\n",
    "print(\"\\nPercentage Increase (Singleplexed vs Multiplexed):\")\n",
    "print(\"=\" * 40)\n",
    "for stat in [\"mean\", \"median\"]:\n",
    "    increase = calculate_percentage_increase(\n",
    "        singleplex_pore_stats[stat], multiplex_pore_stats[stat]\n",
    "    )\n",
    "    print(f\"{stat.capitalize():6s} Number of Pores: {increase:6.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Relation between Flowcell Quality and Mean Whole Genome Depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_seq_stats_data(seq_stats_df, total_depth_df):\n",
    "    sample_to_flowcell = {}\n",
    "    sample_to_multiplexing = {}\n",
    "\n",
    "    for _, row in seq_stats_df.iterrows():\n",
    "        flowcell = row[\"flow_cell_id\"]\n",
    "        multiplexing = row[\"multiplexing\"]\n",
    "\n",
    "        if multiplexing == \"multiplex\":\n",
    "            samples = row[\"flowcell_id\"].split(\"__\")\n",
    "        else:\n",
    "            samples = [row[\"flowcell_id\"]]\n",
    "\n",
    "        for sample in samples:\n",
    "            sample_to_flowcell[sample] = flowcell\n",
    "            sample_to_multiplexing[sample] = multiplexing\n",
    "\n",
    "    total_depth_df[\"flowcell_id\"] = total_depth_df[\"sample\"].map(sample_to_flowcell)\n",
    "    total_depth_df[\"multiplexing\"] = total_depth_df[\"sample\"].map(\n",
    "        sample_to_multiplexing\n",
    "    )\n",
    "\n",
    "    total_depth_df[\"flowcell_id\"] = total_depth_df[\"flowcell_id\"].astype(str)\n",
    "\n",
    "    grouped_depth_df = (\n",
    "        total_depth_df.groupby(\"flowcell_id\")\n",
    "        .agg({\"mean_depth\": \"sum\", \"multiplexing\": \"first\"})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    grouped_depth_df = grouped_depth_df.rename(\n",
    "        columns={\"flowcell_id\": \"flow_cell_id\", \"mean_depth\": \"total_mean_depth\"}\n",
    "    )\n",
    "\n",
    "    merged_df = pd.merge(\n",
    "        seq_stats_df,\n",
    "        grouped_depth_df,\n",
    "        on=\"flow_cell_id\",\n",
    "        how=\"inner\",\n",
    "        suffixes=(None, \"_y\"),\n",
    "    )\n",
    "\n",
    "    merged_df = merged_df.drop(columns=[\"multiplexing_y\"])\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "def plot_pores_vs_depth(grouped_seq_stats_df):\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    main_color = palette[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 8), dpi=300)\n",
    "    sns.scatterplot(\n",
    "        data=grouped_seq_stats_df,\n",
    "        x=\"number_pores_start\",\n",
    "        y=\"total_mean_depth\",\n",
    "        hue=\"multiplexing\",\n",
    "        s=100,\n",
    "    )\n",
    "\n",
    "    x = grouped_seq_stats_df[\"number_pores_start\"]\n",
    "    y = grouped_seq_stats_df[\"total_mean_depth\"]\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "\n",
    "    plt.plot(\n",
    "        x_line,\n",
    "        y_line,\n",
    "        color=main_color,\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        label=\"line of best fit\",\n",
    "    )\n",
    "\n",
    "    n = len(x)\n",
    "    y_pred = slope * x + intercept\n",
    "    s_err = np.sqrt(np.sum((y - y_pred) ** 2) / (n - 2))\n",
    "    t = stats.t.ppf(0.975, n - 2)\n",
    "    ci = (\n",
    "        t\n",
    "        * s_err\n",
    "        * np.sqrt(1 / n + (x_line - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2))\n",
    "    )\n",
    "\n",
    "    plt.fill_between(\n",
    "        x_line,\n",
    "        y_line - ci,\n",
    "        y_line + ci,\n",
    "        color=main_color,\n",
    "        alpha=0.2,\n",
    "        label=\"95% Confidence Interval\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Number of Pores at Start vs Total Mean Whole Genome Depth\")\n",
    "    plt.xlabel(\"Number of Pores at Start\")\n",
    "    plt.ylabel(\"Mean Whole Genome Depth\")\n",
    "    plt.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return slope, intercept, r_value, p_value\n",
    "\n",
    "\n",
    "SEQ_STATS_PATH = \"/scratch/prj/ppn_als_longread/seq_stats.csv\"\n",
    "seq_stats_df = read_seq_stats(SEQ_STATS_PATH)\n",
    "\n",
    "grouped_seq_stats_df = parse_seq_stats_data(seq_stats_df, total_depth_df)\n",
    "slope, intercept, r_value, p_value = plot_pores_vs_depth(grouped_seq_stats_df)\n",
    "\n",
    "print(f\"Slope: {slope:.4f}\")\n",
    "print(f\"Intercept: {intercept:.4f}\")\n",
    "print(f\"R-value: {r_value:.4f}\")\n",
    "print(f\"Linear regression p-value: {p_value:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Barcoding Quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_barcode_mapping():\n",
    "    return {\n",
    "        \"A046_12\": \"barcode01\",\n",
    "        \"A079_07\": \"barcode02\",\n",
    "        \"A081_91\": \"barcode03\",\n",
    "        \"A048_09\": \"barcode04\",\n",
    "        \"A097_92\": \"barcode05\",\n",
    "        \"A085_00\": \"barcode06\",\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_nanostats_barcoded(file_path):\n",
    "    metrics = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        header = f.readline().strip().split(\"\\t\")\n",
    "        values = f.readline().strip().split(\"\\t\")\n",
    "\n",
    "        for barcode, value in zip(\n",
    "            header[1:], values[1:]\n",
    "        ):  # Skip the first column (Metrics)\n",
    "            if barcode == \"unclassified\" or barcode.startswith(\"barcode\"):\n",
    "                metrics[barcode] = int(value)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_multiplexed_flowcell_reads(seq_summaries_dir, ax=None):\n",
    "    sample_barcode_mapping = get_sample_barcode_mapping()\n",
    "    barcode_sample_mapping = {v: k for k, v in sample_barcode_mapping.items()}\n",
    "\n",
    "    flowcell_samples = {\n",
    "        \"A046_12__A079_07\": [\"A046_12\", \"A079_07\"],\n",
    "        \"A081_91__A048_09\": [\"A081_91\", \"A048_09\"],\n",
    "        \"A097_92__A085_00\": [\"A097_92\", \"A085_00\"],\n",
    "    }\n",
    "\n",
    "    flowcell_rename = {\n",
    "        name: f\"Multiplex Flowcell {i+1}\"\n",
    "        for i, name in enumerate(flowcell_samples.keys())\n",
    "    }\n",
    "\n",
    "    sample_rename = {\n",
    "        sample: f\"Sample {i+1 if i != 1 and i != 2 else 3 if i == 1 else 2}\"\n",
    "        for i, sample in enumerate(sorted(set(sum(flowcell_samples.values(), []))))\n",
    "    }\n",
    "    sample_rename[\"Unclassified\"] = \"Unclassified\"\n",
    "\n",
    "    data = []\n",
    "    flowcell_total_reads = {}\n",
    "    flowcell_unclassified_reads = {}\n",
    "\n",
    "    for subdir in os.listdir(seq_summaries_dir):\n",
    "        if \"__\" in subdir:\n",
    "            flowcell_name = subdir\n",
    "            nanostats_path = os.path.join(\n",
    "                seq_summaries_dir, subdir, \"NanoStats_barcoded.txt\"\n",
    "            )\n",
    "            if os.path.exists(nanostats_path):\n",
    "                metrics = parse_nanostats_barcoded(nanostats_path)\n",
    "                flowcell_total_reads[flowcell_name] = 0\n",
    "                flowcell_unclassified_reads[flowcell_name] = 0\n",
    "\n",
    "                for barcode, read_count in metrics.items():\n",
    "                    flowcell_total_reads[flowcell_name] += read_count\n",
    "                    if barcode in barcode_sample_mapping:\n",
    "                        sample = barcode_sample_mapping[barcode]\n",
    "                        if sample in flowcell_samples[flowcell_name]:\n",
    "                            data.append(\n",
    "                                {\n",
    "                                    \"Flowcell\": flowcell_rename[flowcell_name],\n",
    "                                    \"Sample\": sample_rename[sample],\n",
    "                                    \"Read Count\": read_count,\n",
    "                                }\n",
    "                            )\n",
    "                    elif barcode == \"unclassified\":\n",
    "                        flowcell_unclassified_reads[flowcell_name] = read_count\n",
    "                        data.append(\n",
    "                            {\n",
    "                                \"Flowcell\": flowcell_rename[flowcell_name],\n",
    "                                \"Sample\": \"Unclassified\",\n",
    "                                \"Read Count\": read_count,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"Flowcell\"] = pd.Categorical(\n",
    "        df[\"Flowcell\"], categories=list(flowcell_rename.values()), ordered=True\n",
    "    )\n",
    "    df[\"Sample\"] = pd.Categorical(\n",
    "        df[\"Sample\"],\n",
    "        categories=[f\"Sample {i+1}\" for i in range(len(sample_rename) - 1)]\n",
    "        + [\"Unclassified\"],\n",
    "        ordered=True,\n",
    "    )\n",
    "    df = df.sort_values([\"Flowcell\", \"Sample\"])\n",
    "\n",
    "    cv_per_flowcell = {}\n",
    "    for flowcell in df[\"Flowcell\"].unique():\n",
    "        flowcell_data = df[\n",
    "            (df[\"Flowcell\"] == flowcell) & (df[\"Sample\"] != \"Unclassified\")\n",
    "        ]\n",
    "        read_counts = flowcell_data[\"Read Count\"].values\n",
    "        cv = np.std(read_counts) / np.mean(read_counts) * 100\n",
    "        cv_per_flowcell[flowcell] = cv\n",
    "\n",
    "    all_read_counts = df[df[\"Sample\"] != \"Unclassified\"][\"Read Count\"].values\n",
    "    overall_cv = np.std(all_read_counts) / np.mean(all_read_counts) * 100\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6), dpi=300)\n",
    "        standalone = True\n",
    "    else:\n",
    "        standalone = False\n",
    "\n",
    "    color = sns.color_palette(\"colorblind\")[0]\n",
    "\n",
    "    flowcells = df[\"Flowcell\"].unique()\n",
    "    width = 0.25\n",
    "    x = range(len(flowcells) * 3)\n",
    "\n",
    "    bars = ax.bar(x, df[\"Read Count\"], width, color=color)\n",
    "\n",
    "    ax.set_ylabel(\"Number of reads\")\n",
    "    ax.set_title(\"Number of barcoded reads\")\n",
    "\n",
    "    ax.tick_params(axis=\"x\", which=\"both\", bottom=False, top=False, labelbottom=False)\n",
    "\n",
    "    ax.xaxis.grid(False)\n",
    "\n",
    "    for i, bar in enumerate(bars):\n",
    "        sample = df.iloc[i][\"Sample\"]\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2,\n",
    "            -2e6,\n",
    "            sample,\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=45,\n",
    "        )\n",
    "\n",
    "    for i, flowcell in enumerate(flowcells):\n",
    "        ax.text(\n",
    "            i * 3 + 1,\n",
    "            -2.4e6,\n",
    "            flowcell,\n",
    "            ha=\"center\",\n",
    "        )\n",
    "\n",
    "    for i in range(1, len(flowcells)):\n",
    "        ax.axvline(x=i * 3 - 0.5, color=\"gray\", linestyle=\"-\", linewidth=0.5)\n",
    "\n",
    "    if standalone:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    unclassified_percentages = {\n",
    "        flowcell_rename[flowcell]: (unclassified / flowcell_total_reads[flowcell]) * 100\n",
    "        for flowcell, unclassified in flowcell_unclassified_reads.items()\n",
    "    }\n",
    "    avg_unclassified_percentage = mean(unclassified_percentages.values())\n",
    "\n",
    "    print(\n",
    "        f\"Average percentage of unclassified reads across all flowcells: {avg_unclassified_percentage:.2f}%\"\n",
    "    )\n",
    "\n",
    "    print(f\"Coefficient of Variation (CV) for each flowcell:\")\n",
    "    for flowcell, cv in cv_per_flowcell.items():\n",
    "        print(f\"{flowcell}: {cv:.2f}%\")\n",
    "    print(f\"Overall CV across all flowcells: {overall_cv:.2f}%\")\n",
    "\n",
    "    return ax, cv_per_flowcell, overall_cv\n",
    "\n",
    "\n",
    "ax, cv_per_flowcell, overall_cv = plot_multiplexed_flowcell_reads(NP_SEQ_SUMMARIES_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sequencing_plots(\n",
    "    wg_depth_df, total_depth_df, seq_stats_df, grouped_seq_stats_df, seq_summaries_dir\n",
    "):\n",
    "\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    main_color = palette[0]\n",
    "\n",
    "    fig = plt.figure(figsize=(14, 16), dpi=300)\n",
    "    gs = fig.add_gridspec(4, 2, height_ratios=[1, 1, 1, 1])\n",
    "\n",
    "    def adjust_tick_labels(ax):\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "            tick.set_ha(\"right\")\n",
    "        locs, labels = ax.get_xticks(), ax.get_xticklabels()\n",
    "        ax.set_xticks([loc + 0.2 for loc in locs])\n",
    "\n",
    "    # A: Mean Depth per Chromosome\n",
    "    ax_a = fig.add_subplot(gs[0, :])\n",
    "    wg_depth_df[\"Sample ID\"] = wg_depth_df[\"Sample ID\"].astype(str)\n",
    "    wg_depth_df[\"sample_num\"] = (\n",
    "        wg_depth_df[\"Sample ID\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    wg_depth_df = wg_depth_df.sort_values([\"sample_num\", \"chrom\"])\n",
    "\n",
    "    unique_samples = wg_depth_df[\"Sample ID\"].unique()\n",
    "    color_palette = sns.color_palette(\"husl\", n_colors=len(unique_samples))\n",
    "    color_dict = dict(zip(unique_samples, color_palette))\n",
    "\n",
    "    line_plot = sns.lineplot(\n",
    "        data=wg_depth_df,\n",
    "        x=\"chrom\",\n",
    "        y=\"mean\",\n",
    "        hue=\"Sample ID\",\n",
    "        style=\"multiplexing\",\n",
    "        palette=color_dict,\n",
    "        legend=\"full\",\n",
    "        hue_order=unique_samples,\n",
    "        ax=ax_a,\n",
    "    )\n",
    "\n",
    "    for sample_id in unique_samples:\n",
    "        sample_df = wg_depth_df[wg_depth_df[\"Sample ID\"] == sample_id].sort_values(\n",
    "            \"chrom\"\n",
    "        )\n",
    "        color = color_dict[sample_id]\n",
    "        ax_a.fill_between(\n",
    "            sample_df[\"chrom\"],\n",
    "            sample_df[\"mean\"] - sample_df[\"per_base_sem\"],\n",
    "            sample_df[\"mean\"] + sample_df[\"per_base_sem\"],\n",
    "            alpha=0.2,\n",
    "            color=color,\n",
    "        )\n",
    "\n",
    "    ax_a.set_title(\"Mean Depth per Chromosome (with SEM)\")\n",
    "    ax_a.set_xlabel(\"Chromosome\")\n",
    "    ax_a.set_ylabel(\"Mean Depth\")\n",
    "\n",
    "    locs, labels = ax_a.get_xticks(), ax_a.get_xticklabels()\n",
    "    new_locs = [loc + 0.01 for loc in locs]\n",
    "    ax_a.set_xticks(new_locs)\n",
    "    ax_a.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "\n",
    "    ax_a.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    ax_a.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax_a.text(\n",
    "        -0.05,\n",
    "        1.05,\n",
    "        \"A\",\n",
    "        transform=ax_a.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # B: Mean Whole Genome Depth per Sample\n",
    "    ax_b = fig.add_subplot(gs[1, 0])\n",
    "    total_depth_df[\"sample_num\"] = (\n",
    "        total_depth_df[\"anonymised_sample\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "    )\n",
    "    total_depth_df = total_depth_df.sort_values(\"sample_num\")\n",
    "    sns.barplot(\n",
    "        data=total_depth_df,\n",
    "        x=\"anonymised_sample\",\n",
    "        y=\"mean_depth\",\n",
    "        hue=\"multiplexing\",\n",
    "        dodge=False,\n",
    "        order=total_depth_df[\"anonymised_sample\"],\n",
    "        ax=ax_b,\n",
    "    )\n",
    "    ax_b.set_title(\"Mean Whole Genome Depth per Sample\")\n",
    "    ax_b.set_xlabel(\"Sample ID\")\n",
    "    ax_b.set_ylabel(\"Depth\")\n",
    "    adjust_tick_labels(ax_b)\n",
    "    ax_b.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax_b.text(\n",
    "        -0.05,\n",
    "        1.05,\n",
    "        \"B\",\n",
    "        transform=ax_b.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # C: Number of Pores Available at the Start of Sequencing\n",
    "    ax_c = fig.add_subplot(gs[1, 1])\n",
    "    sns.lineplot(\n",
    "        data=seq_stats_df,\n",
    "        x=\"new_flowcell_name\",\n",
    "        y=\"number_pores_start\",\n",
    "        hue=\"multiplexing\",\n",
    "        marker=\"o\",\n",
    "        style=\"multiplexing\",\n",
    "        ax=ax_c,\n",
    "    )\n",
    "    ax_c.set_title(\"Number of Pores Available at the Start of Sequencing\")\n",
    "    ax_c.set_xlabel(\"Flowcell ID\")\n",
    "    ax_c.set_ylabel(\"Number of Pores\")\n",
    "    adjust_tick_labels(ax_c)\n",
    "    ax_c.legend(title=\"Multiplexing\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax_c.set_ylim(bottom=0)\n",
    "    ax_c.text(\n",
    "        -0.05,\n",
    "        1.05,\n",
    "        \"C\",\n",
    "        transform=ax_c.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # D: Number of Pores at Start vs Total Mean Whole Genome Depth\n",
    "    ax_d = fig.add_subplot(gs[2, 0])\n",
    "    sns.scatterplot(\n",
    "        data=grouped_seq_stats_df,\n",
    "        x=\"number_pores_start\",\n",
    "        y=\"total_mean_depth\",\n",
    "        hue=\"multiplexing\",\n",
    "        s=100,\n",
    "        ax=ax_d,\n",
    "    )\n",
    "\n",
    "    x = grouped_seq_stats_df[\"number_pores_start\"]\n",
    "    y = grouped_seq_stats_df[\"total_mean_depth\"]\n",
    "\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    x_line = np.linspace(x.min(), x.max(), 100)\n",
    "    y_line = slope * x_line + intercept\n",
    "\n",
    "    ax_d.plot(\n",
    "        x_line,\n",
    "        y_line,\n",
    "        color=main_color,\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        alpha=0.5,\n",
    "        label=\"line of best fit\",\n",
    "    )\n",
    "\n",
    "    n = len(x)\n",
    "    y_pred = slope * x + intercept\n",
    "    s_err = np.sqrt(np.sum((y - y_pred) ** 2) / (n - 2))\n",
    "    t = stats.t.ppf(0.975, n - 2)\n",
    "    ci = (\n",
    "        t\n",
    "        * s_err\n",
    "        * np.sqrt(1 / n + (x_line - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2))\n",
    "    )\n",
    "\n",
    "    ax_d.fill_between(\n",
    "        x_line,\n",
    "        y_line - ci,\n",
    "        y_line + ci,\n",
    "        color=main_color,\n",
    "        alpha=0.2,\n",
    "        label=\"95% confidence interval\",\n",
    "    )\n",
    "\n",
    "    ax_d.set_title(\"Number of Pores at Start vs Total Mean Whole Genome Depth\")\n",
    "    ax_d.set_xlabel(\"Number of Pores at Start\")\n",
    "    ax_d.set_ylabel(\"Mean Whole Genome Depth\")\n",
    "    ax_d.legend(title=\"\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax_d.text(\n",
    "        -0.05,\n",
    "        1.05,\n",
    "        \"D\",\n",
    "        transform=ax_d.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    stats_text = f\"slope: {slope:.4f}\\nintercept: {intercept:.4f}\\nr-value: {r_value:.4f}\\np-value: {p_value:.4e}\"\n",
    "    ax_d.text(\n",
    "        0.05,\n",
    "        0.95,\n",
    "        stats_text,\n",
    "        transform=ax_d.transAxes,\n",
    "        fontsize=8,\n",
    "        verticalalignment=\"top\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", alpha=0.7),\n",
    "    )\n",
    "\n",
    "    # E: Number of barcoded reads\n",
    "    ax_e = fig.add_subplot(gs[2, 1])\n",
    "    plot_multiplexed_flowcell_reads(seq_summaries_dir, ax=ax_e)\n",
    "    ax_e.text(\n",
    "        -0.05,\n",
    "        1.05,\n",
    "        \"E\",\n",
    "        transform=ax_e.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "combined_fig = create_combined_sequencing_plots(\n",
    "    wg_depth_df,\n",
    "    total_depth_df,\n",
    "    seq_stats_df,\n",
    "    grouped_seq_stats_df,\n",
    "    NP_SEQ_SUMMARIES_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNV Benchmark\n",
    "\n",
    "## Comparative analysis\n",
    "\n",
    "### 1. Sensitivity, Precision, and F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_summary(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        none_line = next(\n",
    "            (line for line in lines if line.strip().startswith(\"None\")), None\n",
    "        )\n",
    "\n",
    "        if none_line:\n",
    "            values = none_line.split()\n",
    "            metrics = {\n",
    "                \"True-pos-baseline\": int(values[1]),\n",
    "                \"True-pos-call\": int(values[2]),\n",
    "                \"False-pos\": int(values[3]),\n",
    "                \"False-neg\": int(values[4]),\n",
    "                \"Precision\": float(values[5]),\n",
    "                \"Sensitivity\": float(values[6]),\n",
    "                \"F-measure\": float(values[7]),\n",
    "            }\n",
    "            return metrics\n",
    "        else:\n",
    "            print(f\"No 'None' threshold line found in {file_path}\")\n",
    "            return {}\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "\n",
    "def calculate_rtg_statistics(df):\n",
    "    return df.groupby(\"complexity\").agg(\n",
    "        {\n",
    "            \"Precision\": [\"mean\", \"std\", \"median\", \"min\", \"max\"],\n",
    "            \"Sensitivity\": [\"mean\", \"std\", \"median\", \"min\", \"max\"],\n",
    "            \"F-measure\": [\"mean\", \"std\", \"median\", \"min\", \"max\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def collect_snv_metrics(sample_ids, technologies, complexities):\n",
    "    snv_metrics = {\"ont\": {\"hc\": [], \"lc\": []}, \"illumina\": {\"hc\": [], \"lc\": []}}\n",
    "\n",
    "    for _, row in sample_ids.iterrows():\n",
    "        ont_id = row[\"ont_id\"]\n",
    "        lp_id = row[\"lp_id\"]\n",
    "\n",
    "        for tech in technologies:\n",
    "            for complexity in complexities:\n",
    "                sample_id = ont_id if tech == \"ont\" else lp_id\n",
    "                summary_file = (\n",
    "                    f\"output/snv/rtg_vcfeval/{complexity}/{sample_id}.snv/summary.txt\"\n",
    "                )\n",
    "                summary = read_summary(summary_file)\n",
    "\n",
    "                if summary:\n",
    "                    metrics_entry = {\n",
    "                        \"sample_id\": sample_id,\n",
    "                        \"complexity\": complexity,\n",
    "                        **summary,\n",
    "                    }\n",
    "                    snv_metrics[tech][complexity].append(metrics_entry)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Skipping empty summary for {sample_id}, {tech}, {complexity}\"\n",
    "                    )\n",
    "\n",
    "    return snv_metrics\n",
    "\n",
    "\n",
    "def process_snv_metrics(snv_metrics):\n",
    "    snv_ont_metrics_df = pd.DataFrame(\n",
    "        snv_metrics[\"ont\"][\"hc\"] + snv_metrics[\"ont\"][\"lc\"]\n",
    "    )\n",
    "    snv_illumina_metrics_df = pd.DataFrame(\n",
    "        snv_metrics[\"illumina\"][\"hc\"] + snv_metrics[\"illumina\"][\"lc\"]\n",
    "    )\n",
    "\n",
    "    snv_ont_stats = calculate_rtg_statistics(snv_ont_metrics_df)\n",
    "    snv_illumina_stats = calculate_rtg_statistics(snv_illumina_metrics_df)\n",
    "\n",
    "    return (\n",
    "        snv_ont_metrics_df,\n",
    "        snv_illumina_metrics_df,\n",
    "        snv_ont_stats,\n",
    "        snv_illumina_stats,\n",
    "    )\n",
    "\n",
    "\n",
    "def perform_ttest(ont_data, illumina_data, metric):\n",
    "    ont_values = ont_data[metric]\n",
    "    illumina_data = illumina_data[metric]\n",
    "    t_stat, p_value = stats.ttest_ind(ont_values, illumina_data)\n",
    "    return t_stat, p_value\n",
    "\n",
    "\n",
    "def prepare_stats_dataframe(df, metrics):\n",
    "    for metric in metrics:\n",
    "        df[(metric, \"t_statistic\")] = None\n",
    "        df[(metric, \"p_value\")] = None\n",
    "        df[(metric, \"adjusted_p_value\")] = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def perform_statistical_tests(\n",
    "    ont_metrics_df,\n",
    "    illumina_metrics_df,\n",
    "    ont_stats,\n",
    "    illumina_stats,\n",
    "    complexities,\n",
    "    metrics,\n",
    "):\n",
    "    all_p_values = []\n",
    "\n",
    "    for complexity in complexities:\n",
    "        ont_data = ont_metrics_df[ont_metrics_df[\"complexity\"] == complexity]\n",
    "        illumina_data = illumina_metrics_df[\n",
    "            illumina_metrics_df[\"complexity\"] == complexity\n",
    "        ]\n",
    "\n",
    "        for metric in metrics:\n",
    "            t_stat, p_value = perform_ttest(ont_data, illumina_data, metric)\n",
    "\n",
    "            ont_stats.loc[complexity, (metric, \"t_statistic\")] = t_stat\n",
    "            ont_stats.loc[complexity, (metric, \"p_value\")] = p_value\n",
    "\n",
    "            illumina_stats.loc[complexity, (metric, \"t_statistic\")] = t_stat\n",
    "            illumina_stats.loc[complexity, (metric, \"p_value\")] = p_value\n",
    "\n",
    "            all_p_values.append(p_value)\n",
    "\n",
    "    return all_p_values, ont_stats, illumina_stats\n",
    "\n",
    "\n",
    "def apply_fdr_correction(\n",
    "    all_p_values, ont_stats, illumina_stats, complexities, metrics\n",
    "):\n",
    "    _, adjusted_p_values, _, _ = multipletests(all_p_values, method=\"fdr_bh\")\n",
    "\n",
    "    adjusted_p_value_index = 0\n",
    "    for complexity in complexities:\n",
    "        for metric in metrics:\n",
    "            adjusted_p_value = adjusted_p_values[adjusted_p_value_index]\n",
    "            ont_stats.loc[complexity, (metric, \"adjusted_p_value\")] = adjusted_p_value\n",
    "            illumina_stats.loc[complexity, (metric, \"adjusted_p_value\")] = (\n",
    "                adjusted_p_value\n",
    "            )\n",
    "            adjusted_p_value_index += 1\n",
    "\n",
    "    return ont_stats, illumina_stats\n",
    "\n",
    "\n",
    "def run_statistical_analysis(\n",
    "    snv_ont_metrics_df,\n",
    "    snv_illumina_metrics_df,\n",
    "    snv_ont_stats,\n",
    "    snv_illumina_stats,\n",
    "    complexities,\n",
    "):\n",
    "    metrics_to_test = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "\n",
    "    snv_ont_stats = prepare_stats_dataframe(snv_ont_stats, metrics_to_test)\n",
    "    snv_illumina_stats = prepare_stats_dataframe(snv_illumina_stats, metrics_to_test)\n",
    "\n",
    "    all_p_values, snv_ont_stats, snv_illumina_stats = perform_statistical_tests(\n",
    "        snv_ont_metrics_df,\n",
    "        snv_illumina_metrics_df,\n",
    "        snv_ont_stats,\n",
    "        snv_illumina_stats,\n",
    "        complexities,\n",
    "        metrics_to_test,\n",
    "    )\n",
    "\n",
    "    snv_ont_stats, snv_illumina_stats = apply_fdr_correction(\n",
    "        all_p_values, snv_ont_stats, snv_illumina_stats, complexities, metrics_to_test\n",
    "    )\n",
    "\n",
    "    return snv_ont_stats, snv_illumina_stats\n",
    "\n",
    "\n",
    "sample_ids = pd.read_csv(\"sample_ids.csv\")\n",
    "technologies = [\"ont\", \"illumina\"]\n",
    "complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "snv_metrics = collect_snv_metrics(sample_ids, technologies, complexities)\n",
    "snv_ont_metrics_df, snv_illumina_metrics_df, snv_ont_stats, snv_illumina_stats = (\n",
    "    process_snv_metrics(snv_metrics)\n",
    ")\n",
    "\n",
    "snv_ont_stats, snv_illumina_stats = run_statistical_analysis(\n",
    "    snv_ont_metrics_df,\n",
    "    snv_illumina_metrics_df,\n",
    "    snv_ont_stats,\n",
    "    snv_illumina_stats,\n",
    "    complexities,\n",
    ")\n",
    "\n",
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(\"ONT SNV Stats:\")\n",
    "    display(snv_ont_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(\"\\nIllumina SNV Stats:\")\n",
    "    display(snv_illumina_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_snv_performance_data(ont_stats, illumina_stats, metrics, complexities):\n",
    "    plot_data = []\n",
    "    for complexity in complexities:\n",
    "        for metric in metrics:\n",
    "            ont_value = ont_stats.loc[complexity, (metric, \"mean\")]\n",
    "            illumina_value = illumina_stats.loc[complexity, (metric, \"mean\")]\n",
    "            adjusted_p_value = ont_stats.loc[complexity, (metric, \"adjusted_p_value\")]\n",
    "\n",
    "            significance = get_significance(adjusted_p_value)\n",
    "\n",
    "            plot_data.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"Complexity\": complexity.upper(),\n",
    "                        \"Metric\": metric,\n",
    "                        \"Technology\": \"long-read\",\n",
    "                        \"Value\": ont_value,\n",
    "                        \"Significance\": significance,\n",
    "                    },\n",
    "                    {\n",
    "                        \"Complexity\": complexity.upper(),\n",
    "                        \"Metric\": metric,\n",
    "                        \"Technology\": \"short-read\",\n",
    "                        \"Value\": illumina_value,\n",
    "                        \"Significance\": significance,\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "def get_significance(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def add_significance(ax, data, y_offset=0.02):\n",
    "    for i, metric in enumerate(data[\"Metric\"].unique()):\n",
    "        metric_data = data[data[\"Metric\"] == metric]\n",
    "        significance = metric_data[\"Significance\"].iloc[0]\n",
    "        if significance:\n",
    "            y = max(metric_data[\"Value\"]) + y_offset\n",
    "            ax.text(\n",
    "                i,\n",
    "                y,\n",
    "                significance,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                color=\"black\",\n",
    "                fontweight=\"bold\",\n",
    "            )\n",
    "\n",
    "\n",
    "def plot_performance_bars(ax, data, x, y, hue, **kwargs):\n",
    "    sns.barplot(x=x, y=y, hue=hue, data=data, errorbar=None, ax=ax, **kwargs)\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "\n",
    "\n",
    "def create_performance_plot(\n",
    "    df,\n",
    "    metrics,\n",
    "    complexities,\n",
    "    ax=None,\n",
    "    figsize=(14, 6),\n",
    "    dpi=300,\n",
    "    ylim=(0, 1.05),\n",
    "    title=\"Performance Comparison\",\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "    else:\n",
    "        fig = ax[0, 0].figure\n",
    "\n",
    "    for i, complexity in enumerate(complexities):\n",
    "        plot_performance_bars(\n",
    "            ax[i],\n",
    "            df[df[\"Complexity\"] == complexity.upper()],\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            hue=\"Technology\",\n",
    "        )\n",
    "        ax[i].set_title(\n",
    "            f\"{'High' if complexity == 'hc' else 'Low'} Complexity\", fontsize=14\n",
    "        )\n",
    "        ax[i].set_ylim(ylim)\n",
    "        add_significance(ax[i], df[df[\"Complexity\"] == complexity.upper()])\n",
    "\n",
    "        ax[i].set_xlabel(\"\")\n",
    "        ax[i].set_ylabel(\"Performance\", fontsize=12)\n",
    "\n",
    "        if i == 0:\n",
    "            ax[i].legend_.remove()\n",
    "        else:\n",
    "            ax[i].legend(title=\"Technology\", bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_snv_performance_metrics(ont_stats, illumina_stats, ax=None, **kwargs):\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "    plot_data = prepare_snv_performance_data(\n",
    "        ont_stats, illumina_stats, metrics, complexities\n",
    "    )\n",
    "    return create_performance_plot(plot_data, metrics, complexities, ax=ax, **kwargs)\n",
    "\n",
    "\n",
    "plot_snv_performance_metrics(snv_ont_stats, snv_illumina_stats)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snv_error_rates(sample_ids, technologies, complexities):\n",
    "    snv_error_rates = {\n",
    "        tech: {comp: {\"FP\": {}, \"FN\": {}} for comp in complexities}\n",
    "        for tech in technologies\n",
    "    }\n",
    "    snv_types = [\n",
    "        \"A>C\",\n",
    "        \"A>G\",\n",
    "        \"A>T\",\n",
    "        \"C>A\",\n",
    "        \"C>G\",\n",
    "        \"C>T\",\n",
    "        \"G>A\",\n",
    "        \"G>C\",\n",
    "        \"G>T\",\n",
    "        \"T>A\",\n",
    "        \"T>C\",\n",
    "        \"T>G\",\n",
    "    ]\n",
    "\n",
    "    sample_counts = {tech: {comp: 0 for comp in complexities} for tech in technologies}\n",
    "\n",
    "    for _, row in sample_ids.iterrows():\n",
    "        ont_id = row[\"ont_id\"]\n",
    "        lp_id = row[\"lp_id\"]\n",
    "\n",
    "        for tech in technologies:\n",
    "            for complexity in complexities:\n",
    "                sample_id = ont_id if tech == \"long-read\" else lp_id\n",
    "                fp_vcf = (\n",
    "                    f\"output/snv/rtg_vcfeval/{complexity}/{sample_id}.snv/fp.vcf.gz\"\n",
    "                )\n",
    "                fn_vcf = (\n",
    "                    f\"output/snv/rtg_vcfeval/{complexity}/{sample_id}.snv/fn.vcf.gz\"\n",
    "                )\n",
    "                query_vcf = (\n",
    "                    f\"output/snv/rtg_vcfeval/{complexity}/{sample_id}.snv/query.vcf.gz\"\n",
    "                )\n",
    "\n",
    "                if not (\n",
    "                    os.path.exists(fp_vcf)\n",
    "                    and os.path.exists(fn_vcf)\n",
    "                    and os.path.exists(query_vcf)\n",
    "                ):\n",
    "                    print(f\"VCF files not found for {sample_id}, {tech}, {complexity}\")\n",
    "                    continue\n",
    "\n",
    "                sample_counts[tech][complexity] += 1\n",
    "\n",
    "                fp_counts = count_snv_types(fp_vcf)\n",
    "                fn_counts = count_snv_types(fn_vcf)\n",
    "                total_variants = count_total_variants(query_vcf)\n",
    "\n",
    "                for snv_type in snv_types:\n",
    "                    if snv_type not in snv_error_rates[tech][complexity][\"FP\"]:\n",
    "                        snv_error_rates[tech][complexity][\"FP\"][snv_type] = []\n",
    "                        snv_error_rates[tech][complexity][\"FN\"][snv_type] = []\n",
    "\n",
    "                    fp_rate = (\n",
    "                        fp_counts.get(snv_type, 0) / total_variants\n",
    "                        if total_variants > 0\n",
    "                        else 0\n",
    "                    )\n",
    "                    fn_rate = (\n",
    "                        fn_counts.get(snv_type, 0) / total_variants\n",
    "                        if total_variants > 0\n",
    "                        else 0\n",
    "                    )\n",
    "\n",
    "                    snv_error_rates[tech][complexity][\"FP\"][snv_type].append(fp_rate)\n",
    "                    snv_error_rates[tech][complexity][\"FN\"][snv_type].append(fn_rate)\n",
    "\n",
    "    return snv_error_rates, sample_counts\n",
    "\n",
    "\n",
    "def count_snv_types(vcf_file):\n",
    "    snv_counts = {}\n",
    "    with pysam.VariantFile(vcf_file) as vcf:\n",
    "        for record in vcf:\n",
    "            ref = record.ref\n",
    "            alt = record.alts[0]\n",
    "            if len(ref) == 1 and len(alt) == 1:\n",
    "                snv_type = f\"{ref}>{alt}\"\n",
    "                snv_counts[snv_type] = snv_counts.get(snv_type, 0) + 1\n",
    "    return snv_counts\n",
    "\n",
    "\n",
    "def count_total_variants(vcf_file):\n",
    "    total_count = 0\n",
    "    with pysam.VariantFile(vcf_file) as vcf:\n",
    "        for _ in vcf:\n",
    "            total_count += 1\n",
    "    return total_count\n",
    "\n",
    "\n",
    "def prepare_snv_error_data(snv_error_rates):\n",
    "    plot_data = []\n",
    "    for tech in snv_error_rates:\n",
    "        for complexity in snv_error_rates[tech]:\n",
    "            for error_type in snv_error_rates[tech][complexity]:\n",
    "                for snv_type, rates in snv_error_rates[tech][complexity][\n",
    "                    error_type\n",
    "                ].items():\n",
    "                    plot_data.append(\n",
    "                        {\n",
    "                            \"Technology\": tech,\n",
    "                            \"Complexity\": complexity,\n",
    "                            \"Error Type\": error_type,\n",
    "                            \"SNV Type\": snv_type,\n",
    "                            \"Error Rate\": np.mean(rates),\n",
    "                        }\n",
    "                    )\n",
    "    return pd.DataFrame(plot_data)\n",
    "\n",
    "\n",
    "def perform_statistical_tests(snv_error_rates, sample_counts):\n",
    "    results = []\n",
    "    p_values = []\n",
    "\n",
    "    for complexity in snv_error_rates[\"long-read\"]:\n",
    "        for error_type in [\"FP\", \"FN\"]:\n",
    "            for snv_type in snv_error_rates[\"long-read\"][complexity][error_type]:\n",
    "                long_read_rates = snv_error_rates[\"long-read\"][complexity][error_type][\n",
    "                    snv_type\n",
    "                ]\n",
    "                short_read_rates = snv_error_rates[\"short-read\"][complexity][\n",
    "                    error_type\n",
    "                ][snv_type]\n",
    "\n",
    "                n = min(len(long_read_rates), len(short_read_rates))\n",
    "\n",
    "                t_statistic, p_value = stats.ttest_rel(\n",
    "                    long_read_rates[:n], short_read_rates[:n]\n",
    "                )\n",
    "\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"Complexity\": complexity,\n",
    "                        \"Error Type\": error_type,\n",
    "                        \"SNV Type\": snv_type,\n",
    "                        \"t-statistic\": t_statistic,\n",
    "                        \"p-value\": p_value,\n",
    "                        \"n\": n,\n",
    "                    }\n",
    "                )\n",
    "                p_values.append(p_value)\n",
    "\n",
    "    rejected, p_values_corrected, _, _ = multipletests(p_values, method=\"fdr_bh\")\n",
    "\n",
    "    for result, p_corrected, is_rejected in zip(results, p_values_corrected, rejected):\n",
    "        result[\"p-value (FDR corrected)\"] = p_corrected\n",
    "        result[\"Significance\"] = get_significance(p_corrected)\n",
    "        result[\"Rejected (FDR)\"] = is_rejected\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def get_significance(p_value):\n",
    "    if p_value < 0.001:\n",
    "        return \"***\"\n",
    "    elif p_value < 0.01:\n",
    "        return \"**\"\n",
    "    elif p_value < 0.05:\n",
    "        return \"*\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def plot_error_rates(plot_data, statistical_results, fig=None, axes=None):\n",
    "    if fig is None and axes is None:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    elif fig is None or axes is None:\n",
    "        raise ValueError(\"Both fig and axes must be provided if one is provided\")\n",
    "\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    error_types = [\"FP\", \"FN\"]\n",
    "\n",
    "    for i, complexity in enumerate(complexities):\n",
    "        for j, error_type in enumerate(error_types):\n",
    "            ax = axes[i, j]\n",
    "            subset = plot_data[\n",
    "                (plot_data[\"Complexity\"] == complexity)\n",
    "                & (plot_data[\"Error Type\"] == error_type)\n",
    "            ]\n",
    "\n",
    "            sns.barplot(\n",
    "                x=\"SNV Type\", y=\"Error Rate\", hue=\"Technology\", data=subset, ax=ax\n",
    "            )\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{'High' if complexity == 'hc' else 'Low'} Complexity - {error_type}\"\n",
    "            )\n",
    "            ax.set_xlabel(\"SNV Type\")\n",
    "            ax.set_ylabel(\"Error Rate\")\n",
    "\n",
    "            for idx, snv_type in enumerate(subset[\"SNV Type\"].unique()):\n",
    "                result = statistical_results[\n",
    "                    (statistical_results[\"Complexity\"] == complexity)\n",
    "                    & (statistical_results[\"Error Type\"] == error_type)\n",
    "                    & (statistical_results[\"SNV Type\"] == snv_type)\n",
    "                ]\n",
    "                if not result.empty:\n",
    "                    significance = result[\"Significance\"].values[0]\n",
    "                    max_height = subset[subset[\"SNV Type\"] == snv_type][\n",
    "                        \"Error Rate\"\n",
    "                    ].max()\n",
    "                    ax.text(\n",
    "                        idx,\n",
    "                        max_height,\n",
    "                        significance,\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "sample_ids = pd.read_csv(\"sample_ids.csv\")\n",
    "technologies = [\"long-read\", \"short-read\"]\n",
    "complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "snv_error_rates, sample_counts = calculate_snv_error_rates(\n",
    "    sample_ids, technologies, complexities\n",
    ")\n",
    "plot_data = prepare_snv_error_data(snv_error_rates)\n",
    "statistical_results = perform_statistical_tests(snv_error_rates, sample_counts)\n",
    "\n",
    "plot_error_rates(plot_data, statistical_results)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistical_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_snv_metrics_plot(\n",
    "    snv_ont_stats, snv_illumina_stats, plot_data, statistical_results\n",
    "):\n",
    "    fig = plt.figure(figsize=(12, 14), dpi=300)\n",
    "    gs = fig.add_gridspec(3, 2)\n",
    "\n",
    "    def add_significance(ax, data, y_offset=0.02):\n",
    "        for i, metric in enumerate(data[\"Metric\"].unique()):\n",
    "            metric_data = data[data[\"Metric\"] == metric]\n",
    "            significance = metric_data[\"Significance\"].iloc[0]\n",
    "            if significance:\n",
    "                y = max(metric_data[\"Value\"]) + y_offset\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    y,\n",
    "                    significance,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    color=\"black\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    # A and B: SNV Performance Metrics\n",
    "    fig.text(0.5, 1.00, \"SNV Performance\", fontsize=12, ha=\"center\")\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "    for i, complexity in enumerate(complexities):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "\n",
    "        data = []\n",
    "        for metric in metrics:\n",
    "            ont_value = snv_ont_stats.loc[complexity, (metric, \"mean\")]\n",
    "            illumina_value = snv_illumina_stats.loc[complexity, (metric, \"mean\")]\n",
    "            p_value = snv_ont_stats.loc[complexity, (metric, \"adjusted_p_value\")]\n",
    "            significance = (\n",
    "                \"***\"\n",
    "                if p_value < 0.001\n",
    "                else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"\"))\n",
    "            )\n",
    "\n",
    "            data.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"Complexity\": complexity.upper(),\n",
    "                        \"Metric\": metric,\n",
    "                        \"Technology\": \"long-read\",\n",
    "                        \"Value\": ont_value,\n",
    "                        \"Significance\": significance,\n",
    "                    },\n",
    "                    {\n",
    "                        \"Complexity\": complexity.upper(),\n",
    "                        \"Metric\": metric,\n",
    "                        \"Technology\": \"short-read\",\n",
    "                        \"Value\": illumina_value,\n",
    "                        \"Significance\": significance,\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        sns.barplot(x=\"Metric\", y=\"Value\", hue=\"Technology\", data=df, ax=ax)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(\n",
    "            f\"{'High' if complexity == 'hc' else 'Low'} Complexity\",  # fontsize=14\n",
    "        )\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Performance\" if i == 0 else \"\")\n",
    "\n",
    "        if i == 1:\n",
    "            ax.legend(title=\"Technology\", loc=\"upper right\")\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "        add_significance(ax, df)\n",
    "        ax.text(\n",
    "            -0.05,\n",
    "            1.05,\n",
    "            \"AB\"[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "    # C, D, E, F: Error Rates\n",
    "    fig.text(0.5, 0.665, \"SNV Error Analysis\", fontsize=12, ha=\"center\")\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    error_types = [\"FP\", \"FN\"]\n",
    "\n",
    "    for i, complexity in enumerate(complexities):\n",
    "        for j, error_type in enumerate(error_types):\n",
    "            ax = fig.add_subplot(gs[i + 1, j])\n",
    "\n",
    "            subset = plot_data[\n",
    "                (plot_data[\"Complexity\"] == complexity)\n",
    "                & (plot_data[\"Error Type\"] == error_type)\n",
    "            ]\n",
    "            sns.barplot(\n",
    "                x=\"SNV Type\", y=\"Error Rate\", hue=\"Technology\", data=subset, ax=ax\n",
    "            )\n",
    "\n",
    "            ax.set_title(\n",
    "                f\"{'High' if complexity == 'hc' else 'Low'} Complexity - {error_type}\"\n",
    "            )\n",
    "            ax.set_xlabel(\"SNV Type\")\n",
    "            ax.set_ylabel(\"Error Rate\")\n",
    "            ax.get_legend().remove()\n",
    "\n",
    "            for idx, snv_type in enumerate(subset[\"SNV Type\"].unique()):\n",
    "                result = statistical_results[\n",
    "                    (statistical_results[\"Complexity\"] == complexity)\n",
    "                    & (statistical_results[\"Error Type\"] == error_type)\n",
    "                    & (statistical_results[\"SNV Type\"] == snv_type)\n",
    "                ]\n",
    "                if not result.empty:\n",
    "                    significance = result[\"Significance\"].values[0]\n",
    "                    max_height = subset[subset[\"SNV Type\"] == snv_type][\n",
    "                        \"Error Rate\"\n",
    "                    ].max()\n",
    "                    ax.text(\n",
    "                        idx,\n",
    "                        max_height,\n",
    "                        significance,\n",
    "                        ha=\"center\",\n",
    "                        va=\"bottom\",\n",
    "                        fontweight=\"bold\",\n",
    "                    )\n",
    "\n",
    "            ax.text(\n",
    "                -0.05,\n",
    "                1.05,\n",
    "                chr(ord(\"C\") + i * 2 + j),\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=16,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "combined_fig = create_combined_snv_metrics_plot(\n",
    "    snv_ont_stats, snv_illumina_stats, plot_data, statistical_results\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indel Benchmark\n",
    "\n",
    "## Comparative analysis\n",
    "\n",
    "### 1. Sensitivity, Precision, and F1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_indel_metrics(indel_sample_ids, indel_complexities):\n",
    "    indel_metrics_collection = {\"ont\": {\"hc\": [], \"lc\": []}}\n",
    "\n",
    "    for _, row in indel_sample_ids.iterrows():\n",
    "        indel_ont_id = row[\"ont_id\"]\n",
    "\n",
    "        for indel_complexity in indel_complexities:\n",
    "            indel_summary_file = f\"output/indel/rtg_vcfeval/{indel_complexity}/{indel_ont_id}.indel/summary.txt\"\n",
    "            indel_summary = read_summary(indel_summary_file)\n",
    "\n",
    "            if indel_summary:\n",
    "                indel_metrics_entry = {\n",
    "                    \"sample_id\": indel_ont_id,\n",
    "                    \"complexity\": indel_complexity,\n",
    "                    **indel_summary,\n",
    "                }\n",
    "                indel_metrics_collection[\"ont\"][indel_complexity].append(\n",
    "                    indel_metrics_entry\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Skipping empty summary for {indel_ont_id}, {indel_complexity}\")\n",
    "\n",
    "    return indel_metrics_collection\n",
    "\n",
    "\n",
    "def process_indel_metrics(indel_metrics_collection):\n",
    "    indel_ont_metrics_df = pd.DataFrame(\n",
    "        indel_metrics_collection[\"ont\"][\"hc\"] + indel_metrics_collection[\"ont\"][\"lc\"]\n",
    "    )\n",
    "    indel_ont_stats = calculate_rtg_statistics(indel_ont_metrics_df)\n",
    "    return indel_ont_metrics_df, indel_ont_stats\n",
    "\n",
    "\n",
    "def plot_indel_performance_metrics(indel_ont_stats):\n",
    "    indel_metrics_list = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    indel_complexities_list = [\"hc\", \"lc\"]\n",
    "\n",
    "    indel_plot_data = []\n",
    "    for indel_complexity in indel_complexities_list:\n",
    "        for indel_metric in indel_metrics_list:\n",
    "            indel_value = indel_ont_stats.loc[indel_complexity, (indel_metric, \"mean\")]\n",
    "            indel_plot_data.append(\n",
    "                {\n",
    "                    \"Complexity\": indel_complexity.upper(),\n",
    "                    \"Metric\": indel_metric,\n",
    "                    \"Value\": indel_value,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    indel_df = pd.DataFrame(indel_plot_data)\n",
    "\n",
    "    indel_fig, (indel_ax1, indel_ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    for indel_ax, indel_comp in zip([indel_ax1, indel_ax2], [\"HC\", \"LC\"]):\n",
    "        sns.barplot(\n",
    "            x=\"Metric\",\n",
    "            y=\"Value\",\n",
    "            data=indel_df[indel_df[\"Complexity\"] == indel_comp],\n",
    "            ax=indel_ax,\n",
    "        )\n",
    "        indel_ax.set_title(\n",
    "            f\"{'High' if indel_comp == 'HC' else 'Low'} Complexity\", fontsize=14\n",
    "        )\n",
    "        indel_ax.set_ylim(0, 1.05)\n",
    "        indel_ax.set_xlabel(\"\")\n",
    "        indel_ax.set_ylabel(\"Performance (%)\", fontsize=12)\n",
    "\n",
    "        for container in indel_ax.containers:\n",
    "            indel_ax.bar_label(container, fmt=\"%.3f\", padding=3)\n",
    "\n",
    "    indel_fig.suptitle(\"Indel Performance for ONT\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "indel_sample_ids = pd.read_csv(\"sample_ids.csv\")\n",
    "indel_complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "indel_metrics = collect_indel_metrics(indel_sample_ids, indel_complexities)\n",
    "indel_ont_metrics_df, indel_ont_stats = process_indel_metrics(indel_metrics)\n",
    "\n",
    "print(\"ONT Indel Stats:\")\n",
    "indel_ont_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indel_performance_metrics(indel_ont_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Error Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_indel_error_metrics(indel_sample_ids, indel_complexities):\n",
    "    indel_error_metrics = {\n",
    "        comp: defaultdict(lambda: defaultdict(int)) for comp in indel_complexities\n",
    "    }\n",
    "    indel_total_variants = {comp: 0 for comp in indel_complexities}\n",
    "\n",
    "    for _, row in indel_sample_ids.iterrows():\n",
    "        indel_ont_id = row[\"ont_id\"]\n",
    "\n",
    "        for indel_complexity in indel_complexities:\n",
    "            base_path = (\n",
    "                f\"output/indel/rtg_vcfeval/{indel_complexity}/{indel_ont_id}.indel\"\n",
    "            )\n",
    "            fp_file = os.path.join(base_path, \"fp.vcf.gz\")\n",
    "            fn_file = os.path.join(base_path, \"fn.vcf.gz\")\n",
    "            tp_file = os.path.join(base_path, \"tp.vcf.gz\")\n",
    "\n",
    "            with pysam.VariantFile(tp_file) as vcf:\n",
    "                indel_total_variants[indel_complexity] += sum(1 for _ in vcf)\n",
    "\n",
    "            with pysam.VariantFile(fp_file) as vcf:\n",
    "                for record in vcf:\n",
    "                    indel_type = (\n",
    "                        \"insertion\"\n",
    "                        if len(record.alts[0]) > len(record.ref)\n",
    "                        else \"deletion\"\n",
    "                    )\n",
    "                    indel_length = abs(len(record.alts[0]) - len(record.ref))\n",
    "                    indel_error_metrics[indel_complexity][\"FP\"][\n",
    "                        f\"{indel_type}_{indel_length}\"\n",
    "                    ] += 1\n",
    "                    indel_total_variants[indel_complexity] += 1\n",
    "\n",
    "            with pysam.VariantFile(fn_file) as vcf:\n",
    "                for record in vcf:\n",
    "                    indel_type = (\n",
    "                        \"insertion\"\n",
    "                        if len(record.alts[0]) > len(record.ref)\n",
    "                        else \"deletion\"\n",
    "                    )\n",
    "                    indel_length = abs(len(record.alts[0]) - len(record.ref))\n",
    "                    indel_error_metrics[indel_complexity][\"FN\"][\n",
    "                        f\"{indel_type}_{indel_length}\"\n",
    "                    ] += 1\n",
    "                    indel_total_variants[indel_complexity] += 1\n",
    "\n",
    "    return indel_error_metrics, indel_total_variants\n",
    "\n",
    "\n",
    "def process_indel_error_metrics(indel_error_metrics, indel_total_variants):\n",
    "    indel_error_plot_data = []\n",
    "    for indel_complexity, error_data in indel_error_metrics.items():\n",
    "        total_variants = indel_total_variants[indel_complexity]\n",
    "        for error_type, indel_types in error_data.items():\n",
    "            for indel_type, count in indel_types.items():\n",
    "                error_rate = (count / total_variants) * 100\n",
    "                indel_error_plot_data.append(\n",
    "                    {\n",
    "                        \"Complexity\": indel_complexity,\n",
    "                        \"Error Type\": error_type,\n",
    "                        \"Indel Type\": indel_type,\n",
    "                        \"Error Rate (%)\": error_rate,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    indel_error_df = pd.DataFrame(indel_error_plot_data)\n",
    "    indel_error_df[\"Indel Length\"] = (\n",
    "        indel_error_df[\"Indel Type\"].str.split(\"_\").str[-1].astype(int)\n",
    "    )\n",
    "    indel_error_df[\"Indel Category\"] = (\n",
    "        indel_error_df[\"Indel Type\"].str.split(\"_\").str[0]\n",
    "    )\n",
    "    indel_error_df = indel_error_df.sort_values(\n",
    "        [\"Indel Category\", \"Indel Length\", \"Complexity\"]\n",
    "    )\n",
    "\n",
    "    return indel_error_df\n",
    "\n",
    "\n",
    "def plot_indel_error_metrics(indel_error_df):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    for i, error_type in enumerate([\"FP\", \"FN\"]):\n",
    "        for j, indel_category in enumerate([\"insertion\", \"deletion\"]):\n",
    "            data = indel_error_df[\n",
    "                (indel_error_df[\"Error Type\"] == error_type)\n",
    "                & (indel_error_df[\"Indel Category\"] == indel_category)\n",
    "            ]\n",
    "\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x=\"Indel Length\",\n",
    "                y=\"Error Rate (%)\",\n",
    "                hue=\"Complexity\",\n",
    "                hue_order=[\"hc\", \"lc\"],\n",
    "                ax=axes[j, i],\n",
    "            )\n",
    "\n",
    "            axes[j, i].set_title(f\"{error_type} - {indel_category.capitalize()}s\")\n",
    "            axes[j, i].set_xlabel(f\"{indel_category.capitalize()} Length\")\n",
    "            axes[j, i].set_ylabel(\"Error Rate (%)\")\n",
    "\n",
    "            unique_lengths = sorted(data[\"Indel Length\"].unique())\n",
    "            axes[j, i].set_xticks(range(len(unique_lengths)))\n",
    "            axes[j, i].set_xticklabels(unique_lengths)\n",
    "\n",
    "            axes[j, i].tick_params(axis=\"x\", rotation=90)\n",
    "            axes[j, i].legend(title=\"Complexity\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_indel_statistical_test(\n",
    "    hc_data, lc_data, hc_total, lc_total, error_type, indel_type\n",
    "):\n",
    "    hc_error = hc_data[error_type].get(indel_type, 0)\n",
    "    lc_error = lc_data[error_type].get(indel_type, 0)\n",
    "\n",
    "    contingency_table = [\n",
    "        [hc_error, hc_total - hc_error],\n",
    "        [lc_error, lc_total - lc_error],\n",
    "    ]\n",
    "\n",
    "    if any(any(cell < 5 for cell in row) for row in contingency_table):\n",
    "        _, p_value = stats.fisher_exact(contingency_table)\n",
    "    else:\n",
    "        _, p_value, _, _ = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def analyze_indel_error_statistics(indel_error_metrics, indel_total_variants):\n",
    "    hc_data = indel_error_metrics[\"hc\"]\n",
    "    lc_data = indel_error_metrics[\"lc\"]\n",
    "    hc_total = indel_total_variants[\"hc\"]\n",
    "    lc_total = indel_total_variants[\"lc\"]\n",
    "\n",
    "    indel_stat_results = []\n",
    "\n",
    "    for error_type in [\"FP\", \"FN\"]:\n",
    "        indel_types = set(hc_data[error_type].keys()) | set(lc_data[error_type].keys())\n",
    "\n",
    "        for indel_type in indel_types:\n",
    "            p_value = perform_indel_statistical_test(\n",
    "                hc_data, lc_data, hc_total, lc_total, error_type, indel_type\n",
    "            )\n",
    "            hc_rate = hc_data[error_type].get(indel_type, 0) / hc_total * 100\n",
    "            lc_rate = lc_data[error_type].get(indel_type, 0) / lc_total * 100\n",
    "\n",
    "            indel_stat_results.append(\n",
    "                {\n",
    "                    \"Error Type\": error_type,\n",
    "                    \"Indel Type\": indel_type,\n",
    "                    \"HC Rate (%)\": hc_rate,\n",
    "                    \"LC Rate (%)\": lc_rate,\n",
    "                    \"p-value\": p_value,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    indel_stat_df = pd.DataFrame(indel_stat_results)\n",
    "\n",
    "    # Perform FDR correction\n",
    "    _, p_values_corrected, _, _ = multipletests(\n",
    "        indel_stat_df[\"p-value\"], method=\"fdr_bh\"\n",
    "    )\n",
    "    indel_stat_df[\"Adjusted p-value\"] = p_values_corrected\n",
    "\n",
    "    indel_stat_df = indel_stat_df.sort_values([\"Error Type\", \"Indel Type\"])\n",
    "\n",
    "    indel_stat_df[\"HC Rate (%)\"] = indel_stat_df[\"HC Rate (%)\"].map(\"{:.4f}\".format)\n",
    "    indel_stat_df[\"LC Rate (%)\"] = indel_stat_df[\"LC Rate (%)\"].map(\"{:.4f}\".format)\n",
    "    indel_stat_df[\"p-value\"] = indel_stat_df[\"p-value\"].map(\"{:.4e}\".format)\n",
    "    indel_stat_df[\"Adjusted p-value\"] = indel_stat_df[\"Adjusted p-value\"].map(\n",
    "        \"{:.4e}\".format\n",
    "    )\n",
    "\n",
    "    indel_stat_df[\"Significance\"] = indel_stat_df[\"Adjusted p-value\"].apply(\n",
    "        lambda x: (\n",
    "            \"***\"\n",
    "            if float(x) < 0.001\n",
    "            else (\"**\" if float(x) < 0.01 else (\"*\" if float(x) < 0.05 else \"\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return indel_stat_df\n",
    "\n",
    "\n",
    "indel_sample_ids = pd.read_csv(\"sample_ids.csv\")\n",
    "indel_complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "indel_error_metrics, indel_total_variants = collect_indel_error_metrics(\n",
    "    indel_sample_ids, indel_complexities\n",
    ")\n",
    "indel_error_df = process_indel_error_metrics(indel_error_metrics, indel_total_variants)\n",
    "\n",
    "plot_indel_error_metrics(indel_error_df)\n",
    "\n",
    "indel_stat_df = analyze_indel_error_statistics(\n",
    "    indel_error_metrics, indel_total_variants\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n",
    "    print(\"Indel Error Analysis DataFrame:\")\n",
    "    display(indel_error_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Size Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_indel_size_distribution(indel_complexity, indel_sample_ids):\n",
    "    indel_ont_sizes = defaultdict(lambda: defaultdict(int))\n",
    "    indel_illumina_sizes = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for _, row in indel_sample_ids.iterrows():\n",
    "        indel_sample_id = row[\"ont_id\"]\n",
    "        indel_base_path = (\n",
    "            f\"output/indel/rtg_vcfeval/{indel_complexity}/{indel_sample_id}.indel\"\n",
    "        )\n",
    "\n",
    "        indel_query_file = os.path.join(indel_base_path, \"query.vcf.gz\")\n",
    "        indel_truth_file = os.path.join(indel_base_path, \"truth.vcf.gz\")\n",
    "\n",
    "        with pysam.VariantFile(indel_query_file) as vcf:\n",
    "            for record in vcf:\n",
    "                indel_type = (\n",
    "                    \"insertion\" if len(record.alts[0]) > len(record.ref) else \"deletion\"\n",
    "                )\n",
    "                indel_length = abs(len(record.alts[0]) - len(record.ref))\n",
    "                indel_ont_sizes[indel_type][indel_length] += 1\n",
    "\n",
    "        with pysam.VariantFile(indel_truth_file) as vcf:\n",
    "            for record in vcf:\n",
    "                indel_type = (\n",
    "                    \"insertion\" if len(record.alts[0]) > len(record.ref) else \"deletion\"\n",
    "                )\n",
    "                indel_length = abs(len(record.alts[0]) - len(record.ref))\n",
    "                indel_illumina_sizes[indel_type][indel_length] += 1\n",
    "\n",
    "    return indel_ont_sizes, indel_illumina_sizes\n",
    "\n",
    "\n",
    "def prepare_indel_plot_data(indel_ont_sizes, indel_illumina_sizes, indel_complexity):\n",
    "    indel_plot_data = []\n",
    "    for indel_type in [\"insertion\", \"deletion\"]:\n",
    "        indel_ont_total = sum(indel_ont_sizes[indel_type].values())\n",
    "        indel_illumina_total = sum(indel_illumina_sizes[indel_type].values())\n",
    "        indel_max_size = max(\n",
    "            max(indel_ont_sizes[indel_type].keys()),\n",
    "            max(indel_illumina_sizes[indel_type].keys()),\n",
    "        )\n",
    "\n",
    "        for indel_size in range(1, indel_max_size + 1):\n",
    "            indel_ont_percent = (\n",
    "                (indel_ont_sizes[indel_type][indel_size] / indel_ont_total) * 100\n",
    "                if indel_ont_total > 0\n",
    "                else 0\n",
    "            )\n",
    "            indel_illumina_percent = (\n",
    "                (indel_illumina_sizes[indel_type][indel_size] / indel_illumina_total)\n",
    "                * 100\n",
    "                if indel_illumina_total > 0\n",
    "                else 0\n",
    "            )\n",
    "\n",
    "            indel_plot_data.append(\n",
    "                {\n",
    "                    \"Indel Type\": indel_type,\n",
    "                    \"Size\": indel_size,\n",
    "                    \"Percentage\": indel_ont_percent,\n",
    "                    \"Platform\": \"ONT\",\n",
    "                    \"Complexity\": indel_complexity,\n",
    "                }\n",
    "            )\n",
    "            indel_plot_data.append(\n",
    "                {\n",
    "                    \"Indel Type\": indel_type,\n",
    "                    \"Size\": indel_size,\n",
    "                    \"Percentage\": indel_illumina_percent,\n",
    "                    \"Platform\": \"Illumina\",\n",
    "                    \"Complexity\": indel_complexity,\n",
    "                }\n",
    "            )\n",
    "    return indel_plot_data\n",
    "\n",
    "\n",
    "def plot_indel_size_distribution(indel_plot_data):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    indel_types = [\"insertion\", \"deletion\"]\n",
    "    indel_complexities = [\"High Complexity\", \"Low Complexity\"]\n",
    "\n",
    "    for i, indel_type in enumerate(indel_types):\n",
    "        for j, indel_complexity in enumerate(indel_complexities):\n",
    "            data = indel_plot_data[\n",
    "                (indel_plot_data[\"Indel Type\"] == indel_type)\n",
    "                & (indel_plot_data[\"Complexity\"] == indel_complexity)\n",
    "            ]\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=data,\n",
    "                x=\"Size\",\n",
    "                y=\"Percentage\",\n",
    "                hue=\"Platform\",\n",
    "                ax=axes[i, j],\n",
    "                marker=\"o\",\n",
    "            )\n",
    "\n",
    "            axes[i, j].set_title(f\"{indel_complexity} - {indel_type.capitalize()}s\")\n",
    "            axes[i, j].set_xlabel(\"Indel Size\")\n",
    "            axes[i, j].set_ylabel(\"Percentage\")\n",
    "            axes[i, j].legend(title=\"Platform\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_indel_summary_stats(indel_sizes):\n",
    "    indel_total = sum(indel_sizes.values())\n",
    "    if indel_total == 0:\n",
    "        return {\"total\": 0, \"mean\": 0, \"median\": 0, \"std\": 0}\n",
    "\n",
    "    indel_sizes_array = np.array([(k, v) for k, v in indel_sizes.items()])\n",
    "    indel_mean = np.average(indel_sizes_array[:, 0], weights=indel_sizes_array[:, 1])\n",
    "    indel_median = np.median(\n",
    "        np.repeat(indel_sizes_array[:, 0], indel_sizes_array[:, 1].astype(int))\n",
    "    )\n",
    "    indel_std = np.sqrt(\n",
    "        np.average(\n",
    "            (indel_sizes_array[:, 0] - indel_mean) ** 2, weights=indel_sizes_array[:, 1]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"total\": indel_total,\n",
    "        \"mean\": indel_mean,\n",
    "        \"median\": indel_median,\n",
    "        \"std\": indel_std,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_indel_summary_stats(indel_ont_sizes, indel_illumina_sizes, indel_complexity):\n",
    "    print(f\"\\n{indel_complexity}\")\n",
    "    for indel_type in [\"insertion\", \"deletion\"]:\n",
    "        indel_ont_stats = calculate_indel_summary_stats(indel_ont_sizes[indel_type])\n",
    "        indel_illumina_stats = calculate_indel_summary_stats(\n",
    "            indel_illumina_sizes[indel_type]\n",
    "        )\n",
    "\n",
    "        print(f\"  {indel_type.capitalize()}s:\")\n",
    "        print(\n",
    "            f\"    ONT     - Total: {indel_ont_stats['total']}, Mean Size: {indel_ont_stats['mean']:.2f}, Std Dev: {indel_ont_stats['std']:.2f}, Median Size: {indel_ont_stats['median']}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"    Illumina - Total: {indel_illumina_stats['total']}, Mean Size: {indel_illumina_stats['mean']:.2f}, Std Dev: {indel_illumina_stats['std']:.2f}, Median Size: {indel_illumina_stats['median']}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def compare_indel_distributions(indel_ont_sizes, indel_illumina_sizes):\n",
    "    indel_ont_dist = [\n",
    "        size for size, count in indel_ont_sizes.items() for _ in range(count)\n",
    "    ]\n",
    "    indel_illumina_dist = [\n",
    "        size for size, count in indel_illumina_sizes.items() for _ in range(count)\n",
    "    ]\n",
    "\n",
    "    ks_statistic, p_value = stats.ks_2samp(indel_ont_dist, indel_illumina_dist)\n",
    "    return ks_statistic, p_value\n",
    "\n",
    "\n",
    "def perform_indel_statistical_tests(\n",
    "    indel_hc_ont_sizes,\n",
    "    indel_hc_illumina_sizes,\n",
    "    indel_lc_ont_sizes,\n",
    "    indel_lc_illumina_sizes,\n",
    "):\n",
    "    indel_all_p_values = []\n",
    "    indel_test_results = []\n",
    "\n",
    "    for indel_complexity, (indel_ont_sizes, indel_illumina_sizes) in [\n",
    "        (\"High Complexity\", (indel_hc_ont_sizes, indel_hc_illumina_sizes)),\n",
    "        (\"Low Complexity\", (indel_lc_ont_sizes, indel_lc_illumina_sizes)),\n",
    "    ]:\n",
    "        for indel_type in [\"insertion\", \"deletion\"]:\n",
    "            ks_statistic, p_value = compare_indel_distributions(\n",
    "                indel_ont_sizes[indel_type], indel_illumina_sizes[indel_type]\n",
    "            )\n",
    "            indel_all_p_values.append(p_value)\n",
    "            indel_test_results.append(\n",
    "                (indel_complexity, indel_type, ks_statistic, p_value)\n",
    "            )\n",
    "\n",
    "    _, indel_corrected_p_values, _, _ = multipletests(\n",
    "        indel_all_p_values, method=\"fdr_bh\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nStatistical Test Results (Kolmogorov-Smirnov test with FDR correction):\")\n",
    "    for (indel_complexity, indel_type, ks_statistic, p_value), corrected_p_value in zip(\n",
    "        indel_test_results, indel_corrected_p_values\n",
    "    ):\n",
    "        print(f\"\\n{indel_complexity} - {indel_type.capitalize()}s:\")\n",
    "        print(f\"    KS statistic: {ks_statistic:.4f}\")\n",
    "        print(f\"    Original p-value: {p_value:.4e}\")\n",
    "        print(f\"    Corrected p-value: {corrected_p_value:.4e}\")\n",
    "\n",
    "\n",
    "indel_sample_ids = pd.read_csv(\"sample_ids.csv\")\n",
    "indel_complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "indel_hc_ont_sizes, indel_hc_illumina_sizes = analyze_indel_size_distribution(\n",
    "    \"hc\", indel_sample_ids\n",
    ")\n",
    "indel_lc_ont_sizes, indel_lc_illumina_sizes = analyze_indel_size_distribution(\n",
    "    \"lc\", indel_sample_ids\n",
    ")\n",
    "\n",
    "indel_hc_plot_data = prepare_indel_plot_data(\n",
    "    indel_hc_ont_sizes, indel_hc_illumina_sizes, \"High Complexity\"\n",
    ")\n",
    "indel_lc_plot_data = prepare_indel_plot_data(\n",
    "    indel_lc_ont_sizes, indel_lc_illumina_sizes, \"Low Complexity\"\n",
    ")\n",
    "\n",
    "indel_plot_data = pd.DataFrame(indel_hc_plot_data + indel_lc_plot_data)\n",
    "\n",
    "plot_indel_size_distribution(indel_plot_data)\n",
    "\n",
    "print(\"\\nIndel Size Distribution Summary Statistics:\")\n",
    "print_indel_summary_stats(\n",
    "    indel_hc_ont_sizes, indel_hc_illumina_sizes, \"High Complexity\"\n",
    ")\n",
    "print_indel_summary_stats(indel_lc_ont_sizes, indel_lc_illumina_sizes, \"Low Complexity\")\n",
    "\n",
    "perform_indel_statistical_tests(\n",
    "    indel_hc_ont_sizes,\n",
    "    indel_hc_illumina_sizes,\n",
    "    indel_lc_ont_sizes,\n",
    "    indel_lc_illumina_sizes,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_indel_plot(indel_ont_stats, indel_error_df, indel_plot_data):\n",
    "    fig = plt.figure(figsize=(12, 16), dpi=300)\n",
    "    gs = fig.add_gridspec(5, 2)\n",
    "\n",
    "    def add_significance(ax, data, y_offset=0.02):\n",
    "        for i, metric in enumerate(data[\"Metric\"].unique()):\n",
    "            metric_data = data[data[\"Metric\"] == metric]\n",
    "            significance = metric_data[\"Significance\"].iloc[0]\n",
    "            if significance:\n",
    "                y = max(metric_data[\"Value\"]) + y_offset\n",
    "                ax.text(\n",
    "                    i,\n",
    "                    y,\n",
    "                    significance,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    color=\"black\",\n",
    "                    fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    # A and B: Indel Performance for ONT\n",
    "    fig.text(0.5, 1.00, \"ONT Indel Performance\", fontsize=12, ha=\"center\")\n",
    "\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "\n",
    "    for i, complexity in enumerate(complexities):\n",
    "        ax = fig.add_subplot(gs[0, i])\n",
    "\n",
    "        data = []\n",
    "        for metric in metrics:\n",
    "            value = indel_ont_stats.loc[complexity, (metric, \"mean\")]\n",
    "            data.append(\n",
    "                {\"Complexity\": complexity.upper(), \"Metric\": metric, \"Value\": value}\n",
    "            )\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "        sns.barplot(x=\"Metric\", y=\"Value\", data=df, ax=ax, legend=False)\n",
    "        ax.set_ylim(0, 1.05)\n",
    "        ax.set_title(f\"{'High' if complexity == 'hc' else 'Low'} Complexity\")\n",
    "        ax.set_xlabel(\"\")\n",
    "        ax.set_ylabel(\"Performance\" if i == 0 else \"\")\n",
    "\n",
    "        ax.text(\n",
    "            -0.05,\n",
    "            1.1,\n",
    "            \"AB\"[i],\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "    # C, D, E, F: Indel Size Distribution\n",
    "    fig.text(0.5, 0.80, \"Indel Size Distribution\", fontsize=12, ha=\"center\")\n",
    "\n",
    "    indel_types = [\"insertion\", \"deletion\"]\n",
    "    complexities = [\"High Complexity\", \"Low Complexity\"]\n",
    "\n",
    "    plot_data = indel_plot_data.copy()\n",
    "    plot_data[\"Platform\"] = plot_data[\"Platform\"].replace(\n",
    "        {\"ONT\": \"long-read\", \"Illumina\": \"short-read\"}\n",
    "    )\n",
    "\n",
    "    for i, indel_type in enumerate(indel_types):\n",
    "        for j, complexity in enumerate(complexities):\n",
    "            ax = fig.add_subplot(gs[i + 1, j])\n",
    "\n",
    "            data = plot_data[\n",
    "                (plot_data[\"Indel Type\"] == indel_type)\n",
    "                & (plot_data[\"Complexity\"] == complexity)\n",
    "            ]\n",
    "\n",
    "            sns.lineplot(\n",
    "                data=data,\n",
    "                x=\"Size\",\n",
    "                y=\"Percentage\",\n",
    "                hue=\"Platform\",\n",
    "                ax=ax,\n",
    "                marker=\"o\",\n",
    "                legend=(i == 0 and j == 1),  # Only show legend for plot D\n",
    "            )\n",
    "\n",
    "            ax.set_title(f\"{complexity} - {indel_type.capitalize()}s\")\n",
    "            ax.set_xlabel(\"Indel Size\")\n",
    "            ax.set_ylabel(\"Percentage\")\n",
    "\n",
    "            if i == 0 and j == 1:  # Only keep legend for plot D\n",
    "                ax.legend(title=\"Platform\")\n",
    "            else:\n",
    "                ax.get_legend().remove() if ax.get_legend() else None\n",
    "\n",
    "            ax.text(\n",
    "                -0.05,\n",
    "                1.1,\n",
    "                chr(ord(\"C\") + i * 2 + j),\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=16,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "    # G, H, I, J: Indel Error Analysis\n",
    "    fig.text(0.5, 0.405, \"Indel Error Analysis\", fontsize=12, ha=\"center\")\n",
    "\n",
    "    error_types = [\"FP\", \"FN\"]\n",
    "    indel_categories = [\"insertion\", \"deletion\"]\n",
    "\n",
    "    error_data = indel_error_df.copy()\n",
    "    error_data[\"Complexity\"] = error_data[\"Complexity\"].replace(\n",
    "        {\"hc\": \"high complexity\", \"lc\": \"low complexity\"}\n",
    "    )\n",
    "\n",
    "    for i, error_type in enumerate(error_types):\n",
    "        for j, indel_category in enumerate(indel_categories):\n",
    "            ax = fig.add_subplot(gs[i + 3, j])\n",
    "\n",
    "            data = error_data[\n",
    "                (error_data[\"Error Type\"] == error_type)\n",
    "                & (error_data[\"Indel Category\"] == indel_category)\n",
    "            ]\n",
    "\n",
    "            sns.barplot(\n",
    "                data=data,\n",
    "                x=\"Indel Length\",\n",
    "                y=\"Error Rate (%)\",\n",
    "                hue=\"Complexity\",\n",
    "                hue_order=[\"high complexity\", \"low complexity\"],\n",
    "                ax=ax,\n",
    "                legend=(i == 0 and j == 1),  # Only show legend for plot H\n",
    "            )\n",
    "\n",
    "            ax.set_title(f\"{error_type} - {indel_category.capitalize()}s\")\n",
    "            ax.set_xlabel(f\"{indel_category.capitalize()} Length\")\n",
    "            ax.set_ylabel(\"Error Rate (%)\")\n",
    "\n",
    "            unique_lengths = sorted(data[\"Indel Length\"].unique())\n",
    "            ax.set_xticks(range(len(unique_lengths)))\n",
    "            ax.set_xticklabels(unique_lengths)\n",
    "\n",
    "            ax.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "            if i == 0 and j == 1:  # Only keep legend for plot H\n",
    "                ax.legend(title=\"Complexity\")\n",
    "            else:\n",
    "                ax.get_legend().remove() if ax.get_legend() else None\n",
    "\n",
    "            ax.text(\n",
    "                -0.05,\n",
    "                1.1,\n",
    "                chr(ord(\"G\") + i * 2 + j),\n",
    "                transform=ax.transAxes,\n",
    "                fontsize=16,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "combined_indel_fig = create_combined_indel_plot(\n",
    "    indel_ont_stats, indel_error_df, indel_plot_data\n",
    ")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impacts of sequencing on SNV and Indel variant calling\n",
    "\n",
    "### 1. Impact of multiplexing on variant calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_comparison_plot(metrics_df, multiplexing_df):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        \"Performance Metrics: Multiplexed vs Singleplexed Samples\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "    for row, complexity in enumerate(complexities):\n",
    "        merged_df = pd.merge(\n",
    "            metrics_df[metrics_df[\"complexity\"] == complexity],\n",
    "            multiplexing_df[[\"sample\", \"multiplexing\"]],\n",
    "            left_on=\"sample_id\",\n",
    "            right_on=\"sample\",\n",
    "        )\n",
    "\n",
    "        for col, metric in enumerate(metrics):\n",
    "            ax = axes[row, col]\n",
    "            sns.violinplot(x=\"multiplexing\", y=metric, data=merged_df, ax=ax)\n",
    "            ax.set_title(f\"{metric} ({complexity.upper()})\")\n",
    "            ax.set_xlabel(\"Multiplexing\")\n",
    "            ax.set_ylabel(metric)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "performance_comparison_fig = create_performance_comparison_plot(\n",
    "    snv_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indel_performance_comparison_plot(indel_metrics_df, multiplexing_df):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\n",
    "        \"Indel Performance Metrics: Multiplexed vs Singleplexed Samples\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "\n",
    "    for row, complexity in enumerate(complexities):\n",
    "        merged_df = pd.merge(\n",
    "            indel_metrics_df[indel_metrics_df[\"complexity\"] == complexity],\n",
    "            multiplexing_df[[\"sample\", \"multiplexing\"]],\n",
    "            left_on=\"sample_id\",\n",
    "            right_on=\"sample\",\n",
    "        )\n",
    "\n",
    "        for col, metric in enumerate(metrics):\n",
    "            ax = axes[row, col]\n",
    "            sns.violinplot(x=\"multiplexing\", y=metric, data=merged_df, ax=ax)\n",
    "            ax.set_title(f\"{metric} ({complexity.upper()})\")\n",
    "            ax.set_xlabel(\"Multiplexing\")\n",
    "            ax.set_ylabel(metric)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "indel_performance_comparison_fig = create_indel_performance_comparison_plot(\n",
    "    indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_performance_comparison_plot(\n",
    "    snv_metrics_df, indel_metrics_df, multiplexing_df\n",
    "):\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(14, 16), dpi=300)\n",
    "    fig.suptitle(\"Performance Metrics vs Multiplexing\", fontsize=16, y=0.99)\n",
    "\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    data_types = [\"SNV\", \"Indel\"]\n",
    "\n",
    "    for data_type_idx, (data_type, metrics_df) in enumerate(\n",
    "        zip(data_types, [snv_metrics_df, indel_metrics_df])\n",
    "    ):\n",
    "        for complexity_idx, complexity in enumerate(complexities):\n",
    "            row = data_type_idx * 2 + complexity_idx\n",
    "            merged_df = pd.merge(\n",
    "                metrics_df[metrics_df[\"complexity\"] == complexity],\n",
    "                multiplexing_df[[\"sample\", \"multiplexing\"]],\n",
    "                left_on=\"sample_id\",\n",
    "                right_on=\"sample\",\n",
    "            )\n",
    "\n",
    "            y_min = min(merged_df[metric].min() for metric in metrics)\n",
    "            y_max = max(merged_df[metric].max() for metric in metrics)\n",
    "\n",
    "            y_range = y_max - y_min\n",
    "            y_min -= 0.4 * y_range\n",
    "            y_max += 0.2 * y_range\n",
    "\n",
    "            row_title = f\"{data_type} {complexity.upper()}\"\n",
    "            fig.text(\n",
    "                -0.01,\n",
    "                0.853 - row * 0.242,\n",
    "                row_title,\n",
    "                va=\"center\",\n",
    "                ha=\"left\",\n",
    "                fontsize=12,\n",
    "                rotation=90,\n",
    "            )\n",
    "\n",
    "            axes[row, 0].text(\n",
    "                -0.15,\n",
    "                1.1,\n",
    "                chr(65 + row),\n",
    "                transform=axes[row, 0].transAxes,\n",
    "                fontsize=16,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "            for col, metric in enumerate(metrics):\n",
    "                ax = axes[row, col]\n",
    "                sns.violinplot(x=\"multiplexing\", y=metric, data=merged_df, ax=ax)\n",
    "                ax.set_title(metric)\n",
    "                ax.set_xlabel(\"\")\n",
    "\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(\"Performance\")\n",
    "                else:\n",
    "                    ax.set_ylabel(\"\")\n",
    "\n",
    "                ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "combined_performance_comparison_fig = create_combined_performance_comparison_plot(\n",
    "    snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_variant_multiplexing_stats(snv_df, indel_df, multiplexing_df):\n",
    "    def calculate_stats(df, variant_type, complexity):\n",
    "        merged_df = pd.merge(\n",
    "            df[df[\"complexity\"] == complexity],\n",
    "            multiplexing_df[[\"sample\", \"multiplexing\"]],\n",
    "            left_on=\"sample_id\",\n",
    "            right_on=\"sample\",\n",
    "        )\n",
    "\n",
    "        stats = merged_df.groupby(\"multiplexing\", observed=True).agg(\n",
    "            {\n",
    "                \"Precision\": [\"mean\", \"std\", \"median\"],\n",
    "                \"Sensitivity\": [\"mean\", \"std\", \"median\"],\n",
    "                \"F-measure\": [\"mean\", \"std\", \"median\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        stats.columns = [f\"{col[0]}_{col[1]}\" for col in stats.columns]\n",
    "        stats[\"variant_type\"] = variant_type\n",
    "        stats[\"complexity\"] = complexity\n",
    "        return stats.reset_index()\n",
    "\n",
    "    stats_list = []\n",
    "    for variant_type, df in [(\"SNV\", snv_df), (\"Indel\", indel_df)]:\n",
    "        for complexity in [\"hc\", \"lc\"]:\n",
    "            stats_list.append(calculate_stats(df, variant_type, complexity))\n",
    "\n",
    "    combined_stats = pd.concat(stats_list, ignore_index=True)\n",
    "\n",
    "    column_order = [\"variant_type\", \"complexity\", \"multiplexing\"] + [\n",
    "        col\n",
    "        for col in combined_stats.columns\n",
    "        if col not in [\"variant_type\", \"complexity\", \"multiplexing\"]\n",
    "    ]\n",
    "    combined_stats = combined_stats[column_order]\n",
    "\n",
    "    return combined_stats\n",
    "\n",
    "\n",
    "def format_number(num):\n",
    "    return f\"{num:.4f}\"\n",
    "\n",
    "\n",
    "def calculate_percentage_increase(singleplex_val, multiplex_val):\n",
    "    return ((singleplex_val - multiplex_val) / multiplex_val) * 100\n",
    "\n",
    "\n",
    "def summarize_variant_stats(stats_df):\n",
    "    for variant_type in [\"SNV\", \"Indel\"]:\n",
    "        for complexity in [\"hc\", \"lc\"]:\n",
    "            print(f\"\\n{variant_type} Statistics ({complexity.upper()}):\")\n",
    "            print(\"=\" * 40)\n",
    "\n",
    "            variant_stats = stats_df[\n",
    "                (stats_df[\"variant_type\"] == variant_type)\n",
    "                & (stats_df[\"complexity\"] == complexity)\n",
    "            ]\n",
    "            singleplex_stats = variant_stats[\n",
    "                variant_stats[\"multiplexing\"] == \"singleplex\"\n",
    "            ].iloc[0]\n",
    "            multiplex_stats = variant_stats[\n",
    "                variant_stats[\"multiplexing\"] == \"multiplex\"\n",
    "            ].iloc[0]\n",
    "\n",
    "            metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "\n",
    "            for metric in metrics:\n",
    "                print(f\"\\n{metric}:\")\n",
    "                for stat in [\"mean\", \"std\", \"median\"]:\n",
    "                    singleplex_val = singleplex_stats[f\"{metric}_{stat}\"]\n",
    "                    multiplex_val = multiplex_stats[f\"{metric}_{stat}\"]\n",
    "                    print(\n",
    "                        f\"  {stat.capitalize():6s}: Singleplex: {format_number(singleplex_val)}, \"\n",
    "                        f\"Multiplex: {format_number(multiplex_val)}\"\n",
    "                    )\n",
    "\n",
    "                increase = calculate_percentage_increase(\n",
    "                    singleplex_stats[f\"{metric}_mean\"],\n",
    "                    multiplex_stats[f\"{metric}_mean\"],\n",
    "                )\n",
    "                print(\n",
    "                    f\"  Mean Percentage Increase (Singleplex vs Multiplex): {increase:6.2f}%\"\n",
    "                )\n",
    "\n",
    "\n",
    "variant_multiplexing_stats = create_variant_multiplexing_stats(\n",
    "    snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n",
    "\n",
    "summarize_variant_stats(variant_multiplexing_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Impact of sequencing depth on variant calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_depth_vs_performance(\n",
    "    wg_depth_df, snv_metrics_df, indel_metrics_df, np_metrics_df\n",
    "):\n",
    "    depth_data = wg_depth_df[wg_depth_df[\"chrom\"] == \"chr1\"][[\"sample\", \"mean\"]]\n",
    "    depth_data = depth_data.rename(columns={\"mean\": \"wg_mean_depth\"})\n",
    "\n",
    "    def asymptotic_func(x, a, b, c):\n",
    "        return a - b * np.exp(-c * x)\n",
    "\n",
    "    def plot_correlation(data, variant_type, complexity):\n",
    "        metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=300)\n",
    "        fig.suptitle(\n",
    "            f\"{variant_type} Performance Metrics vs Whole Genome Mean Depth ({complexity.upper()})\",\n",
    "            fontsize=16,\n",
    "        )\n",
    "\n",
    "        palette = sns.color_palette(\"colorblind\")\n",
    "        main_color = palette[0]\n",
    "\n",
    "        correlations = {}\n",
    "\n",
    "        for ax, metric in zip(axes, metrics):\n",
    "            sns.scatterplot(\n",
    "                data=data, x=\"wg_mean_depth\", y=metric, hue=\"multiplexing\", ax=ax, s=100\n",
    "            )\n",
    "\n",
    "            x = data[\"wg_mean_depth\"]\n",
    "            y = data[metric]\n",
    "\n",
    "            popt, pcov = curve_fit(\n",
    "                asymptotic_func, x, y, p0=[1, 0.1, 0.1], bounds=([0, 0, 0], [2, 1, 1])\n",
    "            )\n",
    "\n",
    "            x_range = np.linspace(x.min(), x.max(), 100)\n",
    "            y_fit = asymptotic_func(x_range, *popt)\n",
    "            ax.plot(\n",
    "                x_range,\n",
    "                y_fit,\n",
    "                color=main_color,\n",
    "                linestyle=\"-\",\n",
    "                linewidth=2,\n",
    "                label=\"Line of best fit\",\n",
    "            )\n",
    "\n",
    "            perr = np.sqrt(np.diag(pcov))\n",
    "            n = len(x)\n",
    "            dof = max(0, n - len(popt))\n",
    "            t = stats.t.ppf(0.975, dof)\n",
    "            y_err = np.sqrt(np.sum((y - asymptotic_func(x, *popt)) ** 2) / dof)\n",
    "\n",
    "            ci = (\n",
    "                t\n",
    "                * y_err\n",
    "                * np.sqrt(\n",
    "                    1 / n + (x_range - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2)\n",
    "                )\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                x_range,\n",
    "                y_fit - ci,\n",
    "                y_fit + ci,\n",
    "                color=main_color,\n",
    "                alpha=0.2,\n",
    "                label=\"95% Confidence Interval\",\n",
    "            )\n",
    "\n",
    "            r_value, p_value = stats.pearsonr(x, y)\n",
    "            correlations[metric] = (r_value, p_value)\n",
    "\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel(\"Whole Genome Mean Depth\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.legend(title=\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nPearson Correlations for {variant_type} ({complexity.upper()}):\")\n",
    "        for metric, (r_value, p_value) in correlations.items():\n",
    "            print(f\"{metric}:\")\n",
    "            print(f\"  Correlation coefficient: {r_value:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4e}\")\n",
    "            print()\n",
    "\n",
    "    for variant_type, metrics_df in [\n",
    "        (\"SNV\", snv_metrics_df),\n",
    "        (\"Indel\", indel_metrics_df),\n",
    "    ]:\n",
    "        for complexity in [\"hc\", \"lc\"]:\n",
    "            data = pd.merge(\n",
    "                metrics_df[metrics_df[\"complexity\"] == complexity],\n",
    "                np_metrics_df[[\"sample\", \"multiplexing\"]],\n",
    "                left_on=\"sample_id\",\n",
    "                right_on=\"sample\",\n",
    "            )\n",
    "            data = pd.merge(data, depth_data, on=\"sample\")\n",
    "            plot_correlation(data, variant_type, complexity)\n",
    "\n",
    "\n",
    "plot_depth_vs_performance(\n",
    "    wg_depth_df, snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_depth_vs_performance_plot(\n",
    "    wg_depth_df, snv_metrics_df, indel_metrics_df, np_metrics_df\n",
    "):\n",
    "    depth_data = wg_depth_df[wg_depth_df[\"chrom\"] == \"chr1\"][[\"sample\", \"mean\"]]\n",
    "    depth_data = depth_data.rename(columns={\"mean\": \"wg_mean_depth\"})\n",
    "\n",
    "    def asymptotic_func(x, a, b, c):\n",
    "        return a - b * np.exp(-c * x)\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(18, 20), dpi=300)\n",
    "    fig.suptitle(\"Performance Metrics vs Whole Genome Mean Depth\", fontsize=16, y=0.99)\n",
    "\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    data_types = [\"SNV\", \"Indel\"]\n",
    "\n",
    "    palette = sns.color_palette(\"colorblind\")\n",
    "    main_color = palette[0]\n",
    "\n",
    "    for data_type_idx, (data_type, metrics_df) in enumerate(\n",
    "        zip(data_types, [snv_metrics_df, indel_metrics_df])\n",
    "    ):\n",
    "        for complexity_idx, complexity in enumerate(complexities):\n",
    "            row = data_type_idx * 2 + complexity_idx\n",
    "\n",
    "            axes[row, 0].text(\n",
    "                -0.15,\n",
    "                1.1,\n",
    "                chr(65 + row),\n",
    "                transform=axes[row, 0].transAxes,\n",
    "                fontsize=16,\n",
    "                fontweight=\"bold\",\n",
    "                va=\"top\",\n",
    "            )\n",
    "\n",
    "            merged_df = pd.merge(\n",
    "                metrics_df[metrics_df[\"complexity\"] == complexity],\n",
    "                np_metrics_df[[\"sample\", \"multiplexing\"]],\n",
    "                left_on=\"sample_id\",\n",
    "                right_on=\"sample\",\n",
    "            )\n",
    "            merged_df = pd.merge(merged_df, depth_data, on=\"sample\")\n",
    "\n",
    "            y_min = min(merged_df[metric].min() for metric in metrics)\n",
    "            y_max = max(merged_df[metric].max() for metric in metrics)\n",
    "\n",
    "            y_range = y_max - y_min\n",
    "            y_min -= 0.4 * y_range\n",
    "            y_max += 0.2 * y_range\n",
    "\n",
    "            row_title = f\"{data_type} {complexity.upper()}\"\n",
    "            fig.text(\n",
    "                -0.01,\n",
    "                0.855 - row * 0.244,\n",
    "                row_title,\n",
    "                va=\"center\",\n",
    "                ha=\"left\",\n",
    "                fontsize=12,\n",
    "                rotation=90,\n",
    "            )\n",
    "\n",
    "            for col, metric in enumerate(metrics):\n",
    "                ax = axes[row, col]\n",
    "                sns.scatterplot(\n",
    "                    data=merged_df,\n",
    "                    x=\"wg_mean_depth\",\n",
    "                    y=metric,\n",
    "                    hue=\"multiplexing\",\n",
    "                    ax=ax,\n",
    "                    legend=row == 0 and col == 0,\n",
    "                    s=100,\n",
    "                )\n",
    "\n",
    "                x = merged_df[\"wg_mean_depth\"]\n",
    "                y = merged_df[metric]\n",
    "\n",
    "                popt, pcov = curve_fit(\n",
    "                    asymptotic_func,\n",
    "                    x,\n",
    "                    y,\n",
    "                    p0=[1, 0.1, 0.1],\n",
    "                    bounds=([0, 0, 0], [2, 1, 1]),\n",
    "                )\n",
    "\n",
    "                x_range = np.linspace(x.min(), x.max(), 100)\n",
    "                y_fit = asymptotic_func(x_range, *popt)\n",
    "                ax.plot(\n",
    "                    x_range,\n",
    "                    y_fit,\n",
    "                    color=main_color,\n",
    "                    linestyle=\"-\",\n",
    "                    linewidth=2,\n",
    "                    label=\"Line of best fit\",\n",
    "                )\n",
    "\n",
    "                perr = np.sqrt(np.diag(pcov))\n",
    "                n = len(x)\n",
    "                dof = max(0, n - len(popt))\n",
    "                t = stats.t.ppf(0.975, dof)\n",
    "                y_err = np.sqrt(np.sum((y - asymptotic_func(x, *popt)) ** 2) / dof)\n",
    "\n",
    "                ci = (\n",
    "                    t\n",
    "                    * y_err\n",
    "                    * np.sqrt(\n",
    "                        1 / n\n",
    "                        + (x_range - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2)\n",
    "                    )\n",
    "                )\n",
    "                ax.fill_between(\n",
    "                    x_range,\n",
    "                    y_fit - ci,\n",
    "                    y_fit + ci,\n",
    "                    color=main_color,\n",
    "                    alpha=0.2,\n",
    "                    label=\"95% Confidence Interval\",\n",
    "                )\n",
    "\n",
    "                r_value, p_value = stats.pearsonr(x, y)\n",
    "\n",
    "                ax.set_title(f\"{metric}\\nr={r_value:.2f}, p={p_value:.2e}\")\n",
    "                ax.set_xlabel(\"\")\n",
    "\n",
    "                if col == 0:\n",
    "                    ax.set_ylabel(\"Performance\")\n",
    "                else:\n",
    "                    ax.set_ylabel(\"\")\n",
    "                    ax.set_yticklabels([])\n",
    "\n",
    "                ax.set_ylim(y_min, y_max)\n",
    "\n",
    "                if row == 0 and col == 0:\n",
    "                    handles, labels = ax.get_legend_handles_labels()\n",
    "                    ax.legend(\n",
    "                        handles=handles[:2] + handles[-2:],\n",
    "                        labels=labels[:2] + labels[-2:],\n",
    "                        bbox_to_anchor=(1, 0.3),\n",
    "                    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "combined_depth_performance_fig = create_combined_depth_vs_performance_plot(\n",
    "    wg_depth_df, snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Impact of read length on variant calling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_read_length_vs_performance(\n",
    "    read_length_df, snv_metrics_df, indel_metrics_df, np_metrics_df\n",
    "):\n",
    "    mean_read_length_data = read_length_df[\n",
    "        read_length_df[\"read_length_type\"] == \"Mean Read Length\"\n",
    "    ]\n",
    "    mean_read_length_data = mean_read_length_data[[\"anonymised_sample\", \"read_length\"]]\n",
    "    mean_read_length_data = mean_read_length_data.rename(\n",
    "        columns={\"read_length\": \"mean_read_length\"}\n",
    "    )\n",
    "\n",
    "    def plot_correlation(data, variant_type, complexity):\n",
    "        metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6), dpi=300)\n",
    "        fig.suptitle(\n",
    "            f\"{variant_type} Performance Metrics vs Mean Read Length ({complexity.upper()})\",\n",
    "            fontsize=16,\n",
    "        )\n",
    "\n",
    "        palette = sns.color_palette(\"colorblind\")\n",
    "        main_color = palette[0]\n",
    "        correlations = {}\n",
    "\n",
    "        for ax, metric in zip(axes, metrics):\n",
    "            sns.scatterplot(\n",
    "                data=data,\n",
    "                x=\"mean_read_length\",\n",
    "                y=metric,\n",
    "                hue=\"multiplexing\",\n",
    "                ax=ax,\n",
    "                s=100,\n",
    "            )\n",
    "\n",
    "            x = data[\"mean_read_length\"]\n",
    "            y = data[metric]\n",
    "\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            x_range = np.linspace(x.min(), x.max(), 100)\n",
    "            y_fit = slope * x_range + intercept\n",
    "\n",
    "            ax.plot(\n",
    "                x_range,\n",
    "                y_fit,\n",
    "                color=main_color,\n",
    "                linestyle=\"-\",\n",
    "                linewidth=2,\n",
    "                label=\"Line of best fit\",\n",
    "            )\n",
    "\n",
    "            n = len(x)\n",
    "            y_err = np.sqrt(np.sum((y - (slope * x + intercept)) ** 2) / (n - 2))\n",
    "            ci = (\n",
    "                stats.t.ppf(0.975, n - 2)\n",
    "                * y_err\n",
    "                * np.sqrt(\n",
    "                    1 / n + (x_range - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2)\n",
    "                )\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                x_range,\n",
    "                y_fit - ci,\n",
    "                y_fit + ci,\n",
    "                color=main_color,\n",
    "                alpha=0.2,\n",
    "                label=\"95% Confidence Interval\",\n",
    "            )\n",
    "\n",
    "            r_value, p_value = stats.pearsonr(x, y)\n",
    "            correlations[metric] = (r_value, p_value)\n",
    "\n",
    "            ax.set_title(metric)\n",
    "            ax.set_xlabel(\"Mean Read Length\")\n",
    "            ax.set_ylabel(metric)\n",
    "            ax.legend(title=\"\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"\\nPearson Correlations for {variant_type} ({complexity.upper()}):\")\n",
    "        for metric, (r_value, p_value) in correlations.items():\n",
    "            print(f\"{metric}:\")\n",
    "            print(f\"  Correlation coefficient: {r_value:.4f}\")\n",
    "            print(f\"  p-value: {p_value:.4e}\")\n",
    "            print()\n",
    "\n",
    "    for variant_type, metrics_df in [\n",
    "        (\"SNV\", snv_metrics_df),\n",
    "        (\"Indel\", indel_metrics_df),\n",
    "    ]:\n",
    "        for complexity in [\"hc\", \"lc\"]:\n",
    "            metrics_df[\"sample_id\"] = metrics_df[\"sample_id\"]\n",
    "\n",
    "            filtered_metrics = metrics_df[metrics_df[\"complexity\"] == complexity]\n",
    "\n",
    "            data = pd.merge(\n",
    "                filtered_metrics,\n",
    "                np_metrics_df[[\"sample\", \"anonymised_sample\", \"multiplexing\"]],\n",
    "                left_on=\"sample_id\",\n",
    "                right_on=\"sample\",\n",
    "            )\n",
    "\n",
    "            data = pd.merge(data, mean_read_length_data, on=\"anonymised_sample\")\n",
    "\n",
    "            if data.empty:\n",
    "                print(f\"No data after merging for {variant_type} ({complexity})\")\n",
    "                continue\n",
    "\n",
    "            plot_correlation(data, variant_type, complexity)\n",
    "\n",
    "\n",
    "plot_read_length_vs_performance(\n",
    "    read_length_df, snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_read_length_vs_performance(\n",
    "    read_length_df, snv_metrics_df, indel_metrics_df, np_metrics_df\n",
    "):\n",
    "    mean_read_length_data = read_length_df[\n",
    "        read_length_df[\"read_length_type\"] == \"Mean Read Length\"\n",
    "    ]\n",
    "    mean_read_length_data = mean_read_length_data[[\"sample\", \"read_length\"]]\n",
    "    mean_read_length_data = mean_read_length_data.rename(\n",
    "        columns={\"read_length\": \"mean_read_length\"}\n",
    "    )\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, figsize=(14, 16), dpi=300)\n",
    "    fig.suptitle(\"Performance Metrics vs Mean Read Length\", fontsize=16, y=0.99)\n",
    "\n",
    "    metrics = [\"Precision\", \"Sensitivity\", \"F-measure\"]\n",
    "    complexities = [\"hc\", \"lc\"]\n",
    "    data_types = [\"SNV\", \"Indel\"]\n",
    "\n",
    "    def plot_correlation(data, variant_type, complexity, row):\n",
    "        y_min = min(data[metric].min() for metric in metrics)\n",
    "        y_max = max(data[metric].max() for metric in metrics)\n",
    "\n",
    "        y_range = y_max - y_min\n",
    "        y_min -= 0.4 * y_range\n",
    "        y_max += 0.2 * y_range\n",
    "\n",
    "        row_title = f\"{variant_type} {complexity.upper()}\"\n",
    "        fig.text(\n",
    "            -0.01,\n",
    "            0.85 - row * 0.242,\n",
    "            row_title,\n",
    "            va=\"center\",\n",
    "            ha=\"left\",\n",
    "            fontsize=12,\n",
    "            rotation=90,\n",
    "        )\n",
    "\n",
    "        # Add letter label to each row\n",
    "        axes[row, 0].text(\n",
    "            -0.15,\n",
    "            1.1,\n",
    "            chr(65 + row),\n",
    "            transform=axes[row, 0].transAxes,\n",
    "            fontsize=16,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "        palette = sns.color_palette(\"colorblind\")\n",
    "        main_color = palette[0]\n",
    "\n",
    "        for col, metric in enumerate(metrics):\n",
    "            ax = axes[row, col]\n",
    "            sns.scatterplot(\n",
    "                data=data,\n",
    "                x=\"mean_read_length\",\n",
    "                y=metric,\n",
    "                hue=\"multiplexing\",\n",
    "                ax=ax,\n",
    "                legend=row == 0 and col == 0,\n",
    "            )\n",
    "\n",
    "            x = data[\"mean_read_length\"]\n",
    "            y = data[metric]\n",
    "\n",
    "            slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "            x_range = np.linspace(x.min(), x.max(), 100)\n",
    "            y_fit = slope * x_range + intercept\n",
    "\n",
    "            ax.plot(\n",
    "                x_range,\n",
    "                y_fit,\n",
    "                color=main_color,\n",
    "                linestyle=\"-\",\n",
    "                linewidth=2,\n",
    "                label=\"Line of best fit\" if row == 0 and col == 0 else None,\n",
    "            )\n",
    "\n",
    "            n = len(x)\n",
    "            y_err = np.sqrt(np.sum((y - (slope * x + intercept)) ** 2) / (n - 2))\n",
    "            ci = (\n",
    "                stats.t.ppf(0.975, n - 2)\n",
    "                * y_err\n",
    "                * np.sqrt(\n",
    "                    1 / n + (x_range - np.mean(x)) ** 2 / np.sum((x - np.mean(x)) ** 2)\n",
    "                )\n",
    "            )\n",
    "            ax.fill_between(\n",
    "                x_range,\n",
    "                y_fit - ci,\n",
    "                y_fit + ci,\n",
    "                color=main_color,\n",
    "                alpha=0.2,\n",
    "                label=\"95% Confidence Interval\" if row == 0 and col == 0 else None,\n",
    "            )\n",
    "\n",
    "            r_value, p_value = stats.pearsonr(x, y)\n",
    "\n",
    "            ax.set_title(f\"{metric}\\nr={r_value:.2f}, p={p_value:.2e}\")\n",
    "            ax.set_xlabel(\"\")\n",
    "\n",
    "            if col == 0:\n",
    "                ax.set_ylabel(\"Performance\")\n",
    "            else:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.set_yticklabels([])\n",
    "\n",
    "            ax.set_ylim(y_min, y_max)\n",
    "\n",
    "            if row == 0 and col == 0:\n",
    "                handles, labels = ax.get_legend_handles_labels()\n",
    "                ax.legend(handles=handles, labels=labels, title=\"\")\n",
    "\n",
    "    for variant_type_idx, (variant_type, metrics_df) in enumerate(\n",
    "        zip(data_types, [snv_metrics_df, indel_metrics_df])\n",
    "    ):\n",
    "        for complexity_idx, complexity in enumerate(complexities):\n",
    "            row = variant_type_idx * 2 + complexity_idx\n",
    "            data = pd.merge(\n",
    "                metrics_df[metrics_df[\"complexity\"] == complexity],\n",
    "                np_metrics_df[[\"sample\", \"multiplexing\"]],\n",
    "                left_on=\"sample_id\",\n",
    "                right_on=\"sample\",\n",
    "            )\n",
    "            data = pd.merge(data, mean_read_length_data, on=\"sample\")\n",
    "            plot_correlation(data, variant_type, complexity, row)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return fig\n",
    "\n",
    "\n",
    "combined_read_length_performance_fig = plot_combined_read_length_vs_performance(\n",
    "    read_length_df, snv_ont_metrics_df, indel_ont_metrics_df, np_metrics_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV Benchmark\n",
    "\n",
    "## 1. SV Consensus Calls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sv_vcf_file(file_path):\n",
    "    svs = []\n",
    "    with pysam.VariantFile(file_path) as vcf:\n",
    "        for record in vcf:\n",
    "            for alt_idx, alt in enumerate(record.alts):\n",
    "                sv_info = extract_sv_info(record, alt, alt_idx)\n",
    "                if sv_info:\n",
    "                    sv_info[\"allele_idx\"] = alt_idx\n",
    "                    svs.append(sv_info)\n",
    "    return pd.DataFrame(svs)\n",
    "\n",
    "\n",
    "def extract_sv_info(record, alt, alt_idx):\n",
    "    chrom = record.chrom\n",
    "    start = record.pos\n",
    "    end = record.info.get(\"END\", start)\n",
    "    sv_type = record.info.get(\"SVTYPE\", \"Unknown\")\n",
    "\n",
    "    if sv_type == \"STR\" or (isinstance(alt, str) and alt.startswith(\"<STR\")):\n",
    "        return handle_str(record, alt, chrom, start, end, alt_idx)\n",
    "    elif isinstance(alt, str) and alt.startswith(\"<\") and alt.endswith(\">\"):\n",
    "        return handle_symbolic_allele(record, alt, chrom, start, end, sv_type, alt_idx)\n",
    "    else:\n",
    "        return handle_standard_sv(record, alt, chrom, start, end, sv_type, alt_idx)\n",
    "\n",
    "\n",
    "def handle_str(record, alt, chrom, start, end, alt_idx):\n",
    "    def parse_int_or_first(value):\n",
    "        if isinstance(value, (int, float)):\n",
    "            return int(value)\n",
    "        elif isinstance(value, str):\n",
    "            return int(value.split(\"/\")[0])\n",
    "        elif isinstance(value, tuple):\n",
    "            return int(value[0])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected value type: {type(value)}\")\n",
    "\n",
    "    repcn = record.samples[0].get(\"REPCN\")\n",
    "\n",
    "    if repcn is not None:\n",
    "        if isinstance(repcn, tuple):\n",
    "            repeat_count = parse_int_or_first(repcn[alt_idx])\n",
    "        else:\n",
    "            repeat_count = parse_int_or_first(repcn)\n",
    "    elif alt.startswith(\"<STR\"):\n",
    "        str_alleles = record.alts\n",
    "        current_alt = str_alleles[alt_idx]\n",
    "        repeat_count = int(current_alt[4:-1])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    ru = record.info.get(\"RU\", \"\")\n",
    "\n",
    "    sv_len = repeat_count * len(ru)\n",
    "\n",
    "    return {\n",
    "        \"type\": \"STR\",\n",
    "        \"length\": sv_len,\n",
    "        \"chrom\": chrom,\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_symbolic_allele(record, alt, chrom, start, end, sv_type, alt_idx):\n",
    "    sv_len = None\n",
    "\n",
    "    if sv_type == \"INV\" and \"SVINSLEN\" in record.info:\n",
    "        sv_len = record.info.get(\"SVINSLEN\")\n",
    "        if isinstance(sv_len, tuple):\n",
    "            sv_len = sv_len[alt_idx] if len(sv_len) > alt_idx else sv_len[0]\n",
    "\n",
    "    if sv_len is None:\n",
    "        sv_len = record.info.get(\"SVLEN\")\n",
    "        if isinstance(sv_len, tuple):\n",
    "            sv_len = sv_len[alt_idx] if len(sv_len) > alt_idx else sv_len[0]\n",
    "\n",
    "    if sv_len is None and sv_type == \"INS\":\n",
    "        left_seq = record.info.get(\"LEFT_SVINSSEQ\", \"\")\n",
    "        right_seq = record.info.get(\"RIGHT_SVINSSEQ\", \"\")\n",
    "        sv_len = len(left_seq) + len(right_seq)\n",
    "\n",
    "    if sv_len is None:\n",
    "        sv_len = end - start\n",
    "\n",
    "    return {\n",
    "        \"type\": sv_type,\n",
    "        \"length\": abs(sv_len) if sv_len is not None else None,\n",
    "        \"chrom\": chrom,\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "    }\n",
    "\n",
    "\n",
    "def handle_standard_sv(record, alt, chrom, start, end, sv_type, alt_idx):\n",
    "    if \"SVLEN\" in record.info:\n",
    "        sv_len = record.info[\"SVLEN\"]\n",
    "        if isinstance(sv_len, tuple):\n",
    "            sv_len = sv_len[alt_idx] if len(sv_len) > alt_idx else sv_len[0]\n",
    "    elif sv_type == \"INS\":\n",
    "        left_seq = record.info.get(\"LEFT_SVINSSEQ\", \"\")\n",
    "        right_seq = record.info.get(\"RIGHT_SVINSSEQ\", \"\")\n",
    "        sv_len = len(left_seq) + len(right_seq)\n",
    "    else:\n",
    "        ref_len = len(record.ref)\n",
    "        alt_len = len(alt)\n",
    "        sv_len = alt_len - ref_len if sv_type == \"INS\" else ref_len - alt_len\n",
    "\n",
    "    return {\n",
    "        \"type\": sv_type,\n",
    "        \"length\": abs(sv_len),\n",
    "        \"chrom\": chrom,\n",
    "        \"start\": start,\n",
    "        \"end\": end,\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_sv_calls(sample_id, ont_id, illumina_id):\n",
    "    ont_file = f\"output/sv/survivor/{ont_id}/{ont_id}.ont.sv_str.filtered.vcf\"\n",
    "    illumina_file = (\n",
    "        f\"output/sv/survivor/{ont_id}/{illumina_id}.illumina.sv.filtered.vcf\"\n",
    "    )\n",
    "    merged_file = f\"output/sv/survivor/{ont_id}/{ont_id}_{illumina_id}_merged.vcf\"\n",
    "\n",
    "    ont_svs = read_sv_vcf_file(ont_file)\n",
    "    illumina_svs = read_sv_vcf_file(illumina_file)\n",
    "    merged_svs = read_sv_vcf_file(merged_file)\n",
    "\n",
    "    ont_svs[\"sv_id\"] = ont_svs.apply(\n",
    "        lambda row: f\"{row['chrom']}_{row['start']}_{row['end']}_{row['type']}\", axis=1\n",
    "    )\n",
    "    illumina_svs[\"sv_id\"] = illumina_svs.apply(\n",
    "        lambda row: f\"{row['chrom']}_{row['start']}_{row['end']}_{row['type']}\", axis=1\n",
    "    )\n",
    "    merged_svs[\"sv_id\"] = merged_svs.apply(\n",
    "        lambda row: f\"{row['chrom']}_{row['start']}_{row['end']}_{row['type']}\", axis=1\n",
    "    )\n",
    "\n",
    "    all_svs = pd.concat([ont_svs, illumina_svs]).drop_duplicates(subset=\"sv_id\")\n",
    "\n",
    "    all_svs[\"ONT\"] = all_svs[\"sv_id\"].isin(ont_svs[\"sv_id\"])\n",
    "    all_svs[\"Illumina\"] = all_svs[\"sv_id\"].isin(illumina_svs[\"sv_id\"])\n",
    "    all_svs[\"Merged\"] = all_svs[\"sv_id\"].isin(merged_svs[\"sv_id\"])\n",
    "\n",
    "    all_svs[\"sample_id\"] = sample_id\n",
    "\n",
    "    all_svs = all_svs.drop(\"sv_id\", axis=1)\n",
    "\n",
    "    return all_svs\n",
    "\n",
    "\n",
    "sv_data_list = []\n",
    "\n",
    "for _, row in sample_ids.iterrows():\n",
    "    sample_data = analyze_sv_calls(row[\"ont_id\"], row[\"ont_id\"], row[\"lp_id\"])\n",
    "    sv_data_list.append(sample_data)\n",
    "\n",
    "sv_data_df = pd.concat(sv_data_list, ignore_index=True)\n",
    "\n",
    "sv_data_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sv_counts(sv_data_df):\n",
    "    counts = []\n",
    "    for sample_id in sv_data_df[\"sample_id\"].unique():\n",
    "        sample_data = sv_data_df[sv_data_df[\"sample_id\"] == sample_id]\n",
    "        counts.append(\n",
    "            {\n",
    "                \"Sample\": sample_id,\n",
    "                \"ONT\": sample_data[\"ONT\"].sum(),\n",
    "                \"Illumina\": sample_data[\"Illumina\"].sum(),\n",
    "                \"Consensus\": sample_data[\"Merged\"].sum(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    counts_df = pd.DataFrame(counts)\n",
    "\n",
    "    counts_df = counts_df.sort_values(\"Sample\")\n",
    "\n",
    "    counts_df[\"anonymised_sample\"] = [f\"Sample {i+1}\" for i in range(len(counts_df))]\n",
    "\n",
    "    return counts_df[[\"anonymised_sample\", \"ONT\", \"Illumina\", \"Consensus\"]]\n",
    "\n",
    "\n",
    "def plot_sv_counts(sv_counts_df, figsize=(12, 6), dpi=300):\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    sv_counts_melted = sv_counts_df.melt(\n",
    "        id_vars=[\"anonymised_sample\"], var_name=\"Platform\", value_name=\"Count\"\n",
    "    )\n",
    "\n",
    "    sns.barplot(\n",
    "        x=\"anonymised_sample\", y=\"Count\", hue=\"Platform\", data=sv_counts_melted, ax=ax\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"SV Call Counts by Sample and Platform\")\n",
    "    ax.set_xlabel(\"Sample\")\n",
    "    ax.set_ylabel(\"Number of SV Calls\")\n",
    "\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_ha(\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sv_counts_df = compare_sv_counts(sv_data_df)\n",
    "print(\"SV call counts:\")\n",
    "print(sv_counts_df)\n",
    "\n",
    "plot_sv_counts(sv_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_consensus_percentages(sv_counts_df):\n",
    "    sv_counts_df[\"ONT_Consensus_Percent\"] = (\n",
    "        sv_counts_df[\"Consensus\"] / sv_counts_df[\"ONT\"] * 100\n",
    "    ).fillna(0)\n",
    "    sv_counts_df[\"Illumina_Consensus_Percent\"] = (\n",
    "        sv_counts_df[\"Consensus\"] / sv_counts_df[\"Illumina\"] * 100\n",
    "    ).fillna(0)\n",
    "\n",
    "    avg_ont_percent = np.mean(sv_counts_df[\"ONT_Consensus_Percent\"])\n",
    "    sd_ont_percent = np.std(sv_counts_df[\"ONT_Consensus_Percent\"])\n",
    "    avg_illumina_percent = np.mean(sv_counts_df[\"Illumina_Consensus_Percent\"])\n",
    "    sd_illumina_percent = np.std(sv_counts_df[\"Illumina_Consensus_Percent\"])\n",
    "\n",
    "    print(\n",
    "        f\"Average consensus percentage for ONT: {avg_ont_percent:.2f}%  {sd_ont_percent:.2f}% (mean  SD)\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Average consensus percentage for Illumina: {avg_illumina_percent:.2f}%  {sd_illumina_percent:.2f}% (mean  SD)\"\n",
    "    )\n",
    "\n",
    "    return sv_counts_df\n",
    "\n",
    "\n",
    "calculate_consensus_percentages(sv_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_difference(sv_counts_df):\n",
    "    sv_counts_df[\"ONT_Illumina_Ratio\"] = (\n",
    "        sv_counts_df[\"ONT\"] / sv_counts_df[\"Illumina\"]\n",
    "    ).fillna(0)\n",
    "    average_difference = sv_counts_df[\"ONT_Illumina_Ratio\"].mean()\n",
    "    sd_difference = sv_counts_df[\"ONT_Illumina_Ratio\"].std()\n",
    "\n",
    "    print(\n",
    "        f\"Average ratio of SV counts between ONT and Illumina: {average_difference:.2f}  {sd_difference:.2f} (mean  SD)\"\n",
    "    )\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "calculate_average_difference(sv_counts_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SV Size Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sv_length_distributions(sv_data_df, figsize=(12, 6), dpi=300):\n",
    "    ont_lengths = (\n",
    "        sv_data_df[sv_data_df[\"ONT\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "    illumina_lengths = (\n",
    "        sv_data_df[sv_data_df[\"Illumina\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    sns.histplot(\n",
    "        ont_lengths,\n",
    "        log_scale=True,\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        alpha=0.5,\n",
    "        label=\"long-read\",\n",
    "    )\n",
    "    sns.histplot(\n",
    "        illumina_lengths,\n",
    "        log_scale=True,\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        alpha=0.5,\n",
    "        label=\"short-read\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Aggregated ONT and Illumina SV Size Distribution\")\n",
    "    plt.xlabel(\"SV Size (log scale)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_sv_length_distributions(sv_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sv_length_stats(lengths):\n",
    "    return {\n",
    "        \"max\": np.max(lengths),\n",
    "        \"min\": np.min(lengths),\n",
    "        \"mean\": np.mean(lengths),\n",
    "        \"std\": np.std(lengths),\n",
    "        \"median\": np.median(lengths),\n",
    "    }\n",
    "\n",
    "\n",
    "def format_number(num):\n",
    "    if isinstance(num, (int, np.integer)):\n",
    "        return f\"{num:,d}\"\n",
    "    elif isinstance(num, float):\n",
    "        return f\"{num:,.2f}\"\n",
    "    return str(num)\n",
    "\n",
    "\n",
    "def print_sv_length_stats(stats, sample_type):\n",
    "    print(f\"\\n{sample_type} SV Length Statistics:\")\n",
    "    print(\"=\" * 40)\n",
    "    for stat, value in stats.items():\n",
    "        formatted_value = format_number(value)\n",
    "        print(f\"  {stat.capitalize():6s}: {formatted_value}\")\n",
    "\n",
    "\n",
    "def analyze_sv_length_distributions(sv_data_df):\n",
    "    ont_lengths = (\n",
    "        sv_data_df[sv_data_df[\"ONT\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "    illumina_lengths = (\n",
    "        sv_data_df[sv_data_df[\"Illumina\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    ont_stats = calculate_sv_length_stats(ont_lengths)\n",
    "    illumina_stats = calculate_sv_length_stats(illumina_lengths)\n",
    "\n",
    "    print_sv_length_stats(ont_stats, \"ONT\")\n",
    "    print_sv_length_stats(illumina_stats, \"Illumina\")\n",
    "\n",
    "\n",
    "analyze_sv_length_distributions(sv_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SV Types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sv_types(sv_data_df):\n",
    "    type_counts = defaultdict(lambda: defaultdict(int))\n",
    "    type_stats = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "    for sample_id in sv_data_df[\"sample_id\"].unique():\n",
    "        sample_data = sv_data_df[sv_data_df[\"sample_id\"] == sample_id]\n",
    "\n",
    "        for platform in [\"ONT\", \"Illumina\"]:\n",
    "            platform_data = sample_data[sample_data[platform]]\n",
    "            type_counts_series = platform_data[\"type\"].value_counts()\n",
    "\n",
    "            for sv_type, count in type_counts_series.items():\n",
    "                type_counts[(sample_id, platform.lower())][sv_type] = count\n",
    "                type_stats[platform.lower()][sv_type].append(count)\n",
    "\n",
    "    df_sv_type_counts = pd.DataFrame(type_counts).T\n",
    "\n",
    "    sv_type_stats = {}\n",
    "    for platform, sv_types in type_stats.items():\n",
    "        for sv_type, counts in sv_types.items():\n",
    "            sv_type_stats[(platform, sv_type)] = {\n",
    "                \"mean\": np.mean(counts),\n",
    "                \"median\": np.median(counts),\n",
    "                \"std_dev\": np.std(counts),\n",
    "            }\n",
    "\n",
    "    df_sv_type_stats = pd.DataFrame(sv_type_stats).T\n",
    "    df_sv_type_stats.index.names = [\"Platform\", \"SV Type\"]\n",
    "\n",
    "    return df_sv_type_counts, df_sv_type_stats\n",
    "\n",
    "\n",
    "sv_type_counts_df, sv_type_stats_df = analyze_sv_types(sv_data_df)\n",
    "\n",
    "print(\"\\nSV type counts:\")\n",
    "print(sv_type_counts_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSV type statistics:\")\n",
    "print(sv_type_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sv_types(sv_data_df, figsize=(15, 10), dpi=300):\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    platform_names = {\"ONT\": \"Long-read\", \"Illumina\": \"Short-read\"}\n",
    "\n",
    "    unique_samples = sorted(sv_data_df[\"sample_id\"].unique())\n",
    "    sample_map = {sample: f\"Sample {i+1}\" for i, sample in enumerate(unique_samples)}\n",
    "    sv_data_df[\"anonymised_sample\"] = sv_data_df[\"sample_id\"].map(sample_map)\n",
    "\n",
    "    for i, platform in enumerate([\"ONT\", \"Illumina\"]):\n",
    "        ax = plt.subplot(2, 1, i + 1)\n",
    "\n",
    "        platform_data = sv_data_df[sv_data_df[platform]]\n",
    "\n",
    "        sv_type_counts = platform_data.pivot_table(\n",
    "            index=\"anonymised_sample\", columns=\"type\", values=platform, aggfunc=\"sum\"\n",
    "        ).fillna(0)\n",
    "\n",
    "        sv_type_counts = sv_type_counts.reindex(\n",
    "            [f\"Sample {i+1}\" for i in range(len(unique_samples))]\n",
    "        )\n",
    "\n",
    "        sv_type_counts.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "\n",
    "        plt.title(f\"SV Types by Sample - {platform_names[platform]}\")\n",
    "        plt.xlabel(\"Sample\")\n",
    "        plt.ylabel(\"Number of SVs\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "        if i == 0:\n",
    "            handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend().remove()\n",
    "\n",
    "    plt.legend(\n",
    "        handles, labels, title=\"SV Type\", bbox_to_anchor=(1.05, 1), loc=\"upper left\"\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_sv_types(sv_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sv_size_distribution_by_type(sv_data_df, figsize=(20, 15), dpi=300):\n",
    "    sv_types = [\"INS\", \"DEL\", \"DUP\", \"INV\", \"BND\", \"STR\"]\n",
    "    sv_full_names = {\n",
    "        \"INS\": \"Insertion\",\n",
    "        \"DEL\": \"Deletion\",\n",
    "        \"DUP\": \"Duplication\",\n",
    "        \"INV\": \"Inversion\",\n",
    "        \"BND\": \"Breakend\",\n",
    "        \"STR\": \"Short Tandem Repeat\",\n",
    "    }\n",
    "\n",
    "    fig, axes = plt.subplots(3, 2, figsize=figsize, dpi=dpi)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, sv_type in enumerate(sv_types):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        for platform, alpha, label in [\n",
    "            (\"ONT\", 0.7, \"long-read\"),\n",
    "            (\"Illumina\", 0.7, \"short-read\"),\n",
    "        ]:\n",
    "            sv_data = sv_data_df[\n",
    "                (sv_data_df[\"type\"] == sv_type) & (sv_data_df[platform])\n",
    "            ]\n",
    "\n",
    "            if not sv_data.empty:\n",
    "                sns.histplot(\n",
    "                    data=sv_data,\n",
    "                    x=\"length\",\n",
    "                    kde=True,\n",
    "                    log_scale=(sv_type not in [\"BND\", \"STR\"]),\n",
    "                    stat=\"density\",\n",
    "                    ax=ax,\n",
    "                    alpha=alpha,\n",
    "                    label=label,\n",
    "                )\n",
    "\n",
    "        ax.set_title(f\"{sv_full_names[sv_type]} Size Distribution\")\n",
    "        ax.set_xlabel(\n",
    "            \"SV Size\" + (\" (log scale)\" if sv_type not in [\"BND\", \"STR\"] else \"\")\n",
    "        )\n",
    "        ax.set_ylabel(\"Density\")\n",
    "\n",
    "        if idx == 0:\n",
    "            ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_sv_size_distribution_by_type(sv_data_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SV Chromosomal Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_lengths = {\n",
    "    \"chr1\": 248956422,\n",
    "    \"chr2\": 242193529,\n",
    "    \"chr3\": 198295559,\n",
    "    \"chr4\": 190214555,\n",
    "    \"chr5\": 181538259,\n",
    "    \"chr6\": 170805979,\n",
    "    \"chr7\": 159345973,\n",
    "    \"chr8\": 145138636,\n",
    "    \"chr9\": 138394717,\n",
    "    \"chr10\": 133797422,\n",
    "    \"chr11\": 135086622,\n",
    "    \"chr12\": 133275309,\n",
    "    \"chr13\": 114364328,\n",
    "    \"chr14\": 107043718,\n",
    "    \"chr15\": 101991189,\n",
    "    \"chr16\": 90338345,\n",
    "    \"chr17\": 83257441,\n",
    "    \"chr18\": 80373285,\n",
    "    \"chr19\": 58617616,\n",
    "    \"chr20\": 64444167,\n",
    "    \"chr21\": 46709983,\n",
    "    \"chr22\": 50818468,\n",
    "    \"chrX\": 156040895,\n",
    "    \"chrY\": 57227415,\n",
    "}\n",
    "\n",
    "\n",
    "def analyze_chrom_distribution(sv_data_df):\n",
    "    valid_chroms = [f\"chr{i}\" for i in range(1, 23)] + [\"chrX\", \"chrY\"]\n",
    "    chrom_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for platform in [\"ONT\", \"Illumina\"]:\n",
    "        platform_data = sv_data_df[sv_data_df[platform]]\n",
    "        for chrom in valid_chroms:\n",
    "            chrom_counts[platform.lower()][chrom] = platform_data[\n",
    "                platform_data[\"chrom\"] == chrom\n",
    "            ].shape[0]\n",
    "\n",
    "    return pd.DataFrame(chrom_counts)\n",
    "\n",
    "\n",
    "def normalize_by_chrom_length(chrom_distribution_df):\n",
    "    normalized_df = chrom_distribution_df.copy().astype(float)\n",
    "    for chrom in normalized_df.index:\n",
    "        normalized_df.loc[chrom] = normalized_df.loc[chrom] / (\n",
    "            chrom_lengths[chrom] / 1e6\n",
    "        )\n",
    "    return normalized_df\n",
    "\n",
    "\n",
    "def plot_chrom_distribution(chrom_distribution_df, figsize=(12, 6), dpi=300):\n",
    "    normalized_df = normalize_by_chrom_length(chrom_distribution_df)\n",
    "    normalized_df = normalized_df.rename(\n",
    "        columns={\"ont\": \"long-read\", \"illumina\": \"short-read\"}\n",
    "    )\n",
    "\n",
    "    chrom_distribution_pct = normalized_df.apply(lambda x: x / x.sum() * 100)\n",
    "\n",
    "    chrom_distribution_melted = chrom_distribution_pct.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Platform\", value_name=\"Normalized Percentage\"\n",
    "    )\n",
    "    chrom_distribution_melted = chrom_distribution_melted.rename(\n",
    "        columns={\"index\": \"Chromosome\"}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    ax = sns.barplot(\n",
    "        x=\"Chromosome\",\n",
    "        y=\"Normalized Percentage\",\n",
    "        hue=\"Platform\",\n",
    "        data=chrom_distribution_melted,\n",
    "    )\n",
    "\n",
    "    plt.title(\"Normalised Chromosomal Distribution of SVs\", fontsize=16)\n",
    "    plt.xlabel(\"Chromosome\", fontsize=12)\n",
    "    plt.ylabel(\"Percentage of SVs per Mb\", fontsize=12)\n",
    "    plt.legend(title=\"Platform\", title_fontsize=12, fontsize=10)\n",
    "\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.set_xticks(range(len(ax.get_xticklabels())))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "\n",
    "    ax.tick_params(axis=\"x\", which=\"major\", pad=0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage:\n",
    "chrom_distribution_df = analyze_chrom_distribution(sv_data_df)\n",
    "plot_chrom_distribution(chrom_distribution_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sv_correlations(chrom_distribution_df, chrom_lengths):\n",
    "    corr_data = pd.DataFrame(\n",
    "        {\n",
    "            \"chrom\": chrom_lengths.keys(),\n",
    "            \"length\": chrom_lengths.values(),\n",
    "            \"ont_count\": chrom_distribution_df[\"ont\"],\n",
    "            \"illumina_count\": chrom_distribution_df[\"illumina\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ont_corr, ont_p = stats.pearsonr(corr_data[\"length\"], corr_data[\"ont_count\"])\n",
    "    illumina_corr, illumina_p = stats.pearsonr(\n",
    "        corr_data[\"length\"], corr_data[\"illumina_count\"]\n",
    "    )\n",
    "\n",
    "    print(f\"ONT correlation: {ont_corr:.3f} (p-value: {ont_p:.3e})\")\n",
    "    print(f\"Illumina correlation: {illumina_corr:.3f} (p-value: {illumina_p:.3e})\")\n",
    "\n",
    "    return corr_data, ont_corr, illumina_corr\n",
    "\n",
    "\n",
    "def plot_sv_correlations(corr_data, ont_corr, illumina_corr, figsize=(12, 5), dpi=300):\n",
    "    fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "\n",
    "    sns.regplot(\n",
    "        x=\"length\",\n",
    "        y=\"ont_count\",\n",
    "        data=corr_data,\n",
    "        ax=ax,\n",
    "        label=\"long-read\",\n",
    "        scatter_kws={\"alpha\": 0.7},\n",
    "    )\n",
    "    sns.regplot(\n",
    "        x=\"length\",\n",
    "        y=\"illumina_count\",\n",
    "        data=corr_data,\n",
    "        ax=ax,\n",
    "        label=\"short-read\",\n",
    "        scatter_kws={\"alpha\": 0.7},\n",
    "    )\n",
    "\n",
    "    ax.set_title(\"SVs vs Chromosome Length\")\n",
    "    ax.set_xlabel(\"Chromosome Length\")\n",
    "    ax.set_ylabel(\"Number of SVs\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "corr_data, ont_corr, illumina_corr = calculate_sv_correlations(\n",
    "    chrom_distribution_df, chrom_lengths\n",
    ")\n",
    "\n",
    "plot_sv_correlations(corr_data, ont_corr, illumina_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combined Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_combined_sv_plots(\n",
    "    sv_data_df, sv_counts_df, chrom_distribution_df, corr_data, ont_corr, illumina_corr\n",
    "):\n",
    "    sample_column = next(\n",
    "        col\n",
    "        for col in sv_counts_df.columns\n",
    "        if col.lower() in [\"sample\", \"sample_id\", \"anonymised_sample\"]\n",
    "    )\n",
    "\n",
    "    unique_samples = sorted(sv_data_df[\"sample_id\"].unique())\n",
    "    sample_map = {sample: f\"Sample {i+1}\" for i, sample in enumerate(unique_samples)}\n",
    "    sv_data_df[\"anonymised_sample\"] = sv_data_df[\"sample_id\"].map(sample_map)\n",
    "    sv_counts_df[\"anonymised_sample\"] = sv_counts_df[sample_column].map(sample_map)\n",
    "\n",
    "    # Figure 1\n",
    "    fig1 = plt.figure(figsize=(12, 8), dpi=300)\n",
    "    gs1 = fig1.add_gridspec(2, 2)\n",
    "\n",
    "    # Plot A: SV Counts per sample\n",
    "    ax_counts = fig1.add_subplot(gs1[0, 0])\n",
    "    sv_counts_df = compare_sv_counts(sv_data_df)\n",
    "    sv_counts_df_renamed = sv_counts_df.rename(\n",
    "        columns={\"ONT\": \"long-read\", \"Illumina\": \"short-read\", \"Consensus\": \"consensus\"}\n",
    "    )\n",
    "    sv_counts_melted = sv_counts_df_renamed.melt(\n",
    "        id_vars=[\"anonymised_sample\"], var_name=\"Platform\", value_name=\"Count\"\n",
    "    )\n",
    "\n",
    "    sns.barplot(\n",
    "        x=\"anonymised_sample\",\n",
    "        y=\"Count\",\n",
    "        hue=\"Platform\",\n",
    "        data=sv_counts_melted,\n",
    "        ax=ax_counts,\n",
    "        order=sv_counts_df[\"anonymised_sample\"],\n",
    "    )\n",
    "    ax_counts.set_title(\"SV Call Counts by Sample and Platform\")\n",
    "    ax_counts.set_xlabel(\"Sample\")\n",
    "    ax_counts.set_ylabel(\"Number of SV Calls\")\n",
    "    for tick in ax_counts.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "        tick.set_ha(\"right\")\n",
    "    ax_counts.legend(title=\"Platform\", loc=\"upper left\")\n",
    "\n",
    "    ax_counts.text(\n",
    "        -0.12,\n",
    "        1.05,\n",
    "        \"A\",\n",
    "        transform=ax_counts.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # Plot B: Aggregated SV Size Distribution\n",
    "    ax_size = fig1.add_subplot(gs1[0, 1])\n",
    "    ont_lengths = (\n",
    "        sv_data_df[sv_data_df[\"ONT\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "    illumina_lengths = (\n",
    "        sv_data_df[sv_data_df[\"Illumina\"]][\"length\"]\n",
    "        .replace([np.inf, -np.inf], np.nan)\n",
    "        .dropna()\n",
    "        .tolist()\n",
    "    )\n",
    "    sns.histplot(\n",
    "        ont_lengths,\n",
    "        log_scale=True,\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        alpha=0.7,\n",
    "        label=\"long-read\",\n",
    "        ax=ax_size,\n",
    "    )\n",
    "    sns.histplot(\n",
    "        illumina_lengths,\n",
    "        log_scale=True,\n",
    "        bins=50,\n",
    "        stat=\"density\",\n",
    "        kde=True,\n",
    "        alpha=0.7,\n",
    "        label=\"short-read\",\n",
    "        ax=ax_size,\n",
    "    )\n",
    "    ax_size.set_title(\"Aggregated SV Size Distribution\")\n",
    "    ax_size.set_xlabel(\"SV Size (log scale)\")\n",
    "    ax_size.set_ylabel(\"Density\")\n",
    "    ax_size.legend()\n",
    "    ax_size.text(\n",
    "        -0.12,\n",
    "        1.05,\n",
    "        \"B\",\n",
    "        transform=ax_size.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # Plot C and D: SV Types per sample\n",
    "    ax_types_ont = fig1.add_subplot(gs1[1, 0])\n",
    "    ax_types_illumina = fig1.add_subplot(gs1[1, 1])\n",
    "\n",
    "    platform_names = {\"ONT\": \"Long-read\", \"Illumina\": \"Short-read\"}\n",
    "\n",
    "    for i, (platform, ax) in enumerate(\n",
    "        [(\"ONT\", ax_types_ont), (\"Illumina\", ax_types_illumina)]\n",
    "    ):\n",
    "        platform_data = sv_data_df[sv_data_df[platform]]\n",
    "\n",
    "        sv_type_counts = platform_data.pivot_table(\n",
    "            index=\"anonymised_sample\", columns=\"type\", values=platform, aggfunc=\"sum\"\n",
    "        ).fillna(0)\n",
    "\n",
    "        # Ensure correct order of samples\n",
    "        sv_type_counts = sv_type_counts.reindex(\n",
    "            [f\"Sample {i+1}\" for i in range(len(unique_samples))]\n",
    "        )\n",
    "\n",
    "        sv_type_counts.plot(kind=\"bar\", stacked=True, ax=ax)\n",
    "\n",
    "        ax.set_title(f\"SV Types by Sample - {platform_names[platform]}\")\n",
    "        ax.set_xlabel(\"Sample\")\n",
    "        ax.set_ylabel(\"Number of SVs\")\n",
    "        for tick in ax.get_xticklabels():\n",
    "            tick.set_rotation(45)\n",
    "            tick.set_ha(\"right\")\n",
    "\n",
    "        if i == 0:\n",
    "            ax.legend(title=\"SV Type\", loc=\"upper left\")\n",
    "        else:\n",
    "            ax.legend().remove()\n",
    "\n",
    "        ax.text(\n",
    "            -0.12,\n",
    "            1.05,\n",
    "            \"C\" if i == 0 else \"D\",\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Figure 2\n",
    "    fig2 = plt.figure(figsize=(12, 12), dpi=300)\n",
    "    gs2 = fig2.add_gridspec(3, 2)\n",
    "\n",
    "    sv_types = [\"INS\", \"DEL\", \"DUP\", \"INV\", \"BND\", \"STR\"]\n",
    "    sv_full_names = {\n",
    "        \"INS\": \"Insertion\",\n",
    "        \"DEL\": \"Deletion\",\n",
    "        \"DUP\": \"Duplication\",\n",
    "        \"INV\": \"Inversion\",\n",
    "        \"BND\": \"Breakend\",\n",
    "        \"STR\": \"Short Tandem Repeat\",\n",
    "    }\n",
    "\n",
    "    for idx, sv_type in enumerate(sv_types):\n",
    "        ax = fig2.add_subplot(gs2[idx // 2, idx % 2])\n",
    "        for platform, alpha, label in [\n",
    "            (\"ONT\", 0.7, \"long-read\"),\n",
    "            (\"Illumina\", 0.7, \"short-read\"),\n",
    "        ]:\n",
    "            sv_data = sv_data_df[\n",
    "                (sv_data_df[\"type\"] == sv_type) & (sv_data_df[platform])\n",
    "            ]\n",
    "            if not sv_data.empty:\n",
    "                sns.histplot(\n",
    "                    data=sv_data,\n",
    "                    x=\"length\",\n",
    "                    kde=True,\n",
    "                    log_scale=(sv_type not in [\"BND\", \"STR\"]),\n",
    "                    stat=\"density\",\n",
    "                    ax=ax,\n",
    "                    alpha=alpha,\n",
    "                    label=label,\n",
    "                )\n",
    "        ax.set_title(f\"{sv_full_names[sv_type]} Size Distribution\")\n",
    "        ax.set_xlabel(\n",
    "            \"SV Size\" + (\" (log scale)\" if sv_type not in [\"BND\", \"STR\"] else \"\")\n",
    "        )\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        if idx == 0:\n",
    "            ax.legend()\n",
    "        ax.text(\n",
    "            -0.07,\n",
    "            1.05,\n",
    "            chr(ord(\"A\") + idx),\n",
    "            transform=ax.transAxes,\n",
    "            fontsize=12,\n",
    "            fontweight=\"bold\",\n",
    "            va=\"top\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Figure 3: Chromosome distribution and correlation plots side-by-side\n",
    "    fig3 = plt.figure(figsize=(12, 4), dpi=300)\n",
    "    gs3 = fig3.add_gridspec(1, 2)\n",
    "\n",
    "    # Chromosome distribution plot\n",
    "    ax_chrom = fig3.add_subplot(gs3[0, 0])\n",
    "    normalized_df = normalize_by_chrom_length(chrom_distribution_df)\n",
    "    normalized_df = normalized_df.rename(\n",
    "        columns={\"ont\": \"long-read\", \"illumina\": \"short-read\"}\n",
    "    )\n",
    "    chrom_distribution_pct = normalized_df.apply(lambda x: x / x.sum() * 100)\n",
    "    chrom_distribution_melted = chrom_distribution_pct.reset_index().melt(\n",
    "        id_vars=\"index\", var_name=\"Platform\", value_name=\"Normalized Percentage\"\n",
    "    )\n",
    "    chrom_distribution_melted = chrom_distribution_melted.rename(\n",
    "        columns={\"index\": \"Chromosome\"}\n",
    "    )\n",
    "    sns.barplot(\n",
    "        x=\"Chromosome\",\n",
    "        y=\"Normalized Percentage\",\n",
    "        hue=\"Platform\",\n",
    "        data=chrom_distribution_melted,\n",
    "        ax=ax_chrom,\n",
    "    )\n",
    "    ax_chrom.set_title(\"Normalised Chromosomal Distribution of SVs\", fontsize=12)\n",
    "    ax_chrom.set_xlabel(\"Chromosome\", fontsize=12)\n",
    "    ax_chrom.set_ylabel(\"Percentage of SVs per Mb\", fontsize=12)\n",
    "    ax_chrom.legend(title=\"Platform\", title_fontsize=12, fontsize=10)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    ax_chrom.text(\n",
    "        -0.07,\n",
    "        1.05,\n",
    "        \"A\",\n",
    "        transform=ax_chrom.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    # Correlation plot (long-read and short-read overlaid)\n",
    "    ax_corr = fig3.add_subplot(gs3[0, 1])\n",
    "    sns.regplot(\n",
    "        x=\"length\",\n",
    "        y=\"ont_count\",\n",
    "        data=corr_data,\n",
    "        ax=ax_corr,\n",
    "        label=f\"long-read (r={ont_corr:.3f})\",\n",
    "        scatter_kws={\"alpha\": 0.7},\n",
    "    )\n",
    "    sns.regplot(\n",
    "        x=\"length\",\n",
    "        y=\"illumina_count\",\n",
    "        data=corr_data,\n",
    "        ax=ax_corr,\n",
    "        label=f\"short-read (r={illumina_corr:.3f})\",\n",
    "        scatter_kws={\"alpha\": 0.7},\n",
    "    )\n",
    "    ax_corr.set_title(\"SV Counts vs Chromosome Length\")\n",
    "    ax_corr.set_xlabel(\"Chromosome Length\")\n",
    "    ax_corr.set_ylabel(\"Number of SVs\")\n",
    "    ax_corr.legend()\n",
    "    ax_corr.text(\n",
    "        -0.07,\n",
    "        1.05,\n",
    "        \"B\",\n",
    "        transform=ax_corr.transAxes,\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "        va=\"top\",\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_combined_sv_plots(\n",
    "    sv_data_df, sv_counts_df, chrom_distribution_df, corr_data, ont_corr, illumina_corr\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
